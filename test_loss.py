"""
å¢å¼ºCTCæŸå¤±å‡½æ•°æµ‹è¯•ç”¨ä¾‹
éªŒè¯åˆå¹¶åçš„æŸå¤±å‡½æ•°åŠŸèƒ½æ­£ç¡®æ€§
"""
import torch
from loss import EnhancedCTCLoss, DistillationLoss
from data import char2idx

def test_basic_functionality():
    """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
    print("=== æµ‹è¯•åŸºæœ¬åŠŸèƒ½ ===")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    B, T, V = 2, 10, len(char2idx)
    logits = torch.randn(B, T, V, requires_grad=True)
    targets = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.long)
    input_lens = torch.tensor([10, 8], dtype=torch.long)
    target_lens = torch.tensor([3, 3], dtype=torch.long)

    # æµ‹è¯•å¢å¼ºCTCæŸå¤±å‡½æ•°
    loss_fn = EnhancedCTCLoss(vocab_size=V, blank=0)

    try:
        loss = loss_fn(logits, targets, input_lens, target_lens)
        print(f"âœ“ æŸå¤±å€¼: {loss.item():.6f}")

        # æµ‹è¯•åå‘ä¼ æ’­
        loss.backward()
        print("âœ“ åå‘ä¼ æ’­æˆåŠŸ")
        if logits.grad is not None:
            print(f"âœ“ æ¢¯åº¦å½¢çŠ¶: {logits.grad.shape}")
            print(f"âœ“ æ¢¯åº¦å‡å€¼: {logits.grad.mean().item():.6f}")
            print(f"âœ“ æ¢¯åº¦èŒƒæ•°: {logits.grad.norm().item():.6f}")

        # æµ‹è¯•æŸå¤±ç»„ä»¶åˆ†è§£
        loss_components = loss_fn.get_loss_components(logits, targets, input_lens, target_lens)
        print(f"âœ“ åŸºç¡€CTCæŸå¤±: {loss_components['base_ctc_loss'].item():.6f}")
        print(f"âœ“ è·¯å¾„æƒé‡: {loss_components['path_weights'].item():.6f}")
        print(f"âœ“ åŠ æƒCTCæŸå¤±: {loss_components['weighted_ctc_loss'].item():.6f}")
        print(f"âœ“ å°¾éƒ¨ç©ºç™½æƒ©ç½š: {loss_components['eos_penalty_loss'].item():.6f}")

    except Exception as e:
        print(f"âœ— é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False

    return True

def test_backward_compatibility():
    """æµ‹è¯•åŸºç¡€åŠŸèƒ½å…¼å®¹æ€§"""
    print("\n=== æµ‹è¯•åŸºç¡€åŠŸèƒ½å…¼å®¹æ€§ ===")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    B, T, V = 2, 10, len(char2idx)
    logits = torch.randn(B, T, V, requires_grad=True)
    targets = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.long)
    input_lens = torch.tensor([10, 8], dtype=torch.long)
    target_lens = torch.tensor([3, 3], dtype=torch.long)

    try:
        # æµ‹è¯•åŸºç¡€CTCåŠŸèƒ½
        basic_loss_fn = EnhancedCTCLoss(vocab_size=V, blank=0)
        basic_loss = basic_loss_fn(logits.clone(), targets, input_lens, target_lens)

        # æµ‹è¯•å¸¦å½¢è¿‘å­—æƒé‡çš„åŠŸèƒ½
        weighted_loss_fn = EnhancedCTCLoss(vocab_size=V, blank=0, confuse_gamma=1.0)
        weighted_loss = weighted_loss_fn(logits.clone(), targets, input_lens, target_lens)

        # æµ‹è¯•å¸¦EOSæƒ©ç½šçš„åŠŸèƒ½
        eos_loss_fn = EnhancedCTCLoss(vocab_size=V, blank=0, eos_penalty=0.1)
        eos_loss = eos_loss_fn(logits.clone(), targets, input_lens, target_lens)

        print(f"âœ“ åŸºç¡€CTCæŸå¤±: {basic_loss.item():.6f}")
        print(f"âœ“ å¸¦å½¢è¿‘å­—æƒé‡æŸå¤±: {weighted_loss.item():.6f}")
        print(f"âœ“ å¸¦EOSæƒ©ç½šæŸå¤±: {eos_loss.item():.6f}")

        # éªŒè¯æ¢¯åº¦æ­£å¸¸
        basic_loss.backward()
        if logits.grad is not None:
            print(f"âœ“ æ¢¯åº¦æ­£å¸¸ï¼ŒèŒƒæ•°: {logits.grad.norm().item():.6f}")

    except Exception as e:
        print(f"âœ— å…¼å®¹æ€§æµ‹è¯•é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False

    return True

def test_confuse_characters():
    """æµ‹è¯•å½¢è¿‘å­—å¤„ç†"""
    print("\n=== æµ‹è¯•å½¢è¿‘å­—å¤„ç† ===")

    # åˆ›å»ºåŒ…å«å½¢è¿‘å­—çš„æµ‹è¯•æ•°æ®
    B, T, V = 2, 10, len(char2idx)
    logits = torch.randn(B, T, V, requires_grad=True)

    # åŒ…å«æ˜“æ··æ·†å­—ç¬¦çš„ç›®æ ‡åºåˆ—ï¼š0Ool1Iç­‰
    targets = torch.tensor([
        [char2idx['0'], char2idx['O'], char2idx['o'], char2idx['l'], char2idx['1']],  # æ˜“æ··æ·†åºåˆ—
        [char2idx['a'], char2idx['b'], char2idx['c'], char2idx['d'], char2idx['e']]   # æ­£å¸¸åºåˆ—
    ], dtype=torch.long)
    input_lens = torch.tensor([10, 10], dtype=torch.long)
    target_lens = torch.tensor([5, 5], dtype=torch.long)

    # æµ‹è¯•ä¸åŒgammaå€¼çš„å½±å“
    for gamma in [0.5, 1.0, 2.0]:
        loss_fn = EnhancedCTCLoss(vocab_size=V, blank=0, confuse_gamma=gamma)
        loss = loss_fn(logits.clone(), targets, input_lens, target_lens)

        components = loss_fn.get_loss_components(logits.clone(), targets, input_lens, target_lens)
        print(f"âœ“ Gamma={gamma}: æ€»æŸå¤±={loss.item():.6f}, è·¯å¾„æƒé‡={components['path_weights'].item():.6f}")

def test_eos_penalty():
    """æµ‹è¯•å°¾éƒ¨ç©ºç™½å­—ç¬¦æƒ©ç½š"""
    print("\n=== æµ‹è¯•å°¾éƒ¨ç©ºç™½å­—ç¬¦æƒ©ç½š ===")

    B, T, V = 2, 10, len(char2idx)

    # åˆ›å»ºå€¾å‘äºåœ¨å°¾éƒ¨äº§ç”Ÿç©ºç™½å­—ç¬¦çš„logits
    logits = torch.randn(B, T, V) * 0.5
    # åœ¨æœ€åå‡ å¸§å¢åŠ ç©ºç™½å­—ç¬¦çš„logitå€¼
    logits[:, -3:, 0] += 3.0  # ç©ºç™½å­—ç¬¦ç´¢å¼•ä¸º0

    logits.requires_grad = True

    targets = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.long)
    input_lens = torch.tensor([10, 8], dtype=torch.long)
    target_lens = torch.tensor([3, 3], dtype=torch.long)

    # æµ‹è¯•ä¸åŒæƒ©ç½šç³»æ•°çš„å½±å“
    for penalty in [0.0, 0.1, 0.5, 1.0]:
        loss_fn = EnhancedCTCLoss(vocab_size=V, blank=0, eos_penalty=penalty)
        loss = loss_fn(logits.clone(), targets, input_lens, target_lens)

        components = loss_fn.get_loss_components(logits.clone(), targets, input_lens, target_lens)
        print(f"âœ“ EOSæƒ©ç½š={penalty}: æ€»æŸå¤±={loss.item():.6f}, å°¾éƒ¨æƒ©ç½š={components['eos_penalty_loss'].item():.6f}")

def test_gradient_stability():
    """æµ‹è¯•æ¢¯åº¦ç¨³å®šæ€§"""
    print("\n=== æµ‹è¯•æ¢¯åº¦ç¨³å®šæ€§ ===")

    B, T, V = 2, 10, len(char2idx)

    # åˆ›å»ºæç«¯æƒ…å†µä¸‹çš„logits
    logits = torch.zeros(B, T, V)
    logits[:, :, 0] = 10.0  # ç©ºç™½å­—ç¬¦æ¦‚ç‡æé«˜
    logits.requires_grad = True

    targets = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.long)
    input_lens = torch.tensor([10, 8], dtype=torch.long)
    target_lens = torch.tensor([3, 3], dtype=torch.long)

    loss_fn = EnhancedCTCLoss(vocab_size=V, blank=0, gradient_clip=True)

    try:
        loss = loss_fn(logits, targets, input_lens, target_lens)
        loss.backward()

        if logits.grad is not None:
            grad_norm = logits.grad.norm().item()
            print(f"âœ“ æç«¯æƒ…å†µä¸‹æ¢¯åº¦èŒƒæ•°: {grad_norm:.6f}")

            # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦åˆç†
            if grad_norm < 1000:  # æ¢¯åº¦æ²¡æœ‰çˆ†ç‚¸
                print("âœ“ æ¢¯åº¦ç¨³å®šæ€§è‰¯å¥½")
            else:
                print("âš  æ¢¯åº¦å¯èƒ½è¿‡å¤§ï¼Œéœ€è¦è°ƒæ•´")

    except Exception as e:
        print(f"âœ— æ¢¯åº¦ç¨³å®šæ€§æµ‹è¯•é”™è¯¯: {e}")
        return False

    return True

def test_performance_comparison():
    """æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    print("\n=== æ€§èƒ½å¯¹æ¯”æµ‹è¯• ===")

    B, T, V = 32, 50, len(char2idx)  # æ›´å¤§çš„batch size

    # åˆ›å»ºéšæœºæ•°æ®
    logits = torch.randn(B, T, V)
    targets = torch.randint(1, V, (B, 10))
    input_lens = torch.full((B,), T, dtype=torch.long)
    target_lens = torch.full((B,), 10, dtype=torch.long)

    import time

    # æµ‹è¯•åŸºç¡€å®ç°
    basic_loss = EnhancedCTCLoss(vocab_size=V, blank=0)

    start_time = time.time()
    for _ in range(10):
        loss1 = basic_loss(logits.clone(), targets, input_lens, target_lens)
    basic_time = time.time() - start_time

    # æµ‹è¯•å¢å¼ºå®ç°ï¼ˆå¸¦æ–°ä¼˜åŒ–ï¼‰
    enhanced_loss = EnhancedCTCLoss(
        vocab_size=V, blank=0,
        char_focal=True,
        focal_gamma=2.0,
        adaptive_margin=True,
        margin_max=0.5
    )

    start_time = time.time()
    for _ in range(10):
        loss2 = enhanced_loss(logits.clone(), targets, input_lens, target_lens)
    enhanced_time = time.time() - start_time

    print(f"âœ“ åŸºç¡€å®ç°å¹³å‡æ—¶é—´: {basic_time/10:.4f}s")
    print(f"âœ“ å¢å¼ºå®ç°å¹³å‡æ—¶é—´: {enhanced_time/10:.4f}s")
    print(f"âœ“ æ€§èƒ½æ¯”ç‡: {enhanced_time/basic_time:.2f}x")
    print(f"âœ“ æŸå¤±å€¼å·®å¼‚: {abs(loss1.item() - loss2.item()):.6f}")

def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("å¼€å§‹å¢å¼ºCTCæŸå¤±å‡½æ•°æµ‹è¯•...\n")

    tests = [
        test_basic_functionality,
        test_backward_compatibility,
        test_confuse_characters,
        test_eos_penalty,
        test_gradient_stability,
        test_performance_comparison
    ]

    results = []
    for test in tests:
        try:
            result = test()
            results.append(result if result is not None else True)
        except Exception as e:
            print(f"æµ‹è¯• {test.__name__} å¤±è´¥: {e}")
            results.append(False)
        print("-" * 50)

    # æ€»ç»“ç»“æœ
    passed = sum(results)
    total = len(results)
    print(f"\næµ‹è¯•æ€»ç»“: {passed}/{total} æµ‹è¯•é€šè¿‡")

    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¢å¼ºCTCæŸå¤±å‡½æ•°å·¥ä½œæ­£å¸¸ã€‚")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°ã€‚")

    return passed == total

def test_char_focal_loss():
    """æµ‹è¯•å­—ç¬¦çº§Focal LossåŠŸèƒ½"""
    print("\n=== æµ‹è¯•å­—ç¬¦çº§Focal Loss ===")

    B, T, V = 2, 10, len(char2idx)
    logits = torch.randn(B, T, V, requires_grad=True)
    targets = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.long)
    input_lens = torch.tensor([10, 8], dtype=torch.long)
    target_lens = torch.tensor([3, 3], dtype=torch.long)

    # æµ‹è¯•ä¸åŒfocalå‚æ•°çš„å½±å“
    for char_focal in [False, True]:
        for gamma in [1.0, 2.0, 3.0]:
            loss_fn = EnhancedCTCLoss(
                vocab_size=V, blank=0,
                char_focal=char_focal,
                focal_gamma=gamma,
                focal_scale=1.0
            )
            loss = loss_fn(logits.clone(), targets, input_lens, target_lens)

            components = loss_fn.get_loss_components(logits.clone(), targets, input_lens, target_lens)

            print(f"âœ“ å­—ç¬¦çº§Focal={char_focal}, Gamma={gamma}: æ€»æŸå¤±={loss.item():.6f}")
            if char_focal:
                print(f"  - å­—ç¬¦çº§FocalæŸå¤±: {components['char_focal_loss'].item():.6f}")
                print(f"  - æ ·æœ¬çº§FocalæŸå¤±: {components['sample_focal_loss'].item():.6f}")

def test_adaptive_margin():
    """æµ‹è¯•è‡ªé€‚åº”Marginæœºåˆ¶"""
    print("\n=== æµ‹è¯•è‡ªé€‚åº”Marginæœºåˆ¶ ===")

    B, T, V = 2, 10, len(char2idx)
    logits = torch.randn(B, T, V, requires_grad=True)
    targets = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.long)
    input_lens = torch.tensor([10, 8], dtype=torch.long)
    target_lens = torch.tensor([3, 3], dtype=torch.long)

    # æµ‹è¯•ä¸åŒmarginé…ç½®
    test_configs = [
        {"adaptive_margin": False, "margin": 0.0},
        {"adaptive_margin": False, "margin": 0.3},
        {"adaptive_margin": True, "margin": 0.0, "margin_max": 0.5},
        {"adaptive_margin": True, "margin": 0.0, "margin_max": 1.0},
    ]

    for config in test_configs:
        loss_fn = EnhancedCTCLoss(vocab_size=V, blank=0, **config)
        loss = loss_fn(logits.clone(), targets, input_lens, target_lens)

        components = loss_fn.get_loss_components(logits.clone(), targets, input_lens, target_lens)

        config_str = ", ".join([f"{k}={v}" for k, v in config.items()])
        print(f"âœ“ {config_str}: æ€»æŸå¤±={loss.item():.6f}")

        if config.get("adaptive_margin", False):
            print(f"  - å½“å‰è‡ªé€‚åº”Margin: {components.get('adaptive_margin', 0):.4f}")

def test_temperature_annealing():
    """æµ‹è¯•æ¸©åº¦é€€ç«æœºåˆ¶"""
    print("\n=== æµ‹è¯•æ¸©åº¦é€€ç«æœºåˆ¶ ===")

    B, T, V = 2, 10, len(char2idx)
    logits = torch.randn(B, T, V, requires_grad=True)
    targets = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.long)
    input_lens = torch.tensor([10, 8], dtype=torch.long)
    target_lens = torch.tensor([3, 3], dtype=torch.long)

    loss_fn = EnhancedCTCLoss(
        vocab_size=V, blank=0,
        temperature_annealing=True,
        char_focal=True,
        focal_gamma=2.0
    )

    # æµ‹è¯•ä¸åŒepochçš„é€€ç«æ•ˆæœ
    max_epoch = 10
    for epoch in range(0, max_epoch + 1, 2):
        loss_fn.schedule(epoch, max_epoch)
        loss = loss_fn(logits.clone(), targets, input_lens, target_lens)

        components = loss_fn.get_loss_components(logits.clone(), targets, input_lens, target_lens)

        print(f"âœ“ Epoch {epoch}/{max_epoch}: æ€»æŸå¤±={loss.item():.6f}")
        print(f"  - Focalç¼©æ”¾å› å­: {components['focal_scale']:.4f}")
        print(f"  - å­—ç¬¦çº§FocalæŸå¤±: {components['char_focal_loss'].item():.6f}")

def test_combined_optimizations():
    """æµ‹è¯•ç»„åˆä¼˜åŒ–æ•ˆæœ"""
    print("\n=== æµ‹è¯•ç»„åˆä¼˜åŒ–æ•ˆæœ ===")

    B, T, V = 4, 15, len(char2idx)

    # åˆ›å»ºåŒ…å«å½¢è¿‘å­—çš„å›°éš¾æ ·æœ¬
    logits = torch.randn(B, T, V)
    # åœ¨æœ€åå‡ å¸§å¢åŠ blankæ¦‚ç‡ï¼Œæ¨¡æ‹Ÿå°¾éƒ¨ç©ºç™½é—®é¢˜
    logits[:, -4:, 0] += 2.0
    logits.requires_grad = True

    # åŒ…å«æ˜“æ··æ·†å­—ç¬¦çš„ç›®æ ‡åºåˆ—
    targets = torch.tensor([
        [char2idx['0'], char2idx['O'], char2idx['o'], char2idx['l'], char2idx['1']],  # é«˜æ··æ·†
        [char2idx['1'], char2idx['l'], char2idx['I'], char2idx['|'], char2idx['i']],  # é«˜æ··æ·†
        [char2idx['a'], char2idx['b'], char2idx['c'], char2idx['d'], char2idx['e']],  # æ­£å¸¸
        [char2idx['p'], char2idx['q'], char2idx['u'], char2idx['v'], char2idx['n']],  # ä¸­ç­‰æ··æ·†
    ], dtype=torch.long)
    input_lens = torch.tensor([15, 14, 15, 13], dtype=torch.long)
    target_lens = torch.tensor([5, 5, 5, 5], dtype=torch.long)

    # åŸºå‡†é…ç½® - åªä½¿ç”¨åŸºç¡€ä¼˜åŒ–
    baseline_loss_fn = EnhancedCTCLoss(
        vocab_size=V, blank=0,
        confuse_gamma=1.0,
        eos_penalty=0.1
    )
    baseline_loss = baseline_loss_fn(logits.clone(), targets, input_lens, target_lens)
    baseline_components = baseline_loss_fn.get_loss_components(logits.clone(), targets, input_lens, target_lens)

    print(f"âœ“ åŸºå‡†é…ç½® - æ€»æŸå¤±: {baseline_loss.item():.6f}")
    print(f"  - åŸºç¡€CTCæŸå¤±: {baseline_components['base_ctc_loss'].item():.6f}")
    print(f"  - è·¯å¾„æƒé‡: {baseline_components['path_weights'].item():.6f}")
    print(f"  - EOSæƒ©ç½š: {baseline_components['eos_penalty_loss'].item():.6f}")

    # å®Œæ•´ä¼˜åŒ–é…ç½® - å¯ç”¨æ‰€æœ‰æ–°åŠŸèƒ½
    full_optimization_loss_fn = EnhancedCTCLoss(
        vocab_size=V, blank=0,
        confuse_gamma=1.2,
        eos_penalty=0.15,
        char_focal=True,
        focal_gamma=2.0,
        focal_scale=1.0,
        adaptive_margin=True,
        margin_max=0.5,
        temperature_annealing=True
    )

    # æ¨¡æ‹Ÿè®­ç»ƒåæœŸçš„é€€ç«çŠ¶æ€
    full_optimization_loss_fn.schedule(5, 10)  # epoch=5, max_epoch=10

    full_optimization_loss = full_optimization_loss_fn(logits.clone(), targets, input_lens, target_lens)
    full_components = full_optimization_loss_fn.get_loss_components(logits.clone(), targets, input_lens, target_lens)

    print(f"âœ“ å®Œæ•´ä¼˜åŒ– - æ€»æŸå¤±: {full_optimization_loss.item():.6f}")
    print(f"  - åŸºç¡€CTCæŸå¤±: {full_components['base_ctc_loss'].item():.6f}")
    print(f"  - å­—ç¬¦çº§FocalæŸå¤±: {full_components['char_focal_loss'].item():.6f}")
    print(f"  - æ ·æœ¬çº§FocalæŸå¤±: {full_components['sample_focal_loss'].item():.6f}")
    print(f"  - è·¯å¾„æƒé‡: {full_components['path_weights'].item():.6f}")
    print(f"  - EOSæƒ©ç½š: {full_components['eos_penalty_loss'].item():.6f}")
    print(f"  - è‡ªé€‚åº”Margin: {full_components.get('adaptive_margin', 0):.4f}")
    print(f"  - Focalç¼©æ”¾å› å­: {full_components['focal_scale']:.4f}")

    # å¯¹æ¯”æ•ˆæœ
    improvement = baseline_loss.item() - full_optimization_loss.item()
    print(f"âœ“ ä¼˜åŒ–æ•ˆæœ: {improvement:.6f} ({improvement/baseline_loss.item()*100:.2f}%)")

def test_temperature_parameter_effects():
    """æµ‹è¯•æ¸©åº¦å‚æ•°å½±å“"""
    print("\n=== æµ‹è¯•æ¸©åº¦å‚æ•°å½±å“ ===")

    B, T, V = 2, 10, len(char2idx)
    logits = torch.randn(B, T, V, requires_grad=True)

    # åŒ…å«æ˜“æ··æ·†å­—ç¬¦çš„ç›®æ ‡åºåˆ—
    targets = torch.tensor([
        [char2idx['0'], char2idx['O'], char2idx['o'], char2idx['l'], char2idx['1']],  # æ˜“æ··æ·†åºåˆ—
        [char2idx['a'], char2idx['b'], char2idx['c'], char2idx['d'], char2idx['e']]   # æ­£å¸¸åºåˆ—
    ], dtype=torch.long)
    input_lens = torch.tensor([10, 10], dtype=torch.long)
    target_lens = torch.tensor([5, 5], dtype=torch.long)

    # æµ‹è¯•ä¸åŒæ¸©åº¦å‚æ•°çš„å½±å“
    base_gamma = 1.0
    for temperature in [0.5, 1.0, 2.0, 5.0]:
        loss_fn = EnhancedCTCLoss(
            vocab_size=V, blank=0,
            confuse_gamma=base_gamma,
            confuse_temperature=temperature
        )
        loss = loss_fn(logits.clone(), targets, input_lens, target_lens)

        components = loss_fn.get_loss_components(logits.clone(), targets, input_lens, target_lens)
        print(f"âœ“ æ¸©åº¦={temperature}: æ€»æŸå¤±={loss.item():.6f}, è·¯å¾„æƒé‡={components['path_weights'].item():.6f}")

        # æ¸©åº¦è¶Šé«˜ï¼Œæƒé‡å·®å¼‚åº”è¯¥è¶Šå°
        if temperature > 1.0:
            print("  - é«˜æ¸©ä¸‹æƒé‡è¶‹äºå¹³ç¼“")

def test_adaptive_eos_window():
    """æµ‹è¯•è‡ªé€‚åº”å°¾éƒ¨çª—å£å¤§å°"""
    print("\n=== æµ‹è¯•è‡ªé€‚åº”å°¾éƒ¨çª—å£å¤§å° ===")

    B, T, V = 3, 12, len(char2idx)

    # åˆ›å»ºä¸åŒé•¿åº¦çš„åºåˆ—
    logits = torch.randn(B, T, V) * 0.5
    # åœ¨å°¾éƒ¨å¢åŠ ç©ºç™½å­—ç¬¦æ¦‚ç‡
    for i in range(B):
        tail_start = max(0, T - 3 - i)  # ä¸åŒé•¿åº¦çš„å°¾éƒ¨
        logits[i, tail_start:, 0] += 2.0

    logits.requires_grad = True

    targets = torch.tensor([[1, 2, 0, 0], [3, 4, 5, 0], [6, 7, 8, 9]], dtype=torch.long)
    input_lens = torch.tensor([12, 10, 8], dtype=torch.long)
    target_lens = torch.tensor([2, 3, 4], dtype=torch.long)

    # æµ‹è¯•å›ºå®šçª—å£ vs è‡ªé€‚åº”çª—å£
    window_sizes = [2, 3, 5]

    for window_size in window_sizes:
        # å›ºå®šçª—å£
        fixed_loss_fn = EnhancedCTCLoss(
            vocab_size=V, blank=0,
            eos_penalty=0.1,
            eos_window_size=window_size,
            eos_adaptive=False
        )
        fixed_loss = fixed_loss_fn(logits.clone(), targets, input_lens, target_lens)
        fixed_components = fixed_loss_fn.get_loss_components(logits.clone(), targets, input_lens, target_lens)

        # è‡ªé€‚åº”çª—å£
        adaptive_loss_fn = EnhancedCTCLoss(
            vocab_size=V, blank=0,
            eos_penalty=0.1,
            eos_window_size=window_size,
            eos_adaptive=True
        )
        adaptive_loss = adaptive_loss_fn(logits.clone(), targets, input_lens, target_lens)
        adaptive_components = adaptive_loss_fn.get_loss_components(logits.clone(), targets, input_lens, target_lens)

        print(f"âœ“ çª—å£å¤§å°={window_size}:")
        print(f"  - å›ºå®šçª—å£: æ€»æŸå¤±={fixed_loss.item():.6f}, EOSæƒ©ç½š={fixed_components['eos_penalty_loss'].item():.6f}")
        print(f"  - è‡ªé€‚åº”çª—å£: æ€»æŸå¤±={adaptive_loss.item():.6f}, EOSæƒ©ç½š={adaptive_components['eos_penalty_loss'].item():.6f}")

def test_gradient_clip_thresholds():
    """æµ‹è¯•æ¢¯åº¦è£å‰ªé˜ˆå€¼"""
    print("\n=== æµ‹è¯•æ¢¯åº¦è£å‰ªé˜ˆå€¼ ===")

    B, T, V = 2, 8, len(char2idx)

    # åˆ›å»ºå®¹æ˜“äº§ç”Ÿå¤§æ¢¯åº¦çš„æç«¯æƒ…å†µ
    logits = torch.zeros(B, T, V)
    logits[:, :, 0] = 15.0  # æé«˜çš„ç©ºç™½å­—ç¬¦æ¦‚ç‡
    logits[:, :, 1] = -10.0  # å…¶ä»–å­—ç¬¦æ¦‚ç‡æä½
    logits.requires_grad = True

    targets = torch.tensor([[1, 2], [3, 4]], dtype=torch.long)
    input_lens = torch.tensor([8, 6], dtype=torch.long)
    target_lens = torch.tensor([2, 2], dtype=torch.long)

    # æµ‹è¯•ä¸åŒæ¢¯åº¦è£å‰ªé…ç½®
    clip_configs = [
        {"gradient_clip": False},
        {"gradient_clip": True},  # é»˜è®¤è£å‰ªé˜ˆå€¼
    ]

    for config in clip_configs:
        loss_fn = EnhancedCTCLoss(vocab_size=V, blank=0, **config)

        try:
            loss = loss_fn(logits.clone(), targets, input_lens, target_lens)
            loss.backward()

            if logits.grad is not None:
                grad_norm = logits.grad.norm().item()
                print(f"âœ“ æ¢¯åº¦è£å‰ª={config['gradient_clip']}: æ¢¯åº¦èŒƒæ•°={grad_norm:.6f}")

                if config["gradient_clip"]:
                    if grad_norm < 1000:  # æ£€æŸ¥è£å‰ªæ•ˆæœ
                        print("  - æ¢¯åº¦è£å‰ªæœ‰æ•ˆï¼šæ¢¯åº¦è¢«æ§åˆ¶åœ¨åˆç†èŒƒå›´")
                    else:
                        print("  - âš ï¸ æ¢¯åº¦è£å‰ªå¯èƒ½æœªç”Ÿæ•ˆ")
                else:
                    print("  - æ— è£å‰ªï¼šæ¢¯åº¦èŒƒæ•°å¯èƒ½è¿‡å¤§")

            # æ¸…ç©ºæ¢¯åº¦
            if logits.grad is not None:
                logits.grad.zero_()

        except Exception as e:
            print(f"âœ— æ¢¯åº¦è£å‰ªæµ‹è¯•é”™è¯¯: {e}")

def test_different_reduction_modes():
    """æµ‹è¯•ä¸åŒreductionæ¨¡å¼"""
    print("\n=== æµ‹è¯•ä¸åŒreductionæ¨¡å¼ ===")

    B, T, V = 3, 8, len(char2idx)
    logits = torch.randn(B, T, V, requires_grad=True)
    targets = torch.tensor([[1, 2, 0], [3, 4, 5], [6, 7, 0]], dtype=torch.long)
    input_lens = torch.tensor([8, 7, 6], dtype=torch.long)
    target_lens = torch.tensor([2, 3, 2], dtype=torch.long)

    # æµ‹è¯•ä¸åŒreductionæ¨¡å¼
    reduction_modes = ['mean', 'sum']

    for reduction in reduction_modes:
        loss_fn = EnhancedCTCLoss(vocab_size=V, blank=0, reduction=reduction)
        loss = loss_fn(logits.clone(), targets, input_lens, target_lens)

        # è·å–ç»„ä»¶åˆ†è§£
        components = loss_fn.get_loss_components(logits.clone(), targets, input_lens, target_lens)

        print(f"âœ“ reduction='{reduction}': æ€»æŸå¤±={loss.item():.6f}")
        print(f"  - åŸºç¡€CTCæŸå¤±: {components['base_ctc_loss'].item():.6f}")
        print(f"  - åŠ æƒCTCæŸå¤±: {components['weighted_ctc_loss'].item():.6f}")

        # éªŒè¯reductionæ•ˆæœ
        if reduction == 'sum':
            # sumæ¨¡å¼åº”è¯¥æ¯”meanæ¨¡å¼æŸå¤±å€¼å¤§ï¼ˆå› ä¸ºbatch_size=3ï¼‰
            print("  - sumæ¨¡å¼æŸå¤±å€¼åº”å¤§äºmeanæ¨¡å¼")
        elif reduction == 'mean':
            print("  - meanæ¨¡å¼æŸå¤±å€¼åº”é€‚ä¸­")

def test_numerical_stability_boundaries():
    """æµ‹è¯•æ•°å€¼ç¨³å®šæ€§è¾¹ç•Œæƒ…å†µ"""
    print("\n=== æµ‹è¯•æ•°å€¼ç¨³å®šæ€§è¾¹ç•Œæƒ…å†µ ===")

    B, T, V = 2, 6, len(char2idx)

    # æµ‹è¯•ä¸åŒçš„è¾¹ç•Œæƒ…å†µ
    boundary_cases = [
        {
            "name": "æé«˜ç©ºç™½æ¦‚ç‡",
            "logits_config": lambda l: l[:, :, 0].fill_(20.0),  # ç©ºç™½å­—ç¬¦logitæé«˜
            "description": "æ¨¡æ‹Ÿç©ºç™½å­—ç¬¦å ç»å¯¹ä¼˜åŠ¿çš„æƒ…å†µ"
        },
        {
            "name": "æä½æ¦‚ç‡",
            "logits_config": lambda l: l.fill_(-20.0),  # æ‰€æœ‰logitsæä½
            "description": "æ¨¡æ‹Ÿæ‰€æœ‰å­—ç¬¦æ¦‚ç‡éƒ½æä½çš„æƒ…å†µ"
        },
        {
            "name": "å•å³°åˆ†å¸ƒ",
            "logits_config": lambda l: l[:, :, 1].fill_(10.0),  # åªæœ‰ç›®æ ‡å­—ç¬¦æœ‰é«˜æ¦‚ç‡
            "description": "æ¨¡æ‹Ÿç›®æ ‡å­—ç¬¦å ç»å¯¹ä¼˜åŠ¿çš„æƒ…å†µ"
        },
        {
            "name": "å‡åŒ€åˆ†å¸ƒ",
            "logits_config": lambda l: l.fill_(0.0),  # æ‰€æœ‰logitsç›¸ç­‰
            "description": "æ¨¡æ‹Ÿæ‰€æœ‰å­—ç¬¦æ¦‚ç‡ç›¸ç­‰çš„æƒ…å†µ"
        }
    ]

    targets = torch.tensor([[1, 2], [3, 4]], dtype=torch.long)
    input_lens = torch.tensor([6, 5], dtype=torch.long)
    target_lens = torch.tensor([2, 2], dtype=torch.long)

    for case in boundary_cases:
        print(f"\n--- æµ‹è¯•: {case['name']} ---")
        print(f"æè¿°: {case['description']}")

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        logits = torch.randn(B, T, V) * 0.1  # åŸºç¡€å°éšæœºå€¼
        case['logits_config'](logits)  # åº”ç”¨è¾¹ç•Œæ¡ä»¶
        logits.requires_grad = True

        loss_fn = EnhancedCTCLoss(
            vocab_size=V, blank=0,
            gradient_clip=True  # å¯ç”¨æ¢¯åº¦è£å‰ªé˜²æ­¢æ•°å€¼é—®é¢˜
        )

        try:
            loss = loss_fn(logits, targets, input_lens, target_lens)
            loss.backward()

            print(f"âœ“ æŸå¤±å€¼: {loss.item():.6f}")

            if logits.grad is not None:
                grad_norm = logits.grad.norm().item()
                print(f"âœ“ æ¢¯åº¦èŒƒæ•°: {grad_norm:.6f}")

                # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦åˆç†
                if torch.isnan(logits.grad).any():
                    print("âœ— å‡ºç°NaNæ¢¯åº¦")
                elif torch.isinf(logits.grad).any():
                    print("âœ— å‡ºç°Infæ¢¯åº¦")
                elif grad_norm > 1000:
                    print("âš ï¸ æ¢¯åº¦å¯èƒ½è¿‡å¤§")
                else:
                    print("âœ“ æ¢¯åº¦æ•°å€¼ç¨³å®š")

            # æµ‹è¯•ç»„ä»¶åˆ†è§£
            components = loss_fn.get_loss_components(logits.clone(), targets, input_lens, target_lens)
            print("âœ“ å„ç»„ä»¶æ•°å€¼æ­£å¸¸")

        except Exception as e:
            print(f"âœ— è¾¹ç•Œæƒ…å†µæµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

        # æ¸…ç©ºæ¢¯åº¦ç”¨äºä¸‹ä¸€ä¸ªæµ‹è¯•
        if logits.grad is not None:
            logits.grad.zero_()


def test_distillation_loss_basic():
    """æµ‹è¯•çŸ¥è¯†è’¸é¦æŸå¤±å‡½æ•°åŸºæœ¬åŠŸèƒ½"""
    print("\n=== æµ‹è¯•çŸ¥è¯†è’¸é¦æŸå¤±åŸºæœ¬åŠŸèƒ½ ===")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    B, L_teacher, L_student, D, V = 2, 10, 8, 512, 100

    # æ•™å¸ˆæ¨¡å‹è¾“å‡ºï¼ˆåºåˆ—è¾ƒé•¿ï¼‰
    teacher_features = torch.randn(B, L_teacher, D, requires_grad=False)
    teacher_logits = torch.randn(B, L_teacher, V, requires_grad=False)

    # å­¦ç”Ÿæ¨¡å‹è¾“å‡ºï¼ˆåºåˆ—è¾ƒçŸ­ï¼‰
    student_features = torch.randn(B, L_student, D, requires_grad=True)
    student_logits = torch.randn(B, L_student, V, requires_grad=True)

    # åˆ›å»ºè’¸é¦æŸå¤±å‡½æ•°
    distill_loss_fn = DistillationLoss(temperature=4.0, alpha_feat=0.5, alpha_logit=0.5)

    try:
        # è®¡ç®—è’¸é¦æŸå¤±
        losses = distill_loss_fn(
            teacher_features=teacher_features,
            student_features=student_features,
            teacher_logits=teacher_logits,
            student_logits=student_logits
        )

        print(f"âœ“ ç‰¹å¾å¯¹é½æŸå¤±: {losses['feature_loss'].item():.6f}")
        print(f"âœ“ KLæ•£åº¦æŸå¤±: {losses['kl_loss'].item():.6f}")
        print(f"âœ“ æ€»è’¸é¦æŸå¤±: {losses['total_distill_loss'].item():.6f}")

        # æµ‹è¯•åå‘ä¼ æ’­
        total_loss = losses['total_distill_loss']
        total_loss.backward()

        # æ£€æŸ¥å­¦ç”Ÿæ¨¡å‹æ¢¯åº¦
        if student_features.grad is not None:
            print(f"âœ“ å­¦ç”Ÿç‰¹å¾æ¢¯åº¦å½¢çŠ¶: {student_features.grad.shape}")
            print(f"âœ“ å­¦ç”Ÿç‰¹å¾æ¢¯åº¦èŒƒæ•°: {student_features.grad.norm().item():.6f}")

        if student_logits.grad is not None:
            print(f"âœ“ å­¦ç”Ÿlogitsæ¢¯åº¦å½¢çŠ¶: {student_logits.grad.shape}")
            print(f"âœ“ å­¦ç”Ÿlogitsæ¢¯åº¦èŒƒæ•°: {student_logits.grad.norm().item():.6f}")

        # éªŒè¯æ•™å¸ˆæ¨¡å‹æ²¡æœ‰æ¢¯åº¦
        if teacher_features.grad is None or teacher_features.grad.abs().sum() == 0:
            print("âœ“ æ•™å¸ˆç‰¹å¾æ²¡æœ‰æ¢¯åº¦ï¼ˆæ­£ç¡®ï¼‰")

        return True

    except Exception as e:
        print(f"âœ— åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_distillation_attention_alignment():
    """æµ‹è¯•äº¤å‰æ³¨æ„åŠ›å¯¹é½æœºåˆ¶"""
    print("\n=== æµ‹è¯•äº¤å‰æ³¨æ„åŠ›å¯¹é½æœºåˆ¶ ===")

    B, L_teacher, L_student, D, V = 2, 12, 6, 256, 50

    # åˆ›å»ºæœ‰æ˜æ˜¾å·®å¼‚çš„åºåˆ—é•¿åº¦
    teacher_features = torch.randn(B, L_teacher, D)
    student_features = torch.randn(B, L_student, D)
    teacher_logits = torch.randn(B, L_teacher, V)
    student_logits = torch.randn(B, L_student, V)

    distill_loss_fn = DistillationLoss(temperature=4.0)

    # è®¡ç®—å¯¹é½å‰åçš„å½¢çŠ¶
    print(f"æ•™å¸ˆç‰¹å¾å½¢çŠ¶: {teacher_features.shape}")
    print(f"å­¦ç”Ÿç‰¹å¾å½¢çŠ¶: {student_features.shape}")
    print(f"æ•™å¸ˆlogitså½¢çŠ¶: {teacher_logits.shape}")
    print(f"å­¦ç”Ÿlogitså½¢çŠ¶: {student_logits.shape}")

    losses = distill_loss_fn(
        teacher_features=teacher_features,
        student_features=student_features,
        teacher_logits=teacher_logits,
        student_logits=student_logits
    )

    print(f"âœ“ å¯¹é½åç‰¹å¾æŸå¤±: {losses['feature_loss'].item():.6f}")
    print(f"âœ“ å¯¹é½åKLæŸå¤±: {losses['kl_loss'].item():.6f}")

    # éªŒè¯æŸå¤±å€¼åˆç†èŒƒå›´
    assert losses['feature_loss'].item() >= 0, "ç‰¹å¾æŸå¤±åº”è¯¥éè´Ÿ"
    assert losses['kl_loss'].item() >= 0, "KLæŸå¤±åº”è¯¥éè´Ÿ"
    print("âœ“ æŸå¤±å€¼èŒƒå›´æ­£å¸¸")


def test_distillation_temperature_effects():
    """æµ‹è¯•æ¸©åº¦å‚æ•°å¯¹è’¸é¦æŸå¤±çš„å½±å“"""
    print("\n=== æµ‹è¯•æ¸©åº¦å‚æ•°å½±å“ ===")

    B, L, D, V = 2, 8, 128, 30

    # åˆ›å»ºç›¸åŒçš„æ•™å¸ˆå’Œå­¦ç”Ÿè¾“å‡º
    teacher_features = torch.randn(B, L, D)
    student_features = teacher_features + torch.randn(B, L, D) * 0.1  # å°å¹…å·®å¼‚
    teacher_logits = torch.randn(B, L, V)
    student_logits = teacher_logits + torch.randn(B, L, V) * 0.1

    temperatures = [1.0, 2.0, 4.0, 8.0, 16.0]

    for temp in temperatures:
        distill_loss_fn = DistillationLoss(temperature=temp, alpha_feat=0.5, alpha_logit=0.5)

        losses = distill_loss_fn(
            teacher_features=teacher_features,
            student_features=student_features,
            teacher_logits=teacher_logits,
            student_logits=student_logits
        )

        print(f"âœ“ æ¸©åº¦={temp}: ç‰¹å¾æŸå¤±={losses['feature_loss'].item():.6f}, KLæŸå¤±={losses['kl_loss'].item():.6f}")

        # æ¸©åº¦è¶Šé«˜ï¼ŒKLæŸå¤±åº”è¯¥è¶Šå°ï¼ˆåˆ†å¸ƒæ›´å¹³æ»‘ï¼‰
        if temp > 4.0:
            print(f"  - é«˜æ¸©ä¸‹KLæŸå¤±ç›¸å¯¹è¾ƒå°: {losses['kl_loss'].item():.6f}")


def test_distillation_mask_functionality():
    """æµ‹è¯•maskåŠŸèƒ½"""
    print("\n=== æµ‹è¯•maskåŠŸèƒ½ ===")

    B, L, D, V = 2, 10, 256, 50

    teacher_features = torch.randn(B, L, D)
    student_features = torch.randn(B, L, D)
    teacher_logits = torch.randn(B, L, V)
    student_logits = torch.randn(B, L, V)

    # åˆ›å»ºmaskï¼Œå±è”½éƒ¨åˆ†ä½ç½®
    mask = torch.zeros(B, L, dtype=torch.bool)
    mask[:, :6] = True  # åªä¿ç•™å‰6ä¸ªä½ç½®

    distill_loss_fn = DistillationLoss(temperature=4.0)

    # æµ‹è¯•æœ‰maskå’Œæ— maskçš„æƒ…å†µ
    losses_with_mask = distill_loss_fn(
        teacher_features=teacher_features,
        student_features=student_features,
        teacher_logits=teacher_logits,
        student_logits=student_logits,
        mask=mask
    )

    losses_without_mask = distill_loss_fn(
        teacher_features=teacher_features,
        student_features=student_features,
        teacher_logits=teacher_logits,
        student_logits=student_logits,
        mask=None
    )

    print(f"âœ“ æœ‰mask - ç‰¹å¾æŸå¤±: {losses_with_mask['feature_loss'].item():.6f}, KLæŸå¤±: {losses_with_mask['kl_loss'].item():.6f}")
    print(f"âœ“ æ— mask - ç‰¹å¾æŸå¤±: {losses_without_mask['feature_loss'].item():.6f}, KLæŸå¤±: {losses_without_mask['kl_loss'].item():.6f}")

    # æœ‰maskæ—¶æŸå¤±åº”è¯¥æ›´å°ï¼ˆåªè®¡ç®—éƒ¨åˆ†ä½ç½®ï¼‰
    assert losses_with_mask['kl_loss'].item() <= losses_without_mask['kl_loss'].item() * 1.1
    print("âœ“ maskåŠŸèƒ½æ­£å¸¸")


def test_distillation_gradient_stability():
    """æµ‹è¯•è’¸é¦æŸå¤±çš„æ¢¯åº¦ç¨³å®šæ€§"""
    print("\n=== æµ‹è¯•æ¢¯åº¦ç¨³å®šæ€§ ===")

    B, L, D, V = 2, 8, 128, 20

    # åˆ›å»ºæç«¯æƒ…å†µ
    teacher_features = torch.randn(B, L, D)
    student_features = torch.randn(B, L, D, requires_grad=True)

    # åˆ›å»ºæç«¯logitså€¼
    teacher_logits = torch.zeros(B, L, V)
    teacher_logits[:, :, 0] = 100.0  # ä¸€ä¸ªç±»åˆ«æ¦‚ç‡æé«˜
    student_logits = torch.full((B, L, V), 50.0, requires_grad=True)  # é¿å…in-placeæ“ä½œ

    distill_loss_fn = DistillationLoss(temperature=4.0)

    try:
        losses = distill_loss_fn(
            teacher_features=teacher_features,
            student_features=student_features,
            teacher_logits=teacher_logits,
            student_logits=student_logits
        )

        total_loss = losses['total_distill_loss']
        total_loss.backward()

        # æ£€æŸ¥æ¢¯åº¦
        if student_features.grad is not None:
            grad_norm_features = student_features.grad.norm().item()
            print(f"âœ“ å­¦ç”Ÿç‰¹å¾æ¢¯åº¦èŒƒæ•°: {grad_norm_features:.6f}")

            if torch.isnan(student_features.grad).any():
                print("âœ— å­¦ç”Ÿç‰¹å¾æ¢¯åº¦å‡ºç°NaN")
            elif torch.isinf(student_features.grad).any():
                print("âœ— å­¦ç”Ÿç‰¹å¾æ¢¯åº¦å‡ºç°Inf")
            else:
                print("âœ“ å­¦ç”Ÿç‰¹å¾æ¢¯åº¦æ­£å¸¸")

        if student_logits.grad is not None:
            grad_norm_logits = student_logits.grad.norm().item()
            print(f"âœ“ å­¦ç”Ÿlogitsæ¢¯åº¦èŒƒæ•°: {grad_norm_logits:.6f}")

            if torch.isnan(student_logits.grad).any():
                print("âœ— å­¦ç”Ÿlogitsæ¢¯åº¦å‡ºç°NaN")
            elif torch.isinf(student_logits.grad).any():
                print("âœ— å­¦ç”Ÿlogitsæ¢¯åº¦å‡ºç°Inf")
            else:
                print("âœ“ å­¦ç”Ÿlogitsæ¢¯åº¦æ­£å¸¸")

        return True

    except Exception as e:
        print(f"âœ— æ¢¯åº¦ç¨³å®šæ€§æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_distillation_alpha_weights():
    """æµ‹è¯•alphaæƒé‡å‚æ•°çš„å½±å“"""
    print("\n=== æµ‹è¯•alphaæƒé‡å‚æ•° ===")

    B, L, D, V = 2, 8, 256, 30

    teacher_features = torch.randn(B, L, D)
    student_features = torch.randn(B, L, D)
    teacher_logits = torch.randn(B, L, V)
    student_logits = torch.randn(B, L, V)

    # æµ‹è¯•ä¸åŒçš„alphaç»„åˆ
    alpha_combinations = [
        {"alpha_feat": 1.0, "alpha_logit": 0.0},  # åªä½¿ç”¨ç‰¹å¾æŸå¤±
        {"alpha_feat": 0.0, "alpha_logit": 1.0},  # åªä½¿ç”¨logitsæŸå¤±
        {"alpha_feat": 0.7, "alpha_logit": 0.3},  # åé‡ç‰¹å¾
        {"alpha_feat": 0.3, "alpha_logit": 0.7},  # åé‡logits
        {"alpha_feat": 0.5, "alpha_logit": 0.5},  # å¹³è¡¡
    ]

    for alpha_config in alpha_combinations:
        distill_loss_fn = DistillationLoss(temperature=4.0, **alpha_config)

        losses = distill_loss_fn(
            teacher_features=teacher_features,
            student_features=student_features,
            teacher_logits=teacher_logits,
            student_logits=student_logits
        )

        total_loss = losses['total_distill_loss'].item()
        feature_loss = losses['feature_loss'].item()
        kl_loss = losses['kl_loss'].item()

        expected_total = (alpha_config['alpha_feat'] * feature_loss +
                         alpha_config['alpha_logit'] * kl_loss)

        print(f"âœ“ alpha_feat={alpha_config['alpha_feat']}, alpha_logit={alpha_config['alpha_logit']}")
        print(f"  - ç‰¹å¾æŸå¤±: {feature_loss:.6f}, KLæŸå¤±: {kl_loss:.6f}")
        print(f"  - æ€»æŸå¤±: {total_loss:.6f}, æœŸæœ›: {expected_total:.6f}")

        # éªŒè¯è®¡ç®—æ­£ç¡®æ€§
        assert abs(total_loss - expected_total) < 1e-6
        print("  - âœ“ æƒé‡è®¡ç®—æ­£ç¡®")


def test_distillation_sequence_length_variations():
    """æµ‹è¯•ä¸åŒåºåˆ—é•¿åº¦ç»„åˆ"""
    print("\n=== æµ‹è¯•ä¸åŒåºåˆ—é•¿åº¦ç»„åˆ ===")

    D, V = 256, 40

    # æµ‹è¯•ä¸åŒçš„åºåˆ—é•¿åº¦ç»„åˆ
    length_combinations = [
        (5, 3),   # æ•™å¸ˆé•¿ï¼Œå­¦ç”ŸçŸ­
        (8, 8),   # ç­‰é•¿
        (3, 5),   # æ•™å¸ˆçŸ­ï¼Œå­¦ç”Ÿé•¿
        (12, 4),  # å·®å¼‚è¾ƒå¤§
        (20, 15), # å®é™…åœºæ™¯
    ]

    for teacher_len, student_len in length_combinations:
        B = 2
        teacher_features = torch.randn(B, teacher_len, D)
        student_features = torch.randn(B, student_len, D, requires_grad=True)
        teacher_logits = torch.randn(B, teacher_len, V)
        student_logits = torch.randn(B, student_len, V, requires_grad=True)

        distill_loss_fn = DistillationLoss(temperature=4.0)

        try:
            losses = distill_loss_fn(
                teacher_features=teacher_features,
                student_features=student_features,
                teacher_logits=teacher_logits,
                student_logits=student_logits
            )

            print(f"âœ“ æ•™å¸ˆé•¿åº¦={teacher_len}, å­¦ç”Ÿé•¿åº¦={student_len}")
            print(f"  - ç‰¹å¾æŸå¤±: {losses['feature_loss'].item():.6f}")
            print(f"  - KLæŸå¤±: {losses['kl_loss'].item():.6f}")
            print(f"  - æ€»æŸå¤±: {losses['total_distill_loss'].item():.6f}")

            # éªŒè¯æ¢¯åº¦å­˜åœ¨
            total_loss = losses['total_distill_loss']
            total_loss.backward()

            if student_features.grad is not None and student_logits.grad is not None:
                print("  - âœ“ æ¢¯åº¦æ­£å¸¸")
            else:
                print("  - âœ— æ¢¯åº¦å¼‚å¸¸")

        except Exception as e:
            print(f"âœ— é•¿åº¦ç»„åˆ ({teacher_len}, {student_len}) å¤±è´¥: {e}")


def run_distillation_tests():
    """è¿è¡Œæ‰€æœ‰è’¸é¦æŸå¤±æµ‹è¯•"""
    print("å¼€å§‹çŸ¥è¯†è’¸é¦æŸå¤±å‡½æ•°æµ‹è¯•...\n")

    tests = [
        test_distillation_loss_basic,
        test_distillation_attention_alignment,
        test_distillation_temperature_effects,
        test_distillation_mask_functionality,
        test_distillation_gradient_stability,
        test_distillation_alpha_weights,
        test_distillation_sequence_length_variations,
    ]

    results = []
    for test in tests:
        try:
            result = test()
            results.append(result if result is not None else True)
            print("-" * 60)
        except Exception as e:
            print(f"æµ‹è¯• {test.__name__} å¤±è´¥: {e}")
            results.append(False)
            print("-" * 60)

    # æ€»ç»“ç»“æœ
    passed = sum(results)
    total = len(results)
    print(f"\nè’¸é¦æŸå¤±æµ‹è¯•æ€»ç»“: {passed}/{total} æµ‹è¯•é€šè¿‡")

    if passed == total:
        print("ğŸ‰ æ‰€æœ‰è’¸é¦æŸå¤±æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("âš ï¸  éƒ¨åˆ†è’¸é¦æŸå¤±æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°ã€‚")

    return passed == total

if __name__ == "__main__":
    # è¿è¡Œå¢å¼ºCTCæŸå¤±æµ‹è¯•
    run_all_tests()

    # è¿è¡ŒçŸ¥è¯†è’¸é¦æŸå¤±æµ‹è¯•
    run_distillation_tests()

    # è¿è¡Œæ–°å¢æµ‹è¯•
    test_char_focal_loss()
    print("-" * 50)
    test_adaptive_margin()
    print("-" * 50)
    test_temperature_annealing()
    print("-" * 50)
    test_temperature_parameter_effects()
    print("-" * 50)
    test_adaptive_eos_window()
    print("-" * 50)
    test_gradient_clip_thresholds()
    print("-" * 50)
    test_different_reduction_modes()
    print("-" * 50)
    test_numerical_stability_boundaries()
    print("-" * 50)
    test_combined_optimizations()
