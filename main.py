"""
é‡åŒ–è®­ç»ƒä¸»å…¥å£ - æ•´åˆæ‰€æœ‰é‡åŒ–åŠŸèƒ½
æä¾›å®Œæ•´çš„é‡åŒ–è®­ç»ƒæµç¨‹
"""
import argparse
import os
import math
import random
import torch
import torch.nn as nn
from typing import Dict, Optional
import json
from pathlib import Path
from timm.scheduler import create_scheduler_v2
from torch.amp import GradScaler, autocast
from tqdm import tqdm

from model import RecNetwork
from loss import RecognitionLoss
from data import VOCAB_SIZE, idx2char, char2idx, blank_id, sos_id, eos_id, VOCAB, OTHER_PAD_SIZE, CONFUSE_WEIGHT_OPTIMIZED, create_dataset, create_dataset_splitted
from debug import debug_virtual_alignment
from deployment import create_deployment_package, ctc_decode_v2, cer_score, exact_match
from quantization import (
    QUANTIZATION_CONFIG,
    QuantizationStrategy, QuantizationManager,
    QuantizationConfig, AdaptiveQuantizationConfig,
    QuantizationEvaluator, QuantizationMetrics,
    create_optimal_config, get_config_template, create_optimization_study,
    PruningManager, create_pruning_config
)


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='OCRé‡åŒ–æ„ŸçŸ¥è®­ç»ƒå·¥å…·')

    # åŸºç¡€é…ç½®
    parser.add_argument('--config', default='quantization_config.json', type=str, help='é‡åŒ–é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--template', type=str, choices=[
        'ocr_conservative', 'ocr_balanced', 'ocr_aggressive',
        'mobile_optimized', 'server_optimized'
    ], default='ocr_balanced', help='ä½¿ç”¨é¢„å®šä¹‰é…ç½®æ¨¡æ¿')

    # è®­ç»ƒé…ç½®
    parser.add_argument('--mode', type=str, choices=['train', 'evaluate', 'deployment', 'optimization_study', 'both'],
                       default='both', help='è¿è¡Œæ¨¡å¼')
    parser.add_argument('--model_name', type=str, default='viptr2', help='è®­ç»ƒæ¨¡å‹åç§°')
    parser.add_argument('--epochs', type=int, default=60, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=512, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--learning_rate', type=float, default=3e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--train_mode', type=str, choices=['ctc', 'ar', 'hybrid'],
                       default='ctc', help='è®­ç»ƒæ¨¡å¼: ctc ä»…è®­ç»ƒCTCè§£ç å¤´ ar ä»…è®­ç»ƒè‡ªå›å½’è§£ç å™¨ hybrid åŒæ—¶è®­ç»ƒä¸¤ä¸ªè§£ç å¤´')
    parser.add_argument('--num_layers', type=int, default=3, help='ç½‘ç»œå±‚æ•°')
    parser.add_argument('--num_heads', type=int, default=6, help='æ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--d_model', type=int, default=384, help='æ¨¡å‹éšè—ç»´åº¦')
    parser.add_argument('--in_channels', type=int, default=3, help='è¾“å…¥é€šé“æ•°')
    parser.add_argument('--max_text_length', type=int, default=70, help='æœ€å¤§æ–‡æœ¬é•¿åº¦')
    parser.add_argument('--dropout', type=float, default=0.05, help='dropoutæ¯”ä¾‹')
    parser.add_argument('--num_workers', type=int, default=0, help='æ•°æ®é›†åŠ è½½è¿›ç¨‹æ•°')
    parser.add_argument('--warmup_lr', type=int, default=3, help='å‰ N è½®çº¿æ€§å¢å¤§å­¦ä¹ ç‡')
    parser.add_argument('--warmup_decoder', type=int, default=10, help='å‰ N è½®ä¸å¼€å¯è’¸é¦')

    # æ•°æ®é…ç½®
    parser.add_argument('--num_train', type=int, default=95000, help='è®­ç»ƒæ ·æœ¬æ•°')
    parser.add_argument('--num_val', type=int, default=5000, help='éªŒè¯æ ·æœ¬æ•°')
    parser.add_argument('--img_height', type=int, default=32, help='å›¾åƒé«˜åº¦')
    parser.add_argument('--img_min_width', type=int, default=128, help='å›¾åƒæœ€å°å®½åº¦')
    parser.add_argument('--img_export_width', type=int, default=512, help='å›¾åƒæœ€å°å®½åº¦')
    parser.add_argument('--min_chars', type=int, default=15, help='æœ€å°‘å­—ç¬¦æ•°')
    parser.add_argument('--max_chars', type=int, default=50, help='æœ€å¤šå­—ç¬¦æ•°')
    parser.add_argument('--dataset_type', type=str, choices=['synthetic', 'lmdb'], default='synthetic', help='æ•°æ®é›†ç±»å‹')
    parser.add_argument('--lmdb_data_dir', type=str, default=None, help='LMDBæ•°æ®é›†ç›®å½•è·¯å¾„')
    parser.add_argument('--key_img_prefix', type=str, default='img-', help='LMDBå›¾åƒkeyå‰ç¼€')
    parser.add_argument('--key_label_prefix', type=str, default='label-', help='LMDBæ ‡ç­¾keyå‰ç¼€')

    # æ¨¡å‹ä¿å­˜é…ç½®
    parser.add_argument('--checkpoint', type=str, default='ocr_latest.pth', help='æ–­ç‚¹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--checkpoint_cer', type=str, default='ocr_best_cer.pth', help='æœ€ä½³CERæ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--checkpoint_em', type=str, default='ocr_best_em.pth', help='æœ€ä½³EMæ¨¡å‹æ–‡ä»¶è·¯å¾„')

    # é‡åŒ–é…ç½®
    parser.add_argument('--enable_quantization', action='store_true', default=False,
                       help='å¯ç”¨é‡åŒ–è®­ç»ƒ')
    parser.add_argument('--quantization_mode', type=str, choices=['qat', 'ptq', 'both'], default='qat',
                       help='é‡åŒ–æ¨¡å¼: qat é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ, ptq è®­ç»ƒåé‡åŒ–, both ä¸¤è€…éƒ½å¯ç”¨')
    parser.add_argument('--quantization_strategy', type=str,
                       choices=['int8_dyn_act_int4_weight', 'int8_weight_only',
                               'int4_weight_only', 'int8_dynamic_activation_int8_weight'],
                       default='int8_dyn_act_int4_weight', help='é‡åŒ–ç­–ç•¥')
    parser.add_argument('--qat_epochs', type=int, default=5, help='QATè®­ç»ƒè½®æ•°')
    parser.add_argument('--weight_bits', type=int, default=4, help='æƒé‡é‡åŒ–ä½æ•°')
    parser.add_argument('--activation_bits', type=int, default=8, help='æ¿€æ´»é‡åŒ–ä½æ•°')

    # é‡åŒ–ä¼˜åŒ–ç ”ç©¶é…ç½®
    parser.add_argument('--study_name', type=str, help='ä¼˜åŒ–ç ”ç©¶åç§°')
    parser.add_argument('--method', type=str, default='bayesian',
                       choices=['bayesian', 'grid_search', 'random_search'],
                       help='ä¼˜åŒ–æ–¹æ³•')
    parser.add_argument('--n_trials', type=int, default=50,
                       help='ä¼˜åŒ–è¯•éªŒæ¬¡æ•°')
    parser.add_argument('--param_config', type=str, help='å‚æ•°é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--optimization_target', type=str, default='balanced',
                       choices=['balanced', 'accuracy', 'compression', 'speed'],
                       help='ä¼˜åŒ–ç›®æ ‡')
    parser.add_argument('--dry_run', action='store_true', default=False,
                       help='æ˜¯å¦åªéªŒè¯ä»£ç æµç¨‹ï¼Œä¸æ‰§è¡Œå®é™…ä¼˜åŒ–')

    # ç¡¬ä»¶é…ç½®
    parser.add_argument('--device', type=str, default='auto',
                       choices=['auto', 'cpu', 'cuda'], help='è®¡ç®—è®¾å¤‡')
    parser.add_argument('--hardware_target', type=str, default='cpu',
                       choices=['cpu', 'gpu', 'mobile'], help='ç›®æ ‡ç¡¬ä»¶å¹³å°')

    # è¾“å‡ºé…ç½®
    parser.add_argument('--output_dir', type=str, default='output',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--save_models', action='store_true', default=True,
                       help='ä¿å­˜æ¨¡å‹æ–‡ä»¶')
    parser.add_argument('--generate_report', action='store_true', default=True,
                       help='ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š')
    parser.add_argument('--visualize', action='store_true', default=True,
                       help='ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨')

    # å‰ªæé…ç½®
    parser.add_argument('--enable_pruning', action='store_true', default=False,
                       help='å¯ç”¨æ¨¡å‹å‰ªæ')
    parser.add_argument('--pruning_strategy', type=str,
                       choices=['l1_unstructured', 'l1_structured', 'ln_structured'],
                       default='l1_unstructured', help='å‰ªæç­–ç•¥')
    parser.add_argument('--pruning_ratio', type=float, default=0.3,
                       help='å…¨å±€å‰ªææ¯”ä¾‹')
    parser.add_argument('--pruning_layers', type=str, nargs='+',
                       default=['backbone', 'neck', 'decoder'], help='éœ€è¦å‰ªæçš„å±‚')
    parser.add_argument('--pruning_epoch', type=int, default=20,
                       help='å‰ªææ‰§è¡Œçš„epoch')
    parser.add_argument('--min_acc_drop', type=float, default=0.01,
                       help='å…è®¸çš„æœ€å¤§ç²¾åº¦ä¸‹é™')
    parser.add_argument('--finetune_epochs', type=int, default=10,
                       help='å‰ªæåçš„å¾®è°ƒè½®æ•°')
    parser.add_argument('--prune_criteria', type=str, choices=['l1', 'l2', 'grad'],
                       default='l1', help='å‰ªææ ‡å‡†')

    # åˆ†å±‚å‰ªææ¯”ä¾‹
    parser.add_argument('--backbone_pruning_ratio', type=float, default=0.2,
                       help='Backboneå‰ªææ¯”ä¾‹')
    parser.add_argument('--neck_pruning_ratio', type=float, default=0.3,
                       help='Neckå‰ªææ¯”ä¾‹')
    parser.add_argument('--decoder_pruning_ratio', type=float, default=0.1,
                       help='Decoderå‰ªææ¯”ä¾‹')

    # é«˜çº§é…ç½®
    parser.add_argument('--deployment_target', type=str, default='server_cpu',
                       choices=['mobile_cpu', 'mobile_gpu', 'edge_tpu', 'server_cpu', 'server_gpu'], help='éƒ¨ç½²ç›®æ ‡')
    parser.add_argument('--auto_optimize', action='store_true', default=True,
                       help='è‡ªåŠ¨ä¼˜åŒ–é‡åŒ–é…ç½®')
    parser.add_argument('--target_compression_ratio', type=float, default=0.25,
                       help='ç›®æ ‡å‹ç¼©æ¯”')
    parser.add_argument('--preserve_accuracy', action='store_true', default=True,
                       help='ä¼˜å…ˆä¿æŒç²¾åº¦')

    return parser.parse_args()

def count_params(model, name):
    '''ç»Ÿè®¡èŠ‚ç‚¹å‚æ•°é‡'''
    c = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f'{name:20s}: {c/1e6:6.2f} M')
    return c

@torch.no_grad()
def calculate_metrics(batch: Dict[str, torch.Tensor], logits: Dict[str, torch.Tensor], labels: torch.Tensor, use_ctc: bool):
    '''è®¡ç®— CER å’Œ Exact-Match æŒ‡æ ‡'''
    em_cnt = 0
    running_cer = 0.0
    skip_tokens = [blank_id, sos_id, eos_id]
    tgt_len = batch['text_lengths'].tolist()

    if use_ctc:
        # ç®€å•å»é‡/å» blank è§£ç 
        pred    = logits['ctc_logits'].argmax(dim=-1).cpu().numpy()               # CTC logit  [B,L,V]
        for p,t,l in zip(pred, labels, tgt_len):
            pp = ctc_decode_v2(p, skip_tokens)
            pred_decoded = torch.tensor(pp, dtype=torch.long)
            running_cer += cer_score(pp, t, l, blank_id, sos_id, eos_id, idx2char)
            em_cnt += exact_match(pred_decoded, t, l, blank_id, eos_id)
    else:
        pred   = logits['ar_logits'].argmax(dim=-1)              # AR logit  [B,L,V]
        for p, t, l in zip(pred, labels[:, 1:], tgt_len):
            running_cer += cer_score(p, t, l, blank_id, sos_id, eos_id, idx2char)
            em_cnt += exact_match(p, t, l, blank_id, eos_id)

    return running_cer, em_cnt

@torch.no_grad()
def validate(device: str, model: torch.nn.Module, loader, criterion, epoch: int, train_mode: str, model_name: str, output_dir: str, max_text_length: int, dtype=torch.float16):
    model.eval()
    use_ctc = True if train_mode == 'ctc' or train_mode == 'hybrid' else False
    running_loss, running_cer, running_em, samples = 0.0, 0.0, 0, 0
    pbar = tqdm(loader, desc='Validate')

    for batch in pbar:
        images = batch['images'].to(device, non_blocking=True)
        labels = batch['labels'].to(device, non_blocking=True)
        label_lens = batch['label_lens'].to(device, non_blocking=True)
        labels_ce = batch['labels_ce'].to(device, non_blocking=True)

        with autocast(device, dtype=dtype):
            logits = model(images)               # BÃ—TÃ—V
            loss = criterion(logits, labels, label_lens, targets_ce=labels_ce)

        if use_ctc:
            cer, em_cnt = calculate_metrics(batch, logits, batch['labels'], use_ctc)
        else:
            cer, em_cnt = calculate_metrics(batch, logits, labels_ce, use_ctc)

        bs = images.size(0)
        running_cer += cer
        running_em += em_cnt
        samples += bs
        running_loss += loss['total_loss'].item() * bs
        pbar.set_postfix(loss=f'{running_loss/samples:.4f}',
                         cer=f'{running_cer/samples:.4f}',
                         acc=f'{running_em/samples:.2%}')

    debug_virtual_alignment(device, model, loader, epoch, model_name, train_mode, output_dir,
                          blank_id=blank_id, eos_id=eos_id, idx2char=idx2char, sos_id=sos_id,
                          max_length=max_text_length)

    return running_loss/samples, running_cer/samples, running_em/samples

def train_one_epoch(device: str, model: torch.nn.Module, loader, criterion, optimizer, scaler, epoch: int, train_mode: str, eval_rate: float, dtype=torch.float16):
    model.train()
    running_loss, ctc_loss, ar_loss, kl_loss, distill_loss, feature_loss, quantization_loss, running_cer, running_em, samples = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0
    pbar = tqdm(loader, desc=f'Train E{epoch}')
    use_ctc = True if train_mode == 'ctc' or train_mode == 'hybrid' else False
    eval_step = set(sorted(random.sample(range(0, len(loader)), k=int(eval_rate * len(loader)))))

    for idx, batch in enumerate(pbar):
        eval_mode = idx in eval_step
        images = batch['images'].to(device, non_blocking=True)
        labels = batch['labels'].to(device, non_blocking=True)
        label_lens = batch['label_lens'].to(device, non_blocking=True)
        labels_ce = batch['labels_ce'].to(device, non_blocking=True)

        ar_mask = (labels_ce != blank_id) & (labels_ce != sos_id) & (labels_ce != eos_id)
        with autocast(device, dtype=dtype):
            # å­˜å‚¨è¾“å…¥ç‰¹å¾ç”¨äºé‡åŒ–æŸå¤±è®¡ç®—
            logits = model(images, labels_ce, epoch=epoch, eval_mode=eval_mode)
            logits['input_features'] = images

            loss = criterion(logits, labels, label_lens, targets_ce=labels_ce, mask=ar_mask)

        scaler.scale(loss['total_loss']).backward()
        scaler.step(optimizer)
        scaler.update()
        total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1e3)
        optimizer.zero_grad(set_to_none=True)

        if use_ctc:
            cer, em_cnt = calculate_metrics(batch, logits, batch['labels'], use_ctc)
        else:
            cer, em_cnt = calculate_metrics(batch, logits, labels_ce, use_ctc)

        bs = images.size(0)
        running_cer += cer
        running_em += em_cnt
        samples += bs
        running_loss += loss['total_loss'].item() * bs
        ctc_loss += loss.get('ctc_loss', torch.tensor(0.0)).item() * bs
        ar_loss += loss.get('ar_loss', torch.tensor(0.0)).item() * bs
        kl_loss += loss.get('kl_loss', torch.tensor(0.0)).item() * bs
        distill_loss += loss.get('total_distill_loss', torch.tensor(0.0)).item() * bs
        feature_loss += loss.get('feature_loss', torch.tensor(0.0)).item() * bs
        quantization_loss += loss.get('quantization_loss', torch.tensor(0.0)).item() * bs

        pbar.set_postfix(loss=f'{running_loss/samples:.4f}',
                         ctc_loss=f'{ctc_loss/samples:.4f}',
                         ar_loss=f'{ar_loss/samples:.4f}',
                         kl_loss=f'{kl_loss/samples:.4f}',
                         distill_loss=f'{distill_loss/samples:.4f}',
                         feature_loss=f'{feature_loss/samples:.4f}',
                         quantization_loss=f'{quantization_loss/samples:.4f}',
                         cer=f'{running_cer/samples:.4f}',
                         acc=f'{running_em/samples:.2%}',
                         norm=f'{total_norm:.2f}')
    return running_loss/samples, running_cer/samples, running_em/samples

def find_scaler_cfg(device, model, loader, criterion, optimizer):
    '''æœç´¢æœ€ä½³æ¢¯åº¦ç¼©æ”¾å‚æ•°'''
    # æ£€æŸ¥æ˜¾å¡æ˜¯å¦æ”¯æŒbf16æ•°æ®ç±»å‹
    use_bf16 = False
    if torch.cuda.is_available():
        use_bf16 = torch.cuda.is_bf16_supported()

    dtype = torch.bfloat16 if use_bf16 and device == 'cuda' else torch.float16

    candidates = [
        (64,  100),
        (128, 200),
        (256, 400),
        (512, 800),
        (1024, 1600),
    ]
    for init, grow in candidates:
        scaler = torch.amp.GradScaler(init_scale=init,
                                         growth_interval=grow,
                                         backoff_factor=0.5,
                                         growth_factor=2,
                                         enabled=True)
        model.train()
        overflows = 0
        for step, batch in enumerate(loader):
            images = batch['images'].to(device, non_blocking=True)
            labels = batch['labels'].to(device, non_blocking=True)
            label_lens = batch['label_lens'].to(device, non_blocking=True)

            with torch.amp.autocast(device, dtype=dtype):
                out = model(images, labels)
                loss = criterion(out, labels, label_lens)['total_loss']

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scale_before = scaler.get_scale()
            scaler.update()
            total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 5)
            scale_after = scaler.get_scale()
            if scale_after < scale_before:
                overflows += 1
            optimizer.zero_grad(set_to_none=True)

            if step >= 200:
                break
        print(f'init={init:3d} growth={grow:3d} | '
              f'overflows={overflows:3d} | '
              f'final_scale={scaler.get_scale()} | '
              f'total_norm={total_norm}')

def get_quantization_scheduler(epoch: int, config: Dict) -> float:
    """è·å–é‡åŒ–ç›¸å…³çš„å­¦ä¹ ç‡è°ƒåº¦å› å­"""
    if not config['enabled']:
        return 1.0

    # QATé˜¶æ®µä½¿ç”¨è¾ƒä½çš„å­¦ä¹ ç‡
    if epoch < config['qat_epochs']:
        return config['qat_learning_rate_multiplier']
    else:
        return 1.0

def prepare_model(args, device: str, checkpoint) -> RecNetwork:
    """å‡†å¤‡å¹¶åˆ›å»ºOCRæ¨¡å‹

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°ï¼ŒåŒ…å«æ¨¡å‹é…ç½®ä¿¡æ¯
        device: æ¨¡å‹è¿è¡Œè®¾å¤‡
        checkpoint: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œå¦‚æœå­˜åœ¨åˆ™åŠ è½½é¢„è®­ç»ƒå‚æ•°

    Returns:
        åˆ›å»ºå¹¶åˆå§‹åŒ–çš„ RecNetwork æ¨¡å‹
    """
    model = RecNetwork(
        train_mode=args.train_mode,
        model_name=args.model_name,
        num_classes=VOCAB_SIZE,
        in_channels=args.in_channels,
        num_layers=args.num_layers,
        num_heads=args.num_heads,
        max_text_length=args.max_text_length,
        hidden_dim=args.d_model,
        dropout=args.dropout,
        pad_token=blank_id,
        sos_token=sos_id,
        eos_token=eos_id,
    ).to(device)

    if os.path.exists(checkpoint):
        ckpt = torch.load(checkpoint, map_location=device, weights_only=False)
        # ä½¿ç”¨strict=FalseåŠ è½½æ¨¡å‹æƒé‡ï¼Œå¿½ç•¥å‰ªæç›¸å…³çš„ä¸åŒ¹é…é”®
        model.load_state_dict(ckpt['model'], strict=False)
    else:
        ckpt = None

    return model, ckpt

def train_quantized_main(args, quantization_manager: QuantizationManager, quantization_config: Dict, model: nn.Module, ckpt, device: str, output_dir: str, pruning_manager: PruningManager, confuse_weight_dict: Optional[Dict[int, float]]):
    # æ£€æŸ¥æ˜¾å¡æ˜¯å¦æ”¯æŒbf16æ•°æ®ç±»å‹
    use_bf16 = False
    if torch.cuda.is_available():
        use_bf16 = torch.cuda.is_bf16_supported()

    print(f"ğŸ’¡ bf16æ”¯æŒ: {'æ˜¯' if use_bf16 else 'å¦'}")

    # å·²æœ‰ ampï¼Œå†æ‰“å¼€ï¼š
    # ä½¿ç”¨æ–°çš„APIè®¾ç½®æ§åˆ¶TF32è¡Œä¸ºï¼Œé¿å…PyTorch 2.9åçš„å¼ƒç”¨è­¦å‘Š
    # if torch.cuda.is_available():
    #     torch.backends.cudnn.conv.fp32_precision = 'tf32'  # å¯¹äºCUDAå·ç§¯æ“ä½œ
    #     torch.backends.cuda.matmul.fp32_precision = 'tf32'  # å¯¹äºCUDAçŸ©é˜µä¹˜æ³•æ“ä½œ

    # ä½¿ç”¨é»˜è®¤é…ç½®å¦‚æœæ²¡æœ‰æä¾›
    if quantization_config is None:
        quantization_config = QUANTIZATION_CONFIG

    # 1. åˆ›å»ºDataLoader
    train_loader, val_loader = create_dataset_splitted(
        dataset_type=args.dataset_type,
        data_dir=args.lmdb_data_dir,
        num_train=args.num_train,
        num_val=args.num_val,
        img_height=args.img_height,
        min_width=args.img_min_width,
        min_chars=args.min_chars,
        max_chars=args.max_chars,
        key_img_prefix=args.key_img_prefix,
        key_label_prefix=args.key_label_prefix,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        pin_memory=True if device == 'cuda' else False,
        persistent_workers=True if device == 'cuda' else False,
    )

    # åŸºç¡€æŸå¤±å‡½æ•°
    criterion = RecognitionLoss(
        max_epoch=args.epochs,
        vocab_size=VOCAB_SIZE,
        ignore_index=blank_id,
        ctc_weight=2.0,
        ar_weight=1.0,
        distill_weight=0.2,
        distill_start_epoch=args.warmup_decoder,
        confuse_weight_dict=confuse_weight_dict,
        quantization_manager=quantization_manager
    ).to(device)

    param_groups = [
        # 1. Backbone
        {"params": model.backbone.parameters(), "lr": args.learning_rate * 0.3},
        # 2. Neck
        {"params": model.neck.parameters(), "lr": args.learning_rate * 1.0},
        # 3. Align
        {"params": model.decoder.feature_align.parameters(), "lr": args.learning_rate * 1.2},
        # 4. CTCHead
        {"params": model.decoder.ctc_decoder.parameters(), "lr": args.learning_rate * 2.0},
        # 5. ArHead
        {"params": model.decoder.ar_decoder.parameters(), "lr": args.learning_rate * 2.0},
    ]

    optimizer = torch.optim.AdamW(param_groups, weight_decay=1e-4)
    scheduler, _ = create_scheduler_v2(
        optimizer,
        sched = 'cosine',
        num_epochs = args.epochs,
        warmup_epochs = args.warmup_lr,
        min_lr = 1e-6
    )

    # æœç´¢æœ€ä½³æ¢¯åº¦ç¼©æ”¾å‚æ•°ï¼Œå½“å‰ 0 overflow å·²è¯´æ˜ 64~1024 åŒºé—´éƒ½å®‰å…¨ï¼Œå†å¾€ä¸Šæµ‹ 2048 æ„ä¹‰ä¸å¤§
    # find_scaler_cfg(device, model, train_loader, criterion, optimizer)
    scaler = GradScaler(init_scale=1024, growth_interval=100, enabled=True)

    # è®¾ç½®autocastè®¾å¤‡ç±»å‹ï¼Œæ”¯æŒbf16åˆ™ä½¿ç”¨bf16-mixedç²¾åº¦
    dtype = torch.bfloat16 if use_bf16 and device == 'cuda' else torch.float16

    total = 0
    total += count_params(model.backbone,   'Backbone')
    total += count_params(model.neck,   'Neck')
    total += count_params(model.decoder.feature_align,   'Align')
    total += count_params(model.decoder.ctc_decoder,   'CTCHead')
    total += count_params(model.decoder.ar_decoder,   'ArHead')
    print('-'*40)
    print(f'Total Trainable: {total/1e6:6.2f} M')

    # 3. æ–­ç‚¹ç»­è®­
    start_epoch = 0
    if ckpt:
        optimizer.load_state_dict(ckpt['opt'])
        scaler.load_state_dict(ckpt['scaler'])
        start_epoch = ckpt['epoch'] + 1
        print(f'ğŸ”¥ ç»­è®­ epoch {start_epoch}')

    # å¦‚æœéœ€è¦PTQï¼Œå…ˆè¿›è¡Œæ ¡å‡†
    if quantization_config['enabled'] and quantization_config['post_training_quantization']:
        print("ğŸ”§ å¼€å§‹è®­ç»ƒåé‡åŒ–æ ¡å‡†...")
        quantization_manager.calibrate_model(train_loader)
        print("âœ… PTQæ ¡å‡†å®Œæˆ")

    eval_rate = 0.0
    current_lr = optimizer.param_groups[0]['lr']
    best_cer, best_em, best_em_save = 1.0, 0.0, 0.0
    patience_loss, patience_em = 10, 10   # è¿ç»­ä¸æ”¹å–„è½®æ•°
    best_train_loss = float('inf')
    counter_loss, counter_em = 0, 0

    # 4. è®­ç»ƒ
    success = True
    try:
        # model = torch.compile(model, mode='max-autotune')   # è®­ç»ƒæ­¥ 10-15 % æé€Ÿ
        dummy_input = torch.randn(1, 3, args.img_height, args.img_min_width).to(device)
        for epoch in range(start_epoch, args.epochs):
            print(f'\n----- Epoch {epoch} -- LR {current_lr:.6f} -- scale {scaler.get_scale()} -----')

            # è°ƒæ•´å­¦ä¹ ç‡
            lr_multipliers = []

            # QATé˜¶æ®µå­¦ä¹ ç‡è°ƒæ•´
            qat_lr_multiplier = get_quantization_scheduler(epoch, quantization_config)
            lr_multipliers.append(qat_lr_multiplier)

            # å‰ªæå¾®è°ƒé˜¶æ®µå­¦ä¹ ç‡è°ƒæ•´
            finetune_lr_multiplier = pruning_manager.get_finetune_lr_multiplier(epoch)
            lr_multipliers.append(finetune_lr_multiplier)

            # æ€»å­¦ä¹ ç‡ä¹˜æ•°
            total_lr_multiplier = 1.0
            for multiplier in lr_multipliers:
                total_lr_multiplier *= multiplier

            # åº”ç”¨å­¦ä¹ ç‡è°ƒæ•´
            for param_group in optimizer.param_groups:
                param_group['lr'] = param_group['lr'] * total_lr_multiplier

            train_loss, train_cer, train_em = train_one_epoch(device, model, train_loader,
                                                            criterion, optimizer, scaler, epoch, args.train_mode, eval_rate, dtype=dtype)
            val_loss, val_cer, val_em = validate(device, model, val_loader, criterion, epoch, args.train_mode, args.model_name, output_dir, args.max_text_length, dtype=dtype)

            criterion.schedule(epoch)
            scheduler.step(epoch)
            current_lr = optimizer.param_groups[0]['lr']

            # è®­ç»ƒæ¨¡å¼æ­£ç¡®ç‡è¾¾åˆ° 60% å¼€å§‹è®©è®­ç»ƒèµ°æ¨ç†åˆ†æ”¯ï¼Œè§£å†³æ¨¡å‹è®­ç»ƒæ—¶ä¸çŸ¥é“è‡ªå›å½’æ¨ç†é”™è¯¯å­—ç¬¦
            if train_em > 0.6:
                eval_rate = 0.3

            # ---------- 1. è®­ç»ƒ loss å¼‚å¸¸ ----------
            if not math.isfinite(train_loss) or not math.isfinite(val_loss):
                success = False
                print('âŒ Train | Val loss å¼‚å¸¸ï¼Œç«‹å³ç»ˆæ­¢è®­ç»ƒ')
                break

            # ---------- 2. ç›‘æ§è®­ç»ƒæŒ‡æ ‡ä¸æå‡å°±æ—©åœ ----------
            # ---------- Train loss 10 è½®ä¸ä¸‹é™ ----------
            if train_loss < best_train_loss - 1e-5:
                best_train_loss = train_loss
                counter_loss = 0
            elif epoch > 5:
                counter_loss += 1
                if counter_loss >= patience_loss:
                    print(f'âš ï¸  Train loss è¿ç»­ {patience_loss}  epoch æ— ä¸‹é™ï¼Œæå‰åœæ­¢')
                    break

            # ---------- Val_EM 10 è½®ä¸æ¶¨ ----------
            if val_em > best_em + 1e-5:
                best_em = val_em
                counter_em = 0
            elif epoch > 5:
                counter_em += 1
                if counter_em >= patience_em:
                    print(f'âš ï¸  Val_EM è¿ç»­ {patience_em}  epoch æ— æå‡ï¼Œæå‰åœæ­¢')
                    break

            # ---------- 3. ä¿å­˜æ¨¡å‹ ----------
            quantization_manager.export_quantized_model(pruning_manager.pruning_applied, f'{output_dir}/models/{args.checkpoint}', epoch, best_cer, best_em,
                                                        dummy_input, optimizer.state_dict(), scaler.state_dict(), model)
            if val_cer > 0 and val_cer < best_cer:
                best_cer = val_cer
                quantization_manager.export_quantized_model(pruning_manager.pruning_applied, f'{output_dir}/models/{args.checkpoint_cer}', epoch, best_cer, best_em,
                                                            dummy_input, optimizer.state_dict(), scaler.state_dict(), model)
                print(f'âœ… æœ€ä½³ CER æ¨¡å‹å·²ä¿å­˜  CER={best_cer:.2%}')
            if val_em > best_em_save:      # ä¸ä¸Šé¢ best_em åˆ†å¼€ï¼Œé¿å…æå‰åœæ­¢å¹²æ‰°
                best_em_save = val_em
                quantization_manager.export_quantized_model(pruning_manager.pruning_applied, f'{output_dir}/models/{args.checkpoint_em}', epoch, best_cer, best_em,
                                                            dummy_input, optimizer.state_dict(), scaler.state_dict(), model)
                print(f'ğŸš€ æœ€ä½³ Exact-Match æ¨¡å‹å·²ä¿å­˜  EM={best_em:.2%}')

            # ---------- 4. Val_EM è¾¾æ ‡é€€å‡ºè®­ç»ƒ ----------
            if val_em >= 0.992:
                print('ğŸ‰  Val_EM è¾¾åˆ° 99.2 %ï¼Œè®­ç»ƒæå‰å®Œæˆ')
                break

            # æ£€æŸ¥æ˜¯å¦éœ€è¦è®°å½•å‰ªæå€™é€‰èŠ‚ç‚¹
            if pruning_manager.is_pruning_time(epoch):
                # è®°å½•å‰ªæå€™é€‰èŠ‚ç‚¹ï¼ˆéƒ¨ç½²æ—¶åº”ç”¨ï¼‰
                pruning_manager.record_pruning_candidates(epoch, val_em, best_em)

    except Exception as e:
        print(f'âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}')
        success = False

    print('\nè®­ç»ƒç»“æŸï¼ˆå¯èƒ½æå‰åœæ­¢ï¼‰')

    # ä¿å­˜å‰ªæå€™é€‰ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    if pruning_manager.pruning_candidates:
        candidates_path = f'{output_dir}/models/pruning_candidates.json'
        pruning_manager.save_pruning_candidates(candidates_path)

    return model, success

def setup_device(device: str) -> str:
    """è®¾ç½®è®¡ç®—è®¾å¤‡"""
    if device == 'auto':
        return 'cuda' if torch.cuda.is_available() else 'cpu'
    return device

def create_quantization_config(args, model: nn.Module, config_file: str) -> QuantizationConfig:
    """åˆ›å»ºé‡åŒ–é…ç½®"""

    # å¦‚æœæœ‰é…ç½®æ–‡ä»¶å°±åŠ è½½
    if os.path.exists(config_file):
        print(f"ğŸ“„ ä»æ–‡ä»¶åŠ è½½é‡åŒ–é…ç½®: {config_file}")
        config = QuantizationConfig.load_from_file(config_file)
    else:
        # ä½¿ç”¨é¢„å®šä¹‰æ¨¡æ¿æˆ–è‡ªåŠ¨ä¼˜åŒ–
        if args.auto_optimize:
            print("ğŸ¯ ä½¿ç”¨è‡ªåŠ¨ä¼˜åŒ–é…ç½®")
            config = create_optimal_config(
                model=model,
                task_type='ocr',
                hardware_target=args.hardware_target,
                target_compression_ratio=args.target_compression_ratio,
                preserve_accuracy=args.preserve_accuracy
            )
        else:
            print(f"ğŸ“‹ ä½¿ç”¨é¢„å®šä¹‰æ¨¡æ¿: {args.template}")
            config = get_config_template(args.template)

        # è¦†ç›–å‘½ä»¤è¡Œå‚æ•°
        config.enabled = args.enable_quantization
        config.strategy = QuantizationStrategy(args.quantization_strategy)
        config.qat_epochs = args.qat_epochs
        config.weight_bits = args.weight_bits
        config.activation_bits = args.activation_bits

    return config

def prepare_output_directory(args) -> str:
    """å‡†å¤‡è¾“å‡ºç›®å½•"""
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # åˆ›å»ºå­ç›®å½•
    (output_dir / 'models').mkdir(exist_ok=True)
    (output_dir / 'reports').mkdir(exist_ok=True)
    (output_dir / 'visualizations').mkdir(exist_ok=True)
    (output_dir / 'logs').mkdir(exist_ok=True)

    return str(output_dir)

def prepare_quantized_manage(args, config: QuantizationConfig, model: nn.Module, output_dir: str):
    """åˆå§‹åŒ–é‡åŒ–ç®¡ç†å™¨"""
    print("ğŸš€ å¼€å§‹å‡†å¤‡é‡åŒ–æ„ŸçŸ¥è®­ç»ƒå‚æ•°...")

    # è®¾ç½®è®­ç»ƒå‚æ•°
    train_config = {
        'IMG_H': args.img_height,
        'IMG_MIN_W': args.img_min_width,
        'MIN_CHARS': args.min_chars,
        'MAX_CHARS': args.max_chars,
        'BATCH_SIZE': args.batch_size,
        'NUM_EPOCHS': args.epochs,
        'LR': args.learning_rate,
        'NUM_TRAIN': args.num_train,
        'NUM_VAL': args.num_val,
        'NUM_LAYERS': args.num_layers,
        'NUM_HEADS': args.num_heads,
        'D_MODEL': args.d_model,
        'IN_CHANNELS': args.in_channels,
        'MAX_TEXT_LENGTH': args.max_text_length,
        'DROPOUT': args.dropout,
        'NUM_WORKERS': args.num_workers,
        'TRAIN_MODE': args.train_mode,
        'MODEL_NAME': args.model_name,
        'WARMUP_LR': args.warmup_lr,
        'WARMUP_DECODER': args.warmup_decoder,
        'CHECKPOINT': os.path.join(output_dir, 'models', 'ocr_latest.pth'),
        'CHECKPOINT_CER': os.path.join(output_dir, 'models', 'ocr_best_cer.pth'),
        'CHECKPOINT_EM': os.path.join(output_dir, 'models', 'ocr_best_em.pth'),
        'EXPORT_PATH': os.path.join(output_dir, 'models', 'quantized_ocr_model.pth'),
        'OUTPUT_DIR': output_dir,
    }

    # ä¿å­˜è®­ç»ƒé…ç½®
    if args.mode in ['train', 'both']:
        config_path = Path(output_dir) / 'train_config.json'
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(train_config, f, indent=2, ensure_ascii=False)

    # è¿è¡Œé‡åŒ–è®­ç»ƒ
    print("ğŸ“Š è®­ç»ƒé…ç½®:")
    print(json.dumps(config.to_dict(), indent=2, ensure_ascii=False))
    print("ğŸ“Š è®­ç»ƒå‚æ•°:")
    print(json.dumps(train_config, indent=2, ensure_ascii=False))

    # å°†QuantizationConfigè½¬æ¢ä¸ºå­—å…¸
    quantization_config = config.to_dict()
    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°è®¾ç½®é‡åŒ–æ¨¡å¼
    if args.quantization_mode == 'qat':
        quantization_config['quantization_aware_training'] = True
        quantization_config['post_training_quantization'] = False
    elif args.quantization_mode == 'ptq':
        quantization_config['quantization_aware_training'] = False
        quantization_config['post_training_quantization'] = True
    else:  # both
        quantization_config['quantization_aware_training'] = True
        quantization_config['post_training_quantization'] = True
    quantization_config['quantization_strategy'] = args.quantization_strategy
    quantization_config['weight_bits'] = args.weight_bits
    quantization_config['activation_bits'] = args.activation_bits
    quantization_config['quantization_granularity'] = 'per_channel'
    quantization_config['observer_type'] = 'moving_average'
    quantization_config['observer_momentum'] = 0.1

    # ä¿å­˜é…ç½®
    if args.mode in ['train', 'both']:
        config_path = Path(output_dir) / 'quantization_config.json'
        config.save_to_file(str(config_path))
        print(f"ğŸ’¾ é‡åŒ–é…ç½®å·²ä¿å­˜: {config_path}")

    print("âœ… åˆå§‹åŒ–é‡åŒ–ç®¡ç†å™¨...")
    quantization_manager = QuantizationManager(quantization_config, model)

    # åº”ç”¨é‡åŒ–
    model = quantization_manager.prepare_model_for_quantization()

    return quantization_manager, quantization_config, model

def evaluate_quantization(original_model: nn.Module, quantized_model: nn.Module,
                         args, config: QuantizationConfig, device: str, output_dir: str):
    """è¯„ä¼°é‡åŒ–æ•ˆæœ"""
    print("ğŸ” å¼€å§‹é‡åŒ–æ•ˆæœè¯„ä¼°...")

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = QuantizationEvaluator(original_model, quantized_model, device)

    # åˆ›å»ºæ•°æ®é›† - ä½¿ç”¨æ›´çœŸå®çš„é…ç½®
    print("ğŸ“Š å‡†å¤‡è¯„ä¼°æ•°æ®...")
    print(f"   - æ ·æœ¬æ•°é‡: {args.num_val}")
    print(f"   - å›¾åƒå°ºå¯¸: {args.img_height}x{args.img_min_width}")
    print(f"   - å­—ç¬¦èŒƒå›´: {args.min_chars}-{args.max_chars}")

    # åˆ›å»ºè¯„ä¼°DataLoader
    val_loader = create_dataset(
        dataset_type=args.dataset_type,
        data_dir=args.lmdb_data_dir,
        num_samples=args.num_val,
        img_height=args.img_height,
        min_width=args.img_min_width,
        min_chars=args.min_chars,
        max_chars=args.max_chars,
        key_img_prefix=args.key_img_prefix,
        key_label_prefix=args.key_label_prefix,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=True if device == 'cuda' else False,
        persistent_workers=True if device == 'cuda' else False,
    )

    # è¿è¡Œè¯„ä¼°
    print("ğŸ“Š è¿è¡Œå…¨é¢è¯„ä¼°...")
    print(f"   - è¯„ä¼°æ‰¹æ¬¡: {min(50, len(val_loader))}")
    print(f"   - è¯„ä¼°è®¾å¤‡: {device}")

    metrics = evaluator.evaluate_quantization(val_loader, num_batches=min(50, len(val_loader)))

    # ç”ŸæˆæŠ¥å‘Š
    if args.generate_report:
        report_path = Path(output_dir) / 'reports' / 'quantization_report.json'
        report = evaluator.generate_report(metrics, str(report_path))
        print(f"ğŸ“„ è¯„ä¼°æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

    # ç”Ÿæˆå¯è§†åŒ–
    if args.visualize:
        viz_path = Path(output_dir) / 'visualizations' / 'quantization_results.png'
        evaluator.visualize_results(metrics, str(viz_path))
        print(f"ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆ: {viz_path}")

    return metrics

def save_models(original_model: nn.Module, quantized_model: nn.Module,
                args, config: QuantizationConfig, output_dir: str):
    """ä¿å­˜æ¨¡å‹æ–‡ä»¶"""
    if not args.save_models:
        return

    models_dir = Path(output_dir) / 'models'

    # ä¿å­˜åŸå§‹æ¨¡å‹
    original_path = models_dir / 'original_model.pth'
    torch.save({
        'model_state_dict': original_model.state_dict(),
        'config': config.to_dict(),
        'model_type': 'original'
    }, original_path)
    print(f"ğŸ’¾ åŸå§‹æ¨¡å‹å·²ä¿å­˜: {original_path}")

    # ä¿å­˜é‡åŒ–æ¨¡å‹
    quantized_path = models_dir / 'quantized_model.pth'
    torch.save({
        'model_state_dict': quantized_model.state_dict(),
        'config': config.to_dict(),
        'model_type': 'quantized'
    }, quantized_path)
    print(f"ğŸ’¾ é‡åŒ–æ¨¡å‹å·²ä¿å­˜: {quantized_path}")

    # å¯¼å‡ºé‡åŒ–æ¨¡å‹ï¼ˆä½¿ç”¨torch.exportï¼‰
    try:
        dummy_input = torch.randn(1, 3, args.img_height, args.img_export_width).to(next(quantized_model.parameters()).device)

        # ä½¿ç”¨torch.exportå¯¼å‡ºé‡åŒ–æ¨¡å‹
        from torch.export import export

        # ç¡®ä¿æ¨¡å‹åœ¨è¯„ä¼°æ¨¡å¼
        quantized_model.eval()

        # å¯¼å‡ºæ¨¡å‹
        exported_program = export(quantized_model, (dummy_input,))

        # ä¿å­˜å¯¼å‡ºçš„ç¨‹åº
        exported_path = models_dir / 'quantized_model_exported.pt2'
        torch.save(exported_program, exported_path)
        print(f"ğŸ’¾ é‡åŒ–æ¨¡å‹å·²å¯¼å‡º (torch.export): {exported_path}")

    except Exception as e:
        print(f"âš ï¸  torch.exportå¯¼å‡ºå¤±è´¥: {e}")

    # ä¿å­˜ONNXæ ¼å¼ï¼ˆå¦‚æœå¯èƒ½ï¼‰
    try:
        dummy_input = torch.randn(1, 3, args.img_height, args.img_export_width)

        # åŸå§‹æ¨¡å‹ONNX
        onnx_original_path = models_dir / 'original_model.onnx'
        torch.onnx.export(
            original_model, dummy_input, str(onnx_original_path),
            input_names=['x'], output_names=['output'],
            dynamic_axes={'x': {0: 'batch_size'}, 'output': {0: 'batch_size'}}
        )
        print(f"ğŸ’¾ åŸå§‹æ¨¡å‹ONNXå·²ä¿å­˜: {onnx_original_path}")

        # é‡åŒ–æ¨¡å‹ONNX
        onnx_quantized_path = models_dir / 'quantized_model.onnx'
        torch.onnx.export(
            quantized_model, dummy_input, str(onnx_quantized_path),
            input_names=['x'], output_names=['output'],
            dynamic_axes={'x': {0: 'batch_size'}, 'output': {0: 'batch_size'}}
        )
        print(f"ğŸ’¾ é‡åŒ–æ¨¡å‹ONNXå·²ä¿å­˜: {onnx_quantized_path}")

    except Exception as e:
        print(f"âš ï¸  ONNXå¯¼å‡ºå¤±è´¥: {e}")

def print_summary(metrics: QuantizationMetrics, config: QuantizationConfig, output_dir: str):
    """æ‰“å°æ€»ç»“ä¿¡æ¯"""
    print("\n" + "="*60)
    print("ğŸ¯ é‡åŒ–è®­ç»ƒæ€»ç»“")
    print("="*60)

    print("ğŸ“Š ç²¾åº¦å½±å“:")
    print(f"  - åŸå§‹å‡†ç¡®ç‡: {metrics.original_accuracy:.4f}")
    print(f"  - é‡åŒ–å‡†ç¡®ç‡: {metrics.quantized_accuracy:.4f}")
    print(f"  - ç²¾åº¦ä¸‹é™: {metrics.accuracy_drop:.4f} ({metrics.accuracy_drop_ratio*100:.2f}%)")

    print("\nğŸ“ æ¨¡å‹å‹ç¼©:")
    print(f"  - åŸå§‹å¤§å°: {metrics.original_model_size_mb:.1f} MB")
    print(f"  - é‡åŒ–å¤§å°: {metrics.quantized_model_size_mb:.1f} MB")
    print(f"  - å‹ç¼©æ¯”: {metrics.compression_ratio:.2f}x")
    print(f"  - å¤§å°å‡å°‘: {metrics.size_reduction_ratio*100:.1f}%")

    print("\nâš¡ æ¨ç†åŠ é€Ÿ:")
    print(f"  - åŸå§‹æ¨ç†æ—¶é—´: {metrics.original_inference_time_ms:.2f} ms")
    print(f"  - é‡åŒ–æ¨ç†æ—¶é—´: {metrics.quantized_inference_time_ms:.2f} ms")
    print(f"  - åŠ é€Ÿæ¯”: {metrics.speedup_ratio:.2f}x")

    print("\nğŸ’¾ å†…å­˜ä¼˜åŒ–:")
    print(f"  - å†…å­˜å‡å°‘: {metrics.memory_reduction_ratio*100:.1f}%")

    print("\nğŸ”§ é‡åŒ–é…ç½®:")
    print(f"  - ç­–ç•¥: {config.strategy}")
    print(f"  - æƒé‡é‡åŒ–: {config.weight_bits}ä½")
    print(f"  - æ¿€æ´»é‡åŒ–: {config.activation_bits}ä½")
    print(f"  - QATè½®æ•°: {config.qat_epochs}")

    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print("="*60)

def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()

    # è®¾ç½®è®¾å¤‡
    success = True
    device = setup_device(args.device)
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")

    # å‡†å¤‡è¾“å‡ºç›®å½•
    output_dir = prepare_output_directory(args)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

    # åˆ›å»ºæ¨¡å‹
    print("ğŸ—ï¸  åˆ›å»ºOCRæ¨¡å‹...")
    checkpoint = f'{output_dir}/models/{args.checkpoint}'
    model, ckpt = prepare_model(args, device, checkpoint)

    # åˆ›å»ºé‡åŒ–é…ç½®
    config_file = Path(output_dir) / args.config
    config = create_quantization_config(args, model, str(config_file))
    print("âš™ï¸  é‡åŒ–é…ç½®åˆ›å»ºå®Œæˆ")

    # åˆå§‹åŒ–é‡åŒ–ç®¡ç†å™¨
    quantization_manager, quantization_config, model = prepare_quantized_manage(args, config, model, output_dir)

    # åˆ›å»ºå‰ªæé…ç½®å’Œç®¡ç†å™¨
    pruning_config = create_pruning_config(args)
    pruning_manager = PruningManager(pruning_config, model)
    print("ğŸŒ± å‰ªæé…ç½®åˆ›å»ºå®Œæˆ")

    # å¦‚æœè¿›ç¨‹æ•°é‡ä¸ºé›¶ä¸”æ˜¯CUDAè®­ç»ƒï¼Œå°†è¿›ç¨‹æ•°é‡è®¾ç½®ä¸ºCPUæ ¸å¿ƒæ•°
    if args.num_workers == 0 and device == 'cuda':
        args.num_workers = os.cpu_count()
        print(f"âš ï¸  æœªæŒ‡å®šè¿›ç¨‹æ•°ï¼Œè‡ªåŠ¨è®¾ç½®ä¸ºCPUæ ¸å¿ƒæ•°: {args.num_workers}")

    # è®­ç»ƒæ¨¡å¼
    if args.mode in ['train', 'both']:
        # è°ƒç”¨ä¸»è¦çš„è®­ç»ƒå‡½æ•°
        print("ğŸ¯ å¼€å§‹é‡åŒ–è®­ç»ƒ...")
        # å°†å­—ç¬¦ä¸ºé”®çš„æ··æ·†æƒé‡å­—å…¸è½¬æ¢ä¸ºç´¢å¼•ä¸ºé”®çš„å­—å…¸
        confuse_weight_dict_idx = {char2idx[char]: weight for char, weight in CONFUSE_WEIGHT_OPTIMIZED.items() if char in char2idx}
        quantized_model, success = train_quantized_main(
            args=args,
            quantization_manager=quantization_manager,
            quantization_config=quantization_config,
            model=model,
            ckpt=ckpt,
            device=device,
            output_dir=output_dir,
            confuse_weight_dict=confuse_weight_dict_idx,
            pruning_manager=pruning_manager
        )

        if quantized_model is None:
            # å¦‚æœè®­ç»ƒå‡½æ•°è¿”å›Noneï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹ä½œä¸ºé‡åŒ–æ¨¡å‹ï¼ˆé™çº§å¤„ç†ï¼‰
            print("âš ï¸  é‡åŒ–è®­ç»ƒå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹è¿›è¡Œè¯„ä¼°")
            quantized_model = model
        else:
            print("âœ… æˆåŠŸè·å¾—é‡åŒ–æ¨¡å‹")
    else:
        # è¯„ä¼°æ¨¡å¼ï¼Œéœ€è¦å…ˆåŠ è½½æˆ–åˆ›å»ºé‡åŒ–æ¨¡å‹
        print("âš ï¸  è¯„ä¼°æ¨¡å¼éœ€è¦å…ˆè®­ç»ƒæˆ–åŠ è½½é‡åŒ–æ¨¡å‹")
        quantized_model = model

    # è¯„ä¼°æ¨¡å¼
    if success and args.mode in ['evaluate', 'both']:
        # è¯„ä¼°é‡åŒ–æ•ˆæœ
        metrics = evaluate_quantization(model, quantized_model, args, config, device, output_dir)

        # ä¿å­˜æ¨¡å‹
        save_models(model, quantized_model, args, config, output_dir)

        # æ‰“å°æ€»ç»“
        print_summary(metrics, config, output_dir)

    # åˆ›å»ºéƒ¨ç½²
    if success and args.mode in ['deployment', 'both']:
        if args.enable_pruning:
            # æ£€æŸ¥æ˜¯å¦æœ‰å‰ªæå€™é€‰ä¿¡æ¯æ–‡ä»¶
            candidates_path = f'{output_dir}/models/pruning_candidates.json'
            if os.path.exists(candidates_path):
                # åŠ è½½å‰ªæå€™é€‰ä¿¡æ¯
                pruning_manager.load_pruning_candidates(candidates_path)

                # åº”ç”¨å‰ªæ
                pruning_manager.apply_pruning_from_candidates()
                pruning_manager.remove_pruning()

                # æ‰“å°å‰ªæä¿¡æ¯
                if pruning_manager.pruning_applied:
                    prune_info = pruning_manager.get_pruned_model_info()
                    print(f"\nğŸŒ³ å‰ªææ€»ç»“:")
                    print(f"   - å‰ªæåº”ç”¨: {'æ˜¯' if prune_info['pruning_applied'] else 'å¦'}")
                    print(f"   - å‰ªæå±‚æ•°é‡: {prune_info['pruned_layers_count']}")
                    print(f"   - å®é™…å‰ªææ¯”ä¾‹: {prune_info['pruning_ratio']:.2%}")
                    print(f"   - å‰ªæå±‚: {prune_info['pruned_layers'][:5]}..." if len(prune_info['pruned_layers']) > 5 else f"   - å‰ªæå±‚: {prune_info['pruned_layers']}")
            else:
                print(f"âš ï¸  æœªæ‰¾åˆ°å‰ªæå€™é€‰ä¿¡æ¯æ–‡ä»¶: {candidates_path}")

        dummy_input = torch.randn(3, 3, args.img_height, args.img_export_width)
        create_deployment_package(quantized_model, quantization_config, args.deployment_target, output_dir, dummy_input,
                                vocab=VOCAB, other_pad_size=OTHER_PAD_SIZE,
                                blank_id=blank_id, sos_id=sos_id, eos_id=eos_id,
                                idx2char=idx2char)

    # åˆ›å»ºéƒ¨ç½²
    if args.mode == 'optimization_study':
        # åˆ›å»ºDataLoader
        print("ğŸ“Š å‡†å¤‡ä¼˜åŒ–æ•°æ®é›†...")
        val_loader = create_dataset(
            dataset_type=args.dataset_type,
            data_dir=args.lmdb_data_dir,
            num_samples=args.num_val,
            img_height=args.img_height,
            min_width=args.img_min_width,
            min_chars=args.min_chars,
            max_chars=args.max_chars,
            key_img_prefix=args.key_img_prefix,
            key_label_prefix=args.key_label_prefix,
            batch_size=args.batch_size,
            shuffle=False,
            num_workers=0,
            pin_memory=True if device == 'cuda' else False,
            persistent_workers=True if device == 'cuda' else False,
        )

        # è¿è¡Œä¼˜åŒ–ç ”ç©¶
        create_optimization_study(
            device=device,
            model=model,
            dataloader=val_loader,
            output_dir=output_dir,
            study_name=args.study_name,
            method=args.method,
            n_trials=args.n_trials,
            param_config=args.param_config,
            optimization_target=args.optimization_target,
            dry_run=args.dry_run
        )

    print("âœ… é‡åŒ–è®­ç»ƒæµç¨‹å®Œæˆï¼")

if __name__ == "__main__":
    main()
