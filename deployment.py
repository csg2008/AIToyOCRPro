#!/usr/bin/env python3
"""
éƒ¨ç½²å·¥å…· - æ”¯æŒé‡åŒ–æ¨¡å‹çš„å„ç§éƒ¨ç½²æ ¼å¼
æä¾›ONNXã€TensorRTã€TorchScriptç­‰æ ¼å¼å¯¼å‡º
"""
import json
import logging
import os
import random
import time
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Any, Dict, List, Optional, Callable

import editdistance
import numpy as np
import onnx
import onnxruntime as ort
import torch
import torch.nn as nn
import torch.onnx
from PIL import Image, ImageDraw, ImageFont
from torch.export import Dim

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def cer_score(pred, gold, length, blank_id: int, sos_id: int, eos_id: int, idx2char: dict):
    """
    è®¡ç®—å­—ç¬¦é”™è¯¯ç‡ (Character Error Rate, CER)ã€‚

    Args:
        pred: æ¨¡å‹è¾“å‡ºçš„ä¸€ç»´ tensor æˆ–åˆ—è¡¨ï¼Œé•¿åº¦ä¸º max_lenï¼Œå¯èƒ½åŒ…å« PAD/EOS æ ‡è®°
        gold: çœŸå€¼ä¸€ç»´ tensor æˆ–åˆ—è¡¨ï¼Œé•¿åº¦ä¸º max_lenï¼Œå¯èƒ½åŒ…å« PAD/EOS æ ‡è®°
        length: çœŸå€¼å®é™…å­—ç¬¦æ•°ï¼ˆä¸å« SOS/EOS/PADï¼‰
        blank_id: ç©ºç™½ç¬¦ (PAD) çš„ ID
        sos_id: èµ·å§‹ç¬¦ (Start of Sequence) çš„ ID
        eos_id: ç»“æŸç¬¦ (End of Sequence) çš„ ID
        idx2char: ç´¢å¼•åˆ°å­—ç¬¦çš„æ˜ å°„å­—å…¸

    Returns:
        float: CER åˆ†æ•°ï¼Œè®¡ç®—å…¬å¼ä¸ºç¼–è¾‘è·ç¦»é™¤ä»¥çœŸå®æ–‡æœ¬é•¿åº¦
    """
    # æŠŠ tensor è½¬æˆ listï¼Œå»æ‰ PAD å’Œ EOS
    def _clean(seq):
        out = []
        idx_seq = seq.tolist() if isinstance(seq, torch.Tensor) else seq
        for idx in idx_seq:
            if idx == blank_id or idx == sos_id:    # PAD
                continue
            if idx == eos_id:                       # EOSï¼Œç›´æ¥æˆªæ–­
                break
            if idx in idx2char:                     # æ­£å¸¸å­—ç¬¦
                out.append(idx2char[idx])
        return out

    pred_clean = _clean(pred)
    gold_clean = _clean(gold[:length])          # åªå–æœ‰æ•ˆé•¿åº¦

    if len(gold_clean) == 0:
        return 1.0

    return editdistance.eval(pred_clean, gold_clean) / len(gold_clean)

def exact_match(pred, gold, length, blank_id, eos_id):
    """
    è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²æ˜¯å¦å®Œå…¨åŒ¹é… (Exact Match)ã€‚

    Args:
        pred: æ¨¡å‹è¾“å‡ºçš„ä¸€ç»´ tensorï¼Œé•¿åº¦ä¸º max_lenï¼Œå¯èƒ½åŒ…å« PAD/EOS æ ‡è®°
        gold: çœŸå€¼ä¸€ç»´ tensorï¼Œé•¿åº¦ä¸º max_lenï¼Œå¯èƒ½åŒ…å« PAD/EOS æ ‡è®°
        length: çœŸå€¼å®é™…å­—ç¬¦æ•°ï¼ˆä¸å« SOS/EOS/PADï¼‰
        blank_id: ç©ºç™½ç¬¦ (PAD) çš„ ID
        eos_id: ç»“æŸç¬¦ (End of Sequence) çš„ ID

    Returns:
        bool: å¦‚æœé¢„æµ‹ç»“æœä¸çœŸå®å€¼å®Œå…¨åŒ¹é…åˆ™è¿”å› Trueï¼Œå¦åˆ™è¿”å› False
    """
    # æœ‰æ•ˆé•¿åº¦
    len_pred = (pred != blank_id).logical_and(pred != eos_id).sum().item()

    min_len = min(len_pred, length)
    if min_len == 0:                       # ç©ºä¸²
        return len_pred == length

    return torch.equal(pred[:min_len], gold[:min_len])

def dbg_em(pred, gold, blank_id: int):
    """
    è°ƒè¯•ç”¨çš„ç²¾ç¡®åŒ¹é…å‡½æ•°ï¼Œç”¨äºæ‰“å°é¢„æµ‹ç»“æœå’ŒçœŸå®å€¼çš„è¯¦ç»†æ¯”è¾ƒä¿¡æ¯ã€‚

    Args:
        pred: æ¨¡å‹è¾“å‡ºçš„ä¸€ç»´ CPU tensorï¼ŒåŒ…å«é¢„æµ‹çš„ç´¢å¼•åºåˆ—
        gold: çœŸå€¼çš„ä¸€ç»´ CPU tensorï¼ŒåŒ…å«çœŸå®çš„ç´¢å¼•åºåˆ—
        blank_id: ç©ºç™½ç¬¦ (PAD) çš„ IDï¼Œç”¨äºè¿‡æ»¤æ‰ç©ºç™½ç¬¦

    Returns:
        None: è¯¥å‡½æ•°ä»…ç”¨äºè°ƒè¯•ï¼Œä¸è¿”å›ä»»ä½•å€¼ï¼Œç›´æ¥æ‰“å°æ¯”è¾ƒç»“æœ
    """
    pred_l = [int(i) for i in pred if int(i) != blank_id]
    gold_l = [int(i) for i in gold if int(i) != blank_id]
    print('pred', pred_l, 'gold', gold_l,
          'len=', len(pred_l), len(gold_l),
          'same?', pred_l == gold_l)

def ctc_decode_v2(pred: List[int], skip_tokens: List[int]) -> List[int]:
    """
    CTC è§£ç å‡½æ•°ï¼Œç”¨äºå°†æ¨¡å‹è¾“å‡ºçš„ç´¢å¼•åºåˆ—è½¬æ¢ä¸ºæœ€ç»ˆçš„å­—ç¬¦ç´¢å¼•åºåˆ—ã€‚

    Args:
        pred: æ¨¡å‹è¾“å‡ºçš„ç´¢å¼•åˆ—è¡¨ï¼Œé•¿åº¦ä¸ºåºåˆ—é•¿åº¦
        skip_tokens: éœ€è¦è·³è¿‡çš„æ ‡è®°åˆ—è¡¨ï¼Œé€šå¸¸åŒ…å« blank_id, sos_id, eos_id ç­‰

    Returns:
        List[int]: è§£ç åçš„ç´¢å¼•åˆ—è¡¨ï¼Œå»é™¤äº†è¿ç»­é‡å¤å’Œè·³è¿‡æ ‡è®°
    """
    pred = [pred[0]] + [pred[j] for j in range(1, len(pred)) if pred[j] != pred[j-1]]
    pred = [p for p in pred if p not in skip_tokens]

    return pred

class ExportWrapper(torch.nn.Module):
    """ç®€åŒ–çš„æ¨ç†æ¨¡å‹åŒ…è£…å™¨"""
    def __init__(self, model: nn.Module):
        super().__init__()

        model.eval()
        self.backbone = model.backbone
        self.neck     = model.neck
        self.head = model.decoder.ctc_decoder

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        x:  [B,3,H,W]  0~1  float32
        return: log_softmax å [B,L,num_classes]
        """
        feat = self.backbone(x)          # [B,L,C]
        feat = self.neck(feat)           # è®­ç»ƒæ—¶ neck éœ€è¦ targetï¼Œæ¨ç†ä¸ç”¨
        out  = self.head(feat)           # å†…éƒ¨å·² log_softmax
        return out

class ModelExporter(ABC):
    """æ¨¡å‹å¯¼å‡ºå™¨åŸºç±»"""

    def __init__(self, model: nn.Module, output_dir: str, vocab: list = None, other_pad_size: int = 0,
                 blank_id: int = 0, sos_id: int = 1, eos_id: int = 2, idx2char: dict = None):
        self.model = ExportWrapper(model)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.vocab = vocab or []
        self.other_pad_size = other_pad_size
        self.blank_id = blank_id
        self.sos_id = sos_id
        self.eos_id = eos_id
        self.idx2char = idx2char or {}

    @abstractmethod
    def export(self, dummy_input: torch.Tensor, **kwargs) -> str:
        """å¯¼å‡ºæ¨¡å‹"""
        pass

    @abstractmethod
    def get_supported_formats(self) -> List[str]:
        """è·å–æ”¯æŒçš„æ ¼å¼"""
        pass

class ONNXExporter(ModelExporter):
    """ONNXå¯¼å‡ºå™¨"""

    def __init__(self, model: nn.Module, output_dir: str, vocab: list = None, other_pad_size: int = 0,
                 blank_id: int = 0, sos_id: int = 1, eos_id: int = 2, idx2char: dict = None, opset_version: int = 18):
        super().__init__(model, output_dir, vocab, other_pad_size, blank_id, sos_id, eos_id, idx2char)
        self.opset_version = opset_version

    def export(self, dummy_input: torch.Tensor,
               input_names: Optional[List[str]] = None,
               output_names: Optional[List[str]] = None,
               dynamic_axes: Optional[Dict[str, Dict[str, Dim]]] = None,
               **kwargs) -> str:
        """å¯¼å‡ºONNXæ¨¡å‹"""

        if input_names is None:
            input_names = ['x']
        if output_names is None:
            output_names = ['logits']
        if dynamic_axes is None:
            # ç»™æ¯ä¸ªåŠ¨æ€ç»´èµ·åå­— + å¯é€‰èŒƒå›´
            batch_size  = Dim("batch_size", min=2, max=1024)   # å¯å†™ Dim("batch", min=1, max=64)
            width       = Dim("width", min=16, max=4096)
            dynamic_axes = {
                "x":   {0: batch_size, 3: width}
            }

        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()

        # æ„å»ºè¾“å‡ºè·¯å¾„
        output_path = self.output_dir / 'model.onnx'

        # å¯¼å‡ºONNX
        try:
            with torch.no_grad():
                self.model(dummy_input)
                if not os.path.exists(output_path):
                    self.export_onnx(str(output_path), dummy_input, input_names, output_names, dynamic_axes)
                self.validate(str(output_path), self.vocab, self.other_pad_size, self.blank_id, self.sos_id, self.eos_id, self.idx2char)

            logger.info(f"âœ… ONNXæ¨¡å‹å¯¼å‡ºæˆåŠŸ: {output_path}")
            return str(output_path)

        except Exception as e:
            logger.error(f"âŒ ONNXå¯¼å‡ºå¤±è´¥: {e}")
            raise

    def get_supported_formats(self) -> List[str]:
        return ['onnx']

    def validate(self, model_path: str, vocab: list, other_pad_size: int, blank_id: int, sos_id: int, eos_id: int, idx2char: dict) -> bool:
        """éªŒè¯ONNXæ¨¡å‹"""
        try:
            MAX_CHARS    = 1500
            STEP         = 10
            providers    = ['CUDAExecutionProvider', 'CPUExecutionProvider']

            # æµ‹è¯•åŠ¨æ€è½´
            self.test_dynamic_axes(model_path)

            # åŠ è½½ ONNX
            ort_sess = ort.InferenceSession(model_path, providers = providers)

            for n_char in range(10, MAX_CHARS + 1, STEP):
                gt_text = ''.join(np.random.choice(vocab[:-other_pad_size], n_char))
                img_tensor = self.make_text_image(gt_text)
                pred_text, dt = self.onnx_infer(ort_sess, img_tensor, blank_id, sos_id, eos_id, idx2char)
                pred_preview = pred_text[:30] + ('...' if len(pred_text) > 30 else '')
                gold_preview = gt_text[:30] + ('...' if len(gt_text) > 30 else '')
                cer = editdistance.eval(pred_text, gt_text) / max(len(gt_text), 1)
                print(n_char, img_tensor.shape[3], f'{cer:.3f}', f'{dt*1000:.1f}',
                    gold_preview, pred_preview, sep='\t')

            return True

        except Exception as e:
            logger.error(f"âŒ ONNXæ¨¡å‹éªŒè¯å¤±è´¥: {e}")
            return False

    def export_onnx(self, onnx_path: str, dummy_input: torch.Tensor,
                    input_names: List[str],
                    output_names: List[str],
                    dynamic_shapes: Dict[str, Dict[int, Dim]]):
        """å¯¼å‡º Onnx æ¨¡å‹"""

        # ç»™æ¯ä¸ªåŠ¨æ€ç»´èµ·åå­— + å¯é€‰èŒƒå›´
        batch_size  = Dim("batch_size", min=2, max=1024)   # å¯å†™ Dim("batch", min=1, max=64)
        width       = Dim("width", min=16, max=4096)

        # æŠŠè¾“å‡ºä¹Ÿå½“å…³é”®å­—å†™è¿›å»ï¼ˆåå­—è·Ÿ forward è¿”å›å˜é‡å¯¹åº”ï¼‰
        # è¾“å‡ºå¼ é‡ä¸éœ€è¦å†™åœ¨è¿™é‡Œï¼ŒDynamo ä¼šæ¨å¯¼ï¼›å¦‚æœå†™äº†ä¼šå¯¼è‡´å‡ºé”™
        dynamic_shapes = {
            "x":   {0: batch_size, 3: width}
        }

        torch.onnx.export(
            self.model,
            args=dummy_input,
            f=onnx_path,
            dynamo=True,
            verbose=True,
            report=True,
            verify=True,
            export_params=True,
            external_data=False,
            do_constant_folding=True,
            opset_version=self.opset_version,
            input_names=input_names,
            output_names=output_names,
            dynamic_shapes=dynamic_shapes
        )

        print(f'ONNX å·²å¯¼å‡º â†’ {onnx_path}')

        return onnx_path

    def export_onnx_fixed(self, onnx_path: str, dummy_input: torch.Tensor):
        """ä¿®å¤åçš„ONNXå¯¼å‡ºå‡½æ•°"""

        # å®šä¹‰åŠ¨æ€è½´
        dynamic_axes = {
            'x': {0: 'batch_size', 3: 'width'},  # åªåŠ¨æ€batchå’Œwidthï¼Œheightå’Œchannelå›ºå®š
            'logits': {0: 'batch_size', 1: 'seq_length'}  # è¾“å‡ºä¹Ÿéœ€è¦åŠ¨æ€è½´
        }

        # ä½¿ç”¨ä¼ ç»Ÿçš„å¯¼å‡ºæ–¹å¼ï¼ˆæ›´ç¨³å®šï¼‰
        # çœŸæ­£æé€Ÿï¼šå…ˆç”¨ jit.trace æ‹¿å›¾
        traced = torch.jit.trace(self.model, dummy_input)

        torch.onnx.export(
            traced,
            dummy_input,
            f=onnx_path,
            dynamo=False,
            export_params=True,
            external_data=False,
            opset_version=self.opset_version,
            do_constant_folding=True,
            input_names=['x'],
            output_names=['logits'],
            dynamic_axes=dynamic_axes,
            verbose=True
        )

        print(f'ONNX å·²å¯¼å‡º â†’ {onnx_path}')

        return onnx_path

    def test_dynamic_axes(self, onnx_path: str):
        """æµ‹è¯•åŠ¨æ€è½´æ˜¯å¦ç”Ÿæ•ˆ"""

        # éªŒè¯æ¨¡å‹
        onnx_model = onnx.load(onnx_path)
        onnx.checker.check_model(onnx_model)

        session = ort.InferenceSession(onnx_path)

        print("\n=== ONNXæ¨¡å‹è¾“å…¥ä¿¡æ¯ ===")
        for input in session.get_inputs():
            print(f"è¾“å…¥å: {input.name}")
            print(f"å½¢çŠ¶: {input.shape}")
            print(f"ç±»å‹: {input.type}")

        print("\n=== ONNXæ¨¡å‹è¾“å‡ºä¿¡æ¯ ===")
        for output in session.get_outputs():
            print(f"è¾“å‡ºå: {output.name}")
            print(f"å½¢çŠ¶: {output.shape}")
            print(f"ç±»å‹: {output.type}")

        # æµ‹è¯•ä¸åŒå°ºå¯¸çš„è¾“å…¥
        test_sizes = [
            (1, 3, 32, 128),
            (2, 3, 32, 256),
            (4, 3, 32, 512)
        ]

        print("\n=== åŠ¨æ€è½´æµ‹è¯• ===")
        for batch_size, channels, height, width in test_sizes:
            test_input = np.random.randn(batch_size, channels, height, width).astype(np.float32)
            try:
                outputs = session.run(None, {'x': test_input})
                print(f"è¾“å…¥å½¢çŠ¶ {test_input.shape} -> è¾“å‡ºå½¢çŠ¶ {outputs[0].shape} âœ“")
            except Exception as e:
                print(f"è¾“å…¥å½¢çŠ¶ {test_input.shape} -> å¤±è´¥: {e} âœ—")

    def make_text_image(self, text: str, height=32):
        """ åˆæˆå•å¼ å›¾ç‰‡ï¼Œè¿”å› (1,3,H,W) çš„ np.ndarray """
        # å°†å®½åº¦å‘ä¸Šå¯¹é½åˆ° 16 çš„å€æ•°ä»¥å…å‡ºé”™
        def round_up_to_patch_size(w, patch_size=16):
            return ((w + patch_size - 1) // patch_size) * patch_size

        # 1. å‡†å¤‡å­—ä½“
        font_path = 'arial.ttf'
        font = ImageFont.truetype(font_path, 24) if os.path.exists(font_path) \
            else ImageFont.load_default()

        # ç”ŸæˆéšæœºèƒŒæ™¯é¢œè‰²ï¼ˆRGBï¼‰
        bg_color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))

        # æ–‡å­—é¢œè‰²ä¸ºèƒŒæ™¯é¢œè‰²çš„åè‰²
        text_color = (255 - bg_color[0], 255 - bg_color[1], 255 - bg_color[2])

        # 2. è®¡ç®—æ–‡æœ¬å®½åº¦
        bbox = font.getbbox(text)
        w = bbox[2] - bbox[0] + 10
        w = round_up_to_patch_size(w)

        # 3. ç”Ÿæˆå›¾åƒå¹¶ç»˜åˆ¶æ–‡å­—
        img = Image.new('RGB', (w, height), bg_color)
        draw = ImageDraw.Draw(img)
        draw.text((5, 3), text, font=font, fill=text_color)

        # 4. è½¬ä¸º float32 å¹¶å½’ä¸€åŒ–åˆ° [0,1]
        arr = np.array(img, dtype=np.float32) / 255.0

        # 5. è°ƒæ•´è¾“å‡ºå½¢çŠ¶ä¸º (1,3,H,W)
        return arr.transpose(2, 0, 1)[None, :, :, :]

    def onnx_infer(self, session, img_np, blank_id: int, sos_id: int, eos_id: int, idx2char: dict):
        """æ¨ç†"""
        in_name  = session.get_inputs()[0].name
        out_name = session.get_outputs()[0].name
        skip_tokens = [blank_id, sos_id, eos_id]

        tic = time.time()
        logits = session.run([out_name], {in_name: img_np})[0]  # [1,L,nc]
        pred_ids = logits.argmax(-1)[0]                         # [L]
        pred_txt = ctc_decode_v2(pred_ids, skip_tokens=skip_tokens)
        pred_str = ''.join([idx2char[i] for i in pred_txt])
        toc = time.time()
        return pred_str, toc - tic

    def verify_onnx_quantization(self, model_path: str):
        """
        éªŒè¯ONNXæ¨¡å‹æ˜¯å¦é‡åŒ–

        Args:
            model_path: ONNXæ¨¡å‹è·¯å¾„

        Returns:
            bool: æ¨¡å‹æ˜¯å¦é‡åŒ–
        """
        print(f"ğŸ” éªŒè¯ONNXæ¨¡å‹: {model_path}")

        # åŠ è½½æ¨¡å‹
        model = onnx.load(model_path)

        # æ£€æŸ¥æ¨¡å‹ç»“æ„
        graph = model.graph

        # 1. æ£€æŸ¥æ˜¯å¦åŒ…å«QuantizeLinearæˆ–DequantizeLinearèŠ‚ç‚¹
        has_quantize_nodes = False
        quantize_node_count = 0
        dequantize_node_count = 0

        for node in graph.node:
            if node.op_type == 'QuantizeLinear':
                has_quantize_nodes = True
                quantize_node_count += 1
            elif node.op_type == 'DequantizeLinear':
                has_quantize_nodes = True
                dequantize_node_count += 1

        print(f"ğŸ“Š é‡åŒ–ç›¸å…³èŠ‚ç‚¹:")
        print(f"   - QuantizeLinearèŠ‚ç‚¹æ•°: {quantize_node_count}")
        print(f"   - DequantizeLinearèŠ‚ç‚¹æ•°: {dequantize_node_count}")
        print(f"   - åŒ…å«é‡åŒ–èŠ‚ç‚¹: {'âœ… æ˜¯' if has_quantize_nodes else 'âŒ å¦'}")

        # 2. æ£€æŸ¥æƒé‡æ•°æ®ç±»å‹
        has_integer_weights = False
        weight_data_types = {}

        for init in graph.initializer:
            tensor = onnx.numpy_helper.to_array(init)
            dtype = tensor.dtype
            weight_data_types[dtype] = weight_data_types.get(dtype, 0) + 1

            # æ£€æŸ¥æ˜¯å¦æœ‰æ•´æ•°ç±»å‹çš„æƒé‡
            if dtype.kind in 'iu':  # integer or unsigned integer
                has_integer_weights = True

        print(f"ğŸ“Š æƒé‡æ•°æ®ç±»å‹:")
        for dtype, count in weight_data_types.items():
            print(f"   - {dtype}: {count} ä¸ªæƒé‡å¼ é‡")
        print(f"   - åŒ…å«æ•´æ•°æƒé‡: {'âœ… æ˜¯' if has_integer_weights else 'âŒ å¦'}")

        # 3. æ£€æŸ¥æ¨¡å‹å…ƒæ•°æ®ä¸­æ˜¯å¦åŒ…å«é‡åŒ–ä¿¡æ¯
        has_quantization_metadata = False
        if hasattr(model, 'metadata_props'):
            for meta in model.metadata_props:
                if 'quant' in meta.key.lower() or 'quantization' in meta.key.lower():
                    has_quantization_metadata = True
                    print(f"   - å…ƒæ•°æ®: {meta.key} = {meta.value}")

        print(f"ğŸ“Š é‡åŒ–å…ƒæ•°æ®:")
        print(f"   - åŒ…å«é‡åŒ–å…ƒæ•°æ®: {'âœ… æ˜¯' if has_quantization_metadata else 'âŒ å¦'}")

        # 4. æ£€æŸ¥å›¾è¾“å…¥è¾“å‡ºçš„æ•°æ®ç±»å‹
        print(f"ğŸ“Š è¾“å…¥è¾“å‡ºæ•°æ®ç±»å‹:")
        for input in graph.input:
            print(f"   - è¾“å…¥ {input.name}: {input.type.tensor_type.elem_type}")
        for output in graph.output:
            print(f"   - è¾“å‡º {output.name}: {output.type.tensor_type.elem_type}")

        # ç»¼åˆåˆ¤æ–­
        is_quantized = has_quantize_nodes or has_integer_weights

        print(f"\nğŸ“‹ ç»¼åˆåˆ¤æ–­:")
        if is_quantized:
            print(f"âœ… æ¨¡å‹æ˜¯é‡åŒ–æ¨¡å‹")

            # è¿›ä¸€æ­¥åˆ†æé‡åŒ–ç±»å‹
            if quantize_node_count > 0 and dequantize_node_count > 0:
                print(f"   - é‡åŒ–ç±»å‹: å…¨é‡åŒ–æ¨¡å‹ (åŒ…å«QuantizeLinearå’ŒDequantizeLinear)")
            elif has_integer_weights:
                print(f"   - é‡åŒ–ç±»å‹: æƒé‡é‡åŒ–æ¨¡å‹ (ä»…æƒé‡ä¸ºæ•´æ•°ç±»å‹)")
        else:
            print(f"âŒ æ¨¡å‹ä¸æ˜¯é‡åŒ–æ¨¡å‹")

        return is_quantized
class TorchScriptExporter(ModelExporter):
    """TorchScriptå¯¼å‡ºå™¨"""

    def __init__(self, model: nn.Module, output_dir: str, vocab: list = None, other_pad_size: int = 0,
                 blank_id: int = 0, sos_id: int = 1, eos_id: int = 2, idx2char: dict = None):
        super().__init__(model, output_dir, vocab, other_pad_size, blank_id, sos_id, eos_id, idx2char)

    def export(self, dummy_input: torch.Tensor,
               method: str = 'trace',
               optimize: bool = True,
               **kwargs) -> str:
        """å¯¼å‡ºTorchScriptæ¨¡å‹"""

        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()

        # æ„å»ºè¾“å‡ºè·¯å¾„
        output_path = self.output_dir / 'model.pt'

        try:
            if method == 'trace':
                # ä½¿ç”¨è¿½è¸ªæ¨¡å¼
                traced_model = torch.jit.trace(self.model, dummy_input)
            elif method == 'script':
                # ä½¿ç”¨è„šæœ¬æ¨¡å¼
                traced_model = torch.jit.script(self.model)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„TorchScriptæ–¹æ³•: {method}")

            # ä¼˜åŒ–æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if optimize:
                traced_model = torch.jit.optimize_for_inference(traced_model)

            # ä¿å­˜æ¨¡å‹
            torch.jit.save(traced_model, str(output_path))

            logger.info(f"âœ… TorchScriptæ¨¡å‹å¯¼å‡ºæˆåŠŸ: {output_path}")
            return str(output_path)

        except Exception as e:
            logger.error(f"âŒ TorchScriptå¯¼å‡ºå¤±è´¥: {e}")
            raise

    def get_supported_formats(self) -> List[str]:
        return ['torchscript', 'pt']

class TensorRTExporter(ModelExporter):
    """TensorRTå¯¼å‡ºå™¨ï¼ˆéœ€è¦TensorRTæ”¯æŒï¼‰"""

    def __init__(self, model: nn.Module, output_dir: str, vocab: list = None, other_pad_size: int = 0,
                 blank_id: int = 0, sos_id: int = 1, eos_id: int = 2, idx2char: dict = None):
        super().__init__(model, output_dir, vocab, other_pad_size, blank_id, sos_id, eos_id, idx2char)
        self._check_tensorrt_availability()

    def _check_tensorrt_availability(self):
        """æ£€æŸ¥TensorRTæ˜¯å¦å¯ç”¨"""
        try:
            import tensorrt as trt
            self.trt_available = True
            logger.info("âœ… TensorRTå¯ç”¨")
        except ImportError:
            self.trt_available = False
            logger.warning("âš ï¸ TensorRTä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ONNXä½œä¸ºä¸­é—´æ ¼å¼")

    def export(self, dummy_input: torch.Tensor,
               max_batch_size: int = 32,
               max_workspace_size: int = 1 << 30,  # 1GB
               fp16_mode: bool = False,
               int8_mode: bool = False,
               **kwargs) -> str:
        """å¯¼å‡ºTensorRTå¼•æ“"""

        if not self.trt_available:
            logger.warning("ä½¿ç”¨ONNXä½œä¸ºTensorRTçš„ä¸­é—´æ ¼å¼")
            return self._export_via_onnx(dummy_input, max_batch_size,
                                       max_workspace_size, fp16_mode, int8_mode)

        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()

        # æ„å»ºè¾“å‡ºè·¯å¾„
        output_path = self.output_dir / 'model.trt'

        try:
            import tensorrt as trt

            # åˆ›å»ºTensorRT logger
            TRT_LOGGER = trt.Logger(trt.Logger.WARNING)

            # åˆ›å»ºbuilder
            builder = trt.Builder(TRT_LOGGER)
            network = builder.create_network(
                1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
            )
            parser = trt.OnnxParser(network, TRT_LOGGER)

            # é¦–å…ˆå¯¼å‡ºONNX
            onnx_path = self.output_dir / 'temp_model.onnx'
            torch.onnx.export(
                self.model,
                dummy_input,
                str(onnx_path),
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}},
                opset_version=13
            )

            # è§£æONNX
            with open(onnx_path, 'rb') as f:
                if not parser.parse(f.read()):
                    for error in range(parser.num_errors):
                        logger.error(parser.get_error(error))
                    raise RuntimeError("ONNXè§£æå¤±è´¥")

            # æ„å»ºé…ç½®
            config = builder.create_builder_config()
            config.max_workspace_size = max_workspace_size

            # è®¾ç½®ç²¾åº¦æ¨¡å¼
            if fp16_mode:
                config.set_flag(trt.BuilderFlag.FP16)
            if int8_mode:
                config.set_flag(trt.BuilderFlag.INT8)
                # è¿™é‡Œéœ€è¦è®¾ç½®INT8æ ¡å‡†å™¨

            # æ„å»ºå¼•æ“
            engine_bytes = builder.build_serialized_network(network, config)

            # ä¿å­˜å¼•æ“
            with open(output_path, 'wb') as f:
                f.write(engine_bytes)

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            onnx_path.unlink(missing_ok=True)

            logger.info(f"âœ… TensorRTå¼•æ“å¯¼å‡ºæˆåŠŸ: {output_path}")
            return str(output_path)

        except Exception as e:
            logger.error(f"âŒ TensorRTå¯¼å‡ºå¤±è´¥: {e}")
            raise

    def _export_via_onnx(self, dummy_input: torch.Tensor,
                        max_batch_size: int, max_workspace_size: int,
                        fp16_mode: bool, int8_mode: bool) -> str:
        """é€šè¿‡ONNXé—´æ¥å¯¼å‡ºTensorRT"""
        try:
            from torch_tensorrt import compile

            # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()

            # ç¼–è¯‘æ¨¡å‹
            trt_model = compile(
                self.model,
                inputs=[dummy_input],
                enabled_precisions={torch.float, torch.half} if fp16_mode else {torch.float},
                workspace_size=max_workspace_size,
                truncate_long_and_double=True
            )

            # ä¿å­˜æ¨¡å‹
            output_path = self.output_dir / 'model_trt.pt'
            torch.jit.save(trt_model, str(output_path))

            logger.info(f"âœ… TensorRTæ¨¡å‹å¯¼å‡ºæˆåŠŸï¼ˆé€šè¿‡torch-tensorrtï¼‰: {output_path}")
            return str(output_path)

        except Exception as e:
            logger.error(f"âŒ TensorRTå¯¼å‡ºå¤±è´¥: {e}")
            raise

    def get_supported_formats(self) -> List[str]:
        return ['tensorrt', 'trt']

class CoreMLExporter(ModelExporter):
    """CoreMLå¯¼å‡ºå™¨ï¼ˆéœ€è¦coremltoolsæ”¯æŒï¼‰"""

    def __init__(self, model: nn.Module, output_dir: str, vocab: list = None, other_pad_size: int = 0,
                 blank_id: int = 0, sos_id: int = 1, eos_id: int = 2, idx2char: dict = None):
        super().__init__(model, output_dir, vocab, other_pad_size, blank_id, sos_id, eos_id, idx2char)
        self._check_coreml_availability()

    def _check_coreml_availability(self):
        """æ£€æŸ¥CoreMLæ˜¯å¦å¯ç”¨"""
        try:
            import coremltools as ct
            self.coreml_available = True
            logger.info("âœ… CoreMLå¯ç”¨")
        except ImportError:
            self.coreml_available = False
            logger.warning("âš ï¸ CoreMLä¸å¯ç”¨ï¼Œéœ€è¦å®‰è£…coremltools")

    def export(self, dummy_input: torch.Tensor,
               input_names: Optional[List[str]] = None,
               output_names: Optional[List[str]] = None,
               minimum_deployment_target: str = '13',
               **kwargs) -> str:
        """å¯¼å‡ºCoreMLæ¨¡å‹"""

        if not self.coreml_available:
            raise RuntimeError("CoreMLä¸å¯ç”¨ï¼Œè¯·å®‰è£…coremltools")

        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()

        # æ„å»ºè¾“å‡ºè·¯å¾„
        output_path = self.output_dir / 'model.mlmodel'

        try:
            import coremltools as ct

            # è¿½è¸ªæ¨¡å‹
            traced_model = torch.jit.trace(self.model, dummy_input)

            # è½¬æ¢ä¸ºCoreML
            coreml_model = ct.convert(
                traced_model,
                inputs=[ct.TensorType(name="input", shape=dummy_input.shape)],
                minimum_deployment_target=ct.target.iOS13
            )

            # ä¿å­˜æ¨¡å‹
            coreml_model.save(str(output_path))

            logger.info(f"âœ… CoreMLæ¨¡å‹å¯¼å‡ºæˆåŠŸ: {output_path}")
            return str(output_path)

        except Exception as e:
            logger.error(f"âŒ CoreMLå¯¼å‡ºå¤±è´¥: {e}")
            raise

    def get_supported_formats(self) -> List[str]:
        return ['coreml', 'mlmodel']

class TFLiteExporter(ModelExporter):
    """TensorFlow Liteå¯¼å‡ºå™¨"""

    def __init__(self, model: nn.Module, output_dir: str, vocab: list = None, other_pad_size: int = 0,
                 blank_id: int = 0, sos_id: int = 1, eos_id: int = 2, idx2char: dict = None):
        super().__init__(model, output_dir, vocab, other_pad_size, blank_id, sos_id, eos_id, idx2char)
        self._check_tflite_availability()

    def _check_tflite_availability(self):
        """æ£€æŸ¥TensorFlow Liteæ˜¯å¦å¯ç”¨"""
        try:
            import tensorflow as tf
            self.tflite_available = True
            logger.info("âœ… TensorFlow Liteå¯ç”¨")
        except ImportError:
            self.tflite_available = False
            logger.warning("âš ï¸ TensorFlow Liteä¸å¯ç”¨ï¼Œéœ€è¦å®‰è£…tensorflow")

    def export(self, dummy_input: torch.Tensor,
               quantization_type: str = 'dynamic',
               representative_dataset: Optional[Any] = None,
               **kwargs) -> str:
        """å¯¼å‡ºTensorFlow Liteæ¨¡å‹"""

        if not self.tflite_available:
            raise RuntimeError("TensorFlow Liteä¸å¯ç”¨ï¼Œè¯·å®‰è£…tensorflow")

        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()

        # æ„å»ºè¾“å‡ºè·¯å¾„
        output_path = self.output_dir / 'model.tflite'

        try:
            import tensorflow as tf
            import torch.onnx

            # é¦–å…ˆå¯¼å‡ºä¸ºONNX
            onnx_path = self.output_dir / 'temp_model.onnx'
            torch.onnx.export(
                self.model,
                dummy_input,
                str(onnx_path),
                input_names=['input'],
                output_names=['output'],
                opset_version=13
            )

            # ä½¿ç”¨ONNX-TFè½¬æ¢
            from onnx_tf.backend import prepare

            # åŠ è½½ONNXæ¨¡å‹
            onnx_model = onnx.load(str(onnx_path))

            # è½¬æ¢ä¸ºTensorFlow
            tf_rep = prepare(onnx_model)

            # ä¿å­˜TensorFlowæ¨¡å‹
            tf_model_path = self.output_dir / 'temp_tf_model'
            tf_rep.export_graph(str(tf_model_path))

            # è½¬æ¢ä¸ºTFLite
            converter = tf.lite.TFLiteConverter.from_saved_model(str(tf_model_path))

            # è®¾ç½®é‡åŒ–é…ç½®
            if quantization_type == 'dynamic':
                converter.optimizations = [tf.lite.Optimize.DEFAULT]
            elif quantization_type == 'int8':
                converter.optimizations = [tf.lite.Optimize.DEFAULT]
                converter.target_spec.supported_types = [tf.int8]
                if representative_dataset:
                    converter.representative_dataset = representative_dataset
            elif quantization_type == 'float16':
                converter.optimizations = [tf.lite.Optimize.DEFAULT]
                converter.target_spec.supported_types = [tf.float16]

            # è½¬æ¢
            tflite_model = converter.convert()

            # ä¿å­˜TFLiteæ¨¡å‹
            with open(output_path, 'wb') as f:
                f.write(tflite_model)

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            onnx_path.unlink(missing_ok=True)
            import shutil
            shutil.rmtree(tf_model_path, ignore_errors=True)

            logger.info(f"âœ… TensorFlow Liteæ¨¡å‹å¯¼å‡ºæˆåŠŸ: {output_path}")
            return str(output_path)

        except Exception as e:
            logger.error(f"âŒ TensorFlow Liteå¯¼å‡ºå¤±è´¥: {e}")
            raise

    def get_supported_formats(self) -> List[str]:
        return ['tflite']

class QuantizedModelExporter:
    """é‡åŒ–æ¨¡å‹å¯¼å‡ºç®¡ç†å™¨"""

    def __init__(self, model: nn.Module, output_dir: str,
                 quantization_config: Optional[Dict[str, Any]] = None,
                 vocab: list = None, other_pad_size: int = 0,
                 blank_id: int = 0, sos_id: int = 1, eos_id: int = 2,
                 idx2char: dict = None):
        self.model = model
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.quantization_config = quantization_config or {}
        self.vocab = vocab or []
        self.other_pad_size = other_pad_size
        self.blank_id = blank_id
        self.sos_id = sos_id
        self.eos_id = eos_id
        self.idx2char = idx2char or {}

        # åˆå§‹åŒ–æ‰€æœ‰å¯¼å‡ºå™¨
        self.exporters = {
            'onnx': ONNXExporter(model, str(self.output_dir / 'onnx'), vocab, other_pad_size, blank_id, sos_id, eos_id, idx2char),
            'torchscript': TorchScriptExporter(model, str(self.output_dir / 'torchscript'), vocab, other_pad_size, blank_id, sos_id, eos_id, idx2char),
            'tensorrt': TensorRTExporter(model, str(self.output_dir / 'tensorrt'), vocab, other_pad_size, blank_id, sos_id, eos_id, idx2char),
            'coreml': CoreMLExporter(model, str(self.output_dir / 'coreml'), vocab, other_pad_size, blank_id, sos_id, eos_id, idx2char),
            'tflite': TFLiteExporter(model, str(self.output_dir / 'tflite'), vocab, other_pad_size, blank_id, sos_id, eos_id, idx2char)
        }

    def export_all_formats(self, dummy_input: torch.Tensor,
                          formats: Optional[List[str]] = None) -> Dict[str, str]:
        """å¯¼å‡ºæ‰€æœ‰æ”¯æŒçš„æ ¼å¼"""

        if formats is None:
            formats = ['onnx', 'torchscript']  # é»˜è®¤å¯¼å‡ºå¸¸ç”¨æ ¼å¼

        results = {}

        for format_name in formats:
            if format_name in self.exporters:
                try:
                    exporter = self.exporters[format_name]
                    output_path = exporter.export(dummy_input)
                    results[format_name] = output_path
                    logger.info(f"âœ… {format_name.upper()}å¯¼å‡ºæˆåŠŸ")
                except Exception as e:
                    logger.error(f"âŒ {format_name.upper()}å¯¼å‡ºå¤±è´¥: {e}")
                    results[format_name] = None
            else:
                logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„æ ¼å¼: {format_name}")
                results[format_name] = None

        # ä¿å­˜å¯¼å‡ºé…ç½®
        self._save_export_config(results)

        return results

    def export_specific_format(self, format_name: str,
                              dummy_input: torch.Tensor,
                              **kwargs) -> Optional[str]:
        """å¯¼å‡ºç‰¹å®šæ ¼å¼"""

        if format_name in self.exporters:
            try:
                exporter = self.exporters[format_name]
                output_path = exporter.export(dummy_input, **kwargs)
                logger.info(f"âœ… {format_name.upper()}å¯¼å‡ºæˆåŠŸ")
                return output_path
            except Exception as e:
                logger.error(f"âŒ {format_name.upper()}å¯¼å‡ºå¤±è´¥: {e}")
                return None
        else:
            logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„æ ¼å¼: {format_name}")
            return None

    def _save_export_config(self, export_results: Dict[str, str]):
        """ä¿å­˜å¯¼å‡ºé…ç½®"""
        config = {
            'quantization_config': self.quantization_config,
            'export_results': export_results,
            'export_time': torch.tensor([]).device.type,  # è·å–å½“å‰è®¾å¤‡ä¿¡æ¯
            'pytorch_version': torch.__version__
        }

        config_path = self.output_dir / 'export_config.json'
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)

        logger.info(f"ğŸ’¾ å¯¼å‡ºé…ç½®å·²ä¿å­˜: {config_path}")

    def get_export_summary(self) -> Dict[str, Any]:
        """è·å–å¯¼å‡ºæ‘˜è¦"""
        summary = {
            'total_formats': len(self.exporters),
            'available_formats': list(self.exporters.keys()),
            'output_directory': str(self.output_dir),
            'quantization_enabled': bool(self.quantization_config)
        }

        # æ£€æŸ¥å·²å¯¼å‡ºçš„æ–‡ä»¶
        exported_files = {}
        for format_name, exporter in self.exporters.items():
            for ext in exporter.get_supported_formats():
                expected_file = self.output_dir / format_name / f'model.{ext}'
                if expected_file.exists():
                    file_size = expected_file.stat().st_size / (1024 * 1024)  # MB
                    exported_files[format_name] = {
                        'path': str(expected_file),
                        'size_mb': round(file_size, 2)
                    }

        summary['exported_files'] = exported_files
        return summary

class DeploymentOptimizer:
    """éƒ¨ç½²ä¼˜åŒ–å™¨ - é’ˆå¯¹ç‰¹å®šéƒ¨ç½²åœºæ™¯ä¼˜åŒ–æ¨¡å‹"""

    def __init__(self, model: nn.Module, deployment_target: str):
        self.model = model
        self.deployment_target = deployment_target
        self.optimization_passes = self._get_optimization_passes()

    def _get_optimization_passes(self) -> List[Callable]:
        """è·å–é’ˆå¯¹ç›®æ ‡å¹³å°çš„ä¼˜åŒ–pass"""
        passes = []

        if self.deployment_target == 'mobile':
            passes.extend([
                self._optimize_for_mobile,
                self._remove_unused_operations,
                self._fuse_operations
            ])
        elif self.deployment_target == 'edge':
            passes.extend([
                self._optimize_for_edge,
                self._quantize_activations,
                self._simplify_graph
            ])
        elif self.deployment_target == 'server':
            passes.extend([
                self._optimize_for_server,
                self._enable_parallel_execution,
                self._optimize_memory_layout
            ])

        return passes

    def optimize_for_deployment(self) -> nn.Module:
        """ä¸ºéƒ¨ç½²ä¼˜åŒ–æ¨¡å‹"""
        optimized_model = self.model

        for optimization_pass in self.optimization_passes:
            try:
                optimized_model = optimization_pass(optimized_model)
                logger.info(f"âœ… åº”ç”¨ä¼˜åŒ–: {optimization_pass.__name__}")
            except Exception as e:
                logger.warning(f"âš ï¸ ä¼˜åŒ–å¤±è´¥ {optimization_pass.__name__}: {e}")

        return optimized_model

    def _optimize_for_mobile(self, model: nn.Module) -> nn.Module:
        """ç§»åŠ¨ç«¯ä¼˜åŒ–"""
        # ç®€åŒ–æ¨¡å‹ç»“æ„
        # å‡å°‘æ“ä½œæ•°é‡
        # ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼
        return model

    def _optimize_for_edge(self, model: nn.Module) -> nn.Module:
        """è¾¹ç¼˜è®¾å¤‡ä¼˜åŒ–"""
        # å‡å°‘è®¡ç®—å¤æ‚åº¦
        # ä¼˜åŒ–åŠŸè€—
        return model

    def _optimize_for_server(self, model: nn.Module) -> nn.Module:
        """æœåŠ¡å™¨ç«¯ä¼˜åŒ–"""
        # å¯ç”¨å¹¶è¡ŒåŒ–
        # ä¼˜åŒ–æ‰¹å¤„ç†
        return model

    def _remove_unused_operations(self, model: nn.Module) -> nn.Module:
        """ç§»é™¤æœªä½¿ç”¨çš„æ“ä½œ"""
        # å›¾ä¼˜åŒ–ï¼šç§»é™¤æ­»ä»£ç 
        return model

    def _fuse_operations(self, model: nn.Module) -> nn.Module:
        """èåˆæ“ä½œ"""
        # åˆå¹¶è¿ç»­çš„æ“ä½œä»¥å‡å°‘å†…å­˜è®¿é—®
        return model

    def _quantize_activations(self, model: nn.Module) -> nn.Module:
        """é‡åŒ–æ¿€æ´»å‡½æ•°"""
        # åº”ç”¨æ¿€æ´»å‡½æ•°é‡åŒ–
        return model

    def _simplify_graph(self, model: nn.Module) -> nn.Module:
        """ç®€åŒ–è®¡ç®—å›¾"""
        # å›¾ç®€åŒ–ä¼˜åŒ–
        return model

    def _enable_parallel_execution(self, model: nn.Module) -> nn.Module:
        """å¯ç”¨å¹¶è¡Œæ‰§è¡Œ"""
        # å¹¶è¡ŒåŒ–ä¼˜åŒ–
        return model

    def _optimize_memory_layout(self, model: nn.Module) -> nn.Module:
        """ä¼˜åŒ–å†…å­˜å¸ƒå±€"""
        # å†…å­˜è®¿é—®ä¼˜åŒ–
        return model

# éƒ¨ç½²é…ç½®æ¨¡æ¿
DEPLOYMENT_CONFIGS = {
    'mobile_cpu': {
        'formats': ['tflite', 'onnx'],
        'optimizations': ['mobile', 'quantization'],
        'target_device': 'arm_cpu',
        'precision': 'int8',
        'max_model_size_mb': 10
    },
    'mobile_gpu': {
        'formats': ['coreml', 'onnx'],
        'optimizations': ['mobile', 'gpu'],
        'target_device': 'mobile_gpu',
        'precision': 'fp16',
        'max_model_size_mb': 20
    },
    'edge_tpu': {
        'formats': ['tflite'],
        'optimizations': ['edge', 'quantization'],
        'target_device': 'edge_tpu',
        'precision': 'int8',
        'max_model_size_mb': 5
    },
    'server_cpu': {
        'formats': ['onnx', 'torchscript'],
        'optimizations': ['server', 'cpu'],
        'target_device': 'x86_cpu',
        'precision': 'int8',
        'max_model_size_mb': 100
    },
    'server_gpu': {
        'formats': ['tensorrt', 'onnx'],
        'optimizations': ['server', 'gpu'],
        'target_device': 'nvidia_gpu',
        'precision': 'fp16',
        'max_model_size_mb': 200
    }
}

def create_deployment_package(model: nn.Module,
                            quantization_config: Dict[str, Any],
                            deployment_target: str,
                            output_dir: str,
                            dummy_input: torch.Tensor,
                            vocab: list = None,
                            other_pad_size: int = 0,
                            blank_id: int = 0,
                            sos_id: int = 1,
                            eos_id: int = 2,
                            idx2char: dict = None) -> str:
    """åˆ›å»ºéƒ¨ç½²åŒ…"""

    logger.info(f"ğŸ“¦ åˆ›å»ºéƒ¨ç½²åŒ…ï¼Œç›®æ ‡å¹³å°: {deployment_target}")

    # è·å–éƒ¨ç½²é…ç½®
    if deployment_target not in DEPLOYMENT_CONFIGS:
        raise ValueError(f"ä¸æ”¯æŒçš„éƒ¨ç½²ç›®æ ‡: {deployment_target}")

    config = DEPLOYMENT_CONFIGS[deployment_target]

    # åˆ›å»ºè¾“å‡ºç›®å½•
    package_dir = Path(output_dir) / f'deployment_{deployment_target}'
    package_dir.mkdir(parents=True, exist_ok=True)

    # ä¼˜åŒ–æ¨¡å‹
    optimizer = DeploymentOptimizer(model, deployment_target)
    optimized_model = optimizer.optimize_for_deployment()

    # å¯¼å‡ºæ¨¡å‹
    exporter = QuantizedModelExporter(
        optimized_model,
        str(package_dir),
        quantization_config,
        vocab,
        other_pad_size,
        blank_id,
        sos_id,
        eos_id,
        idx2char
    )

    # å¯¼å‡ºæŒ‡å®šæ ¼å¼
    results = exporter.export_all_formats(
        dummy_input,
        formats=config['formats']
    )

    # åˆ›å»ºéƒ¨ç½²æ–‡æ¡£
    deployment_doc = {
        'target_platform': deployment_target,
        'optimization_config': config,
        'exported_models': results,
        'model_info': {
            'total_parameters': sum(p.numel() for p in model.parameters()),
            'model_size_mb': sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024),
            'quantization_applied': bool(quantization_config)
        },
        'deployment_instructions': get_deployment_instructions(deployment_target)
    }

    # ä¿å­˜éƒ¨ç½²æ–‡æ¡£
    doc_path = package_dir / 'deployment_guide.json'
    with open(doc_path, 'w', encoding='utf-8') as f:
        json.dump(deployment_doc, f, indent=2, ensure_ascii=False)

    # åˆ›å»ºæ¨ç†è„šæœ¬æ¨¡æ¿
    inference_script = create_inference_script(deployment_target, results)
    script_path = package_dir / 'inference_example.py'
    with open(script_path, 'w', encoding='utf-8') as f:
        f.write(inference_script)

    logger.info(f"âœ… éƒ¨ç½²åŒ…åˆ›å»ºå®Œæˆ: {package_dir}")
    return str(package_dir)

def get_deployment_instructions(target: str) -> Dict[str, str]:
    """è·å–éƒ¨ç½²è¯´æ˜"""
    instructions = {
        'mobile_cpu': {
            'runtime': 'TensorFlow Lite',
            'installation': 'pip install tensorflow',
            'optimization': 'ä½¿ç”¨XNNPACKåç«¯ï¼Œå¯ç”¨å¤šçº¿ç¨‹',
            'memory_requirements': 'è‡³å°‘100MBå¯ç”¨å†…å­˜'
        },
        'mobile_gpu': {
            'runtime': 'CoreMLæˆ–ONNX Runtime',
            'installation': 'pip install coremltools onnxruntime',
            'optimization': 'å¯ç”¨GPUåŠ é€Ÿï¼Œä½¿ç”¨Metalåç«¯',
            'memory_requirements': 'è‡³å°‘200MBå¯ç”¨å†…å­˜'
        },
        'edge_tpu': {
            'runtime': 'TensorFlow Lite + Edge TPU',
            'installation': 'å®‰è£…Edge TPUè¿è¡Œæ—¶',
            'optimization': 'æ¨¡å‹å¿…é¡»å®Œå…¨é‡åŒ–åˆ°INT8',
            'memory_requirements': 'è‡³å°‘50MBå¯ç”¨å†…å­˜'
        },
        'server_cpu': {
            'runtime': 'ONNX Runtimeæˆ–TorchScript',
            'installation': 'pip install onnxruntime',
            'optimization': 'å¯ç”¨AVXæŒ‡ä»¤é›†ï¼Œä½¿ç”¨å¤šçº¿ç¨‹',
            'memory_requirements': 'è‡³å°‘1GBå¯ç”¨å†…å­˜'
        },
        'server_gpu': {
            'runtime': 'TensorRTæˆ–ONNX Runtime',
            'installation': 'pip install tensorrt onnxruntime-gpu',
            'optimization': 'å¯ç”¨TensorRTä¼˜åŒ–ï¼Œä½¿ç”¨CUDA',
            'memory_requirements': 'è‡³å°‘2GBæ˜¾å­˜'
        }
    }

    return instructions.get(target, {})

def create_inference_script(target: str, exported_models: Dict[str, str]) -> str:
    """åˆ›å»ºæ¨ç†è„šæœ¬æ¨¡æ¿"""

    script_parts = []

    # å¤´éƒ¨æ³¨é‡Š
    script_parts.append('"""')
    script_parts.append(f'æ¨ç†è„šæœ¬ - {target}å¹³å°')
    script_parts.append('è‡ªåŠ¨ç”Ÿæˆï¼Œè¯·æ ¹æ®å®é™…éœ€æ±‚ä¿®æ”¹')
    script_parts.append('"""')
    script_parts.append('')

    # å¯¼å…¥ä¾èµ–
    if 'tflite' in exported_models:
        script_parts.append('import tensorflow as tf')
        script_parts.append('import numpy as np')

    if 'onnx' in exported_models:
        script_parts.append('import onnxruntime as ort')
        script_parts.append('import numpy as np')

    if 'coreml' in exported_models:
        script_parts.append('import coremltools as ct')

    if 'torchscript' in exported_models:
        script_parts.append('import torch')

    if 'tensorrt' in exported_models:
        script_parts.append('import tensorrt as trt')
        script_parts.append('import pycuda.driver as cuda')
        script_parts.append('import pycuda.autoinit')

    script_parts.append('')

    # æ¨¡å‹åŠ è½½å‡½æ•°
    script_parts.append('def load_model(model_path: str):')
    script_parts.append('    """åŠ è½½æ¨¡å‹"""')

    if 'tflite' in exported_models:
        script_parts.append('    if model_path.endswith(".tflite"):')
        script_parts.append('        interpreter = tf.lite.Interpreter(model_path=model_path)')
        script_parts.append('        interpreter.allocate_tensors()')
        script_parts.append('        return interpreter')

    if 'onnx' in exported_models:
        script_parts.append('    elif model_path.endswith(".onnx"):')
        script_parts.append('        return ort.InferenceSession(model_path)')

    if 'coreml' in exported_models:
        script_parts.append('    elif model_path.endswith(".mlmodel"):')
        script_parts.append('        return ct.models.MLModel(model_path)')

    if 'torchscript' in exported_models:
        script_parts.append('    elif model_path.endswith(".pt"):')
        script_parts.append('        return torch.jit.load(model_path)')

    if 'tensorrt' in exported_models:
        script_parts.append('    elif model_path.endswith(".trt"):')
        script_parts.append('        # TensorRTå¼•æ“åŠ è½½é€»è¾‘')
        script_parts.append('        pass')

    script_parts.append('    else:')
    script_parts.append('        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹æ ¼å¼: {model_path}")')
    script_parts.append('')

    # æ¨ç†å‡½æ•°
    script_parts.append('def inference(model, input_data):')
    script_parts.append('    """è¿è¡Œæ¨ç†"""')

    if 'tflite' in exported_models:
        script_parts.append('    if hasattr(model, "get_input_details"):  # TensorFlow Lite')
        script_parts.append('        input_details = model.get_input_details()')
        script_parts.append('        output_details = model.get_output_details()')
        script_parts.append('        model.set_tensor(input_details[0]["index"], input_data)')
        script_parts.append('        model.invoke()')
        script_parts.append('        return model.get_tensor(output_details[0]["index"])')

    if 'onnx' in exported_models:
        script_parts.append('    elif hasattr(model, "run"):  # ONNX Runtime')
        script_parts.append('        input_name = model.get_inputs()[0].name')
        script_parts.append('        return model.run(None, {input_name: input_data})[0]')

    if 'coreml' in exported_models:
        script_parts.append('    elif hasattr(model, "predict"):  # CoreML')
        script_parts.append('        return model.predict({"input": input_data})')

    if 'torchscript' in exported_models:
        script_parts.append('    elif hasattr(model, "forward"):  # TorchScript')
        script_parts.append('        with torch.no_grad():')
        script_parts.append('            return model(torch.from_numpy(input_data)).numpy()')

    script_parts.append('')
    script_parts.append('    else:')
    script_parts.append('        raise ValueError("æœªçŸ¥çš„æ¨¡å‹ç±»å‹")')
    script_parts.append('')

    # ä¸»å‡½æ•°
    script_parts.append('def main():')
    script_parts.append('    """ä¸»å‡½æ•°ç¤ºä¾‹"""')
    script_parts.append('    # æ¨¡å‹è·¯å¾„ï¼ˆè¯·æ ¹æ®å®é™…å¯¼å‡ºç»“æœä¿®æ”¹ï¼‰')

    for format_name, model_path in exported_models.items():
        if model_path:
            script_parts.append(f'    # model_path = "{model_path}"  # {format_name.upper()}æ ¼å¼')

    script_parts.append('')
    script_parts.append('    # åŠ è½½æ¨¡å‹')
    script_parts.append('    # model = load_model(model_path)')
    script_parts.append('')
    script_parts.append('    # å‡†å¤‡è¾“å…¥æ•°æ®ï¼ˆè¯·æ ¹æ®å®é™…è¾“å…¥å½¢çŠ¶ä¿®æ”¹ï¼‰')
    script_parts.append('    # input_data = np.random.randn(1, 3, 32, 128).astype(np.float32)')
    script_parts.append('')
    script_parts.append('    # è¿è¡Œæ¨ç†')
    script_parts.append('    # output = inference(model, input_data)')
    script_parts.append('    # print(f"æ¨ç†ç»“æœå½¢çŠ¶: {output.shape}")')
    script_parts.append('')
    script_parts.append('if __name__ == "__main__":')
    script_parts.append('    main()')

    return '\n'.join(script_parts)
