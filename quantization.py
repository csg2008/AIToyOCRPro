"""
é‡åŒ–é…ç½®ç®¡ç†å™¨ - åŠ¨æ€è°ƒæ•´é‡åŒ–å‚æ•°å’Œç­–ç•¥
æä¾›å¤šç§é‡åŒ–ç­–ç•¥å’Œä¼˜åŒ–é€‰é¡¹
"""
import json
import os
import time
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple
from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np
import psutil
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.utils.prune as prune
from torch.export import export
from torch.utils.data import DataLoader
import yaml
from tqdm import tqdm
import optuna
from optuna import Trial
from functools import reduce
import operator
import itertools
import warnings
from torchao.quantization import (
    Int8DynActInt4WeightLinear,
    Int8DynActInt4WeightQuantizer,
    quantize_,
    int8_dynamic_activation_int4_weight,
    int8_weight_only,
    int4_weight_only,
    int8_dynamic_activation_int8_weight,
)
from torchao.quantization.qat import (
    Int8DynActInt4WeightQATQuantizer,
    Int4WeightOnlyQATQuantizer,
)

class PruningConfig:
    """å‰ªæé…ç½®ç±»"""
    def __init__(self, config: Dict):
        self.enabled = config.get('enabled', False)
        self.pruning_strategy = config.get('pruning_strategy', 'l1_unstructured')
        self.pruning_ratio = config.get('pruning_ratio', 0.3)
        self.pruning_layers = config.get('pruning_layers', ['backbone', 'neck', 'decoder'])
        self.pruning_epoch = config.get('pruning_epoch', 20)
        self.min_acc_drop = config.get('min_acc_drop', 0.01)
        self.finetune_epochs = config.get('finetune_epochs', 10)
        self.prune_criteria = config.get('prune_criteria', 'l1')

        # åˆ†å±‚å‰ªææ¯”ä¾‹
        self.layer_specific_ratios = config.get('layer_specific_ratios', {
            'backbone': 0.2,
            'neck': 0.3,
            'decoder': 0.1
        })

        # å‰ªææ–¹æ³•å‚æ•°
        self.prune_params = config.get('prune_params', {
            'n': 1,
            'dim': 0
        })

    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'enabled': self.enabled,
            'pruning_strategy': self.pruning_strategy,
            'pruning_ratio': self.pruning_ratio,
            'pruning_layers': self.pruning_layers,
            'pruning_epoch': self.pruning_epoch,
            'min_acc_drop': self.min_acc_drop,
            'finetune_epochs': self.finetune_epochs,
            'prune_criteria': self.prune_criteria,
            'layer_specific_ratios': self.layer_specific_ratios,
            'prune_params': self.prune_params
        }

class PruningManager:
    """å‰ªæç®¡ç†å™¨"""
    def __init__(self, config: PruningConfig, model: nn.Module):
        self.config = config
        self.model = model
        self.pruned_layers = []
        self.original_model = None
        self.pruning_applied = False

    def get_pruning_ratio(self, layer_name: str) -> float:
        """è·å–ç‰¹å®šå±‚çš„å‰ªææ¯”ä¾‹"""
        for layer_type, ratio in self.config.layer_specific_ratios.items():
            if layer_type in layer_name:
                return ratio
        return self.config.pruning_ratio

    def apply_pruning(self, epoch: int, current_acc: float, best_acc: float) -> bool:
        """åº”ç”¨å‰ªæ"""
        if not self.config.enabled:
            return False

        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°å‰ªææ¡ä»¶
        if epoch != self.config.pruning_epoch:
            return False

        # æ£€æŸ¥ç²¾åº¦æ˜¯å¦è¶³å¤Ÿé«˜
        if current_acc < best_acc * 0.95:
            return False

        print(f"ğŸ¯ å¼€å§‹å‰ªæ...")
        print(f"   - å½“å‰ç²¾åº¦: {current_acc:.4f}")
        print(f"   - æœ€ä½³ç²¾åº¦: {best_acc:.4f}")
        print(f"   - å‰ªæç­–ç•¥: {self.config.pruning_strategy}")

        # ä¿å­˜åŸå§‹æ¨¡å‹
        self.original_model = self.model.state_dict()

        # åº”ç”¨å‰ªæ
        if self.config.pruning_strategy == 'l1_unstructured':
            self._apply_l1_unstructured_pruning()
        elif self.config.pruning_strategy == 'l1_structured':
            self._apply_l1_structured_pruning()
        elif self.config.pruning_strategy == 'ln_structured':
            self._apply_ln_structured_pruning()

        self.pruning_applied = True
        print(f"âœ… å‰ªæå®Œæˆ")
        return True

    def _apply_l1_unstructured_pruning(self):
        """åº”ç”¨L1éç»“æ„åŒ–å‰ªæ"""
        # éå†æ¨¡å‹çš„æ‰€æœ‰å±‚
        for name, module in self.model.named_modules():
            # åªå‰ªæå·ç§¯å±‚å’Œå…¨è¿æ¥å±‚
            if isinstance(module, (nn.Conv2d, nn.Linear)):
                # æ£€æŸ¥æ˜¯å¦åœ¨æŒ‡å®šçš„å‰ªæå±‚ä¸­
                if any(layer_type in name for layer_type in self.config.pruning_layers):
                    pruning_ratio = self.get_pruning_ratio(name)
                    print(f"   - å‰ªæå±‚: {name}, å‰ªææ¯”ä¾‹: {pruning_ratio:.2f}")

                    # åº”ç”¨L1éç»“æ„åŒ–å‰ªæ
                    prune.l1_unstructured(module, name='weight', amount=pruning_ratio)
                    self.pruned_layers.append((name, module))

    def _apply_l1_structured_pruning(self):
        """åº”ç”¨L1ç»“æ„åŒ–å‰ªæ"""
        # éå†æ¨¡å‹çš„æ‰€æœ‰å±‚
        for name, module in self.model.named_modules():
            # åªå‰ªæå·ç§¯å±‚å’Œå…¨è¿æ¥å±‚
            if isinstance(module, (nn.Conv2d, nn.Linear)):
                # æ£€æŸ¥æ˜¯å¦åœ¨æŒ‡å®šçš„å‰ªæå±‚ä¸­
                if any(layer_type in name for layer_type in self.config.pruning_layers):
                    pruning_ratio = self.get_pruning_ratio(name)
                    print(f"   - å‰ªæå±‚: {name}, å‰ªææ¯”ä¾‹: {pruning_ratio:.2f}")

                    # åº”ç”¨L1ç»“æ„åŒ–å‰ªæ
                    prune.ln_structured(module, name='weight', amount=pruning_ratio, n=1, dim=0)
                    self.pruned_layers.append((name, module))

    def _apply_ln_structured_pruning(self):
        """åº”ç”¨LNç»“æ„åŒ–å‰ªæ"""
        # éå†æ¨¡å‹çš„æ‰€æœ‰å±‚
        for name, module in self.model.named_modules():
            # åªå‰ªæå·ç§¯å±‚å’Œå…¨è¿æ¥å±‚
            if isinstance(module, (nn.Conv2d, nn.Linear)):
                # æ£€æŸ¥æ˜¯å¦åœ¨æŒ‡å®šçš„å‰ªæå±‚ä¸­
                if any(layer_type in name for layer_type in self.config.pruning_layers):
                    pruning_ratio = self.get_pruning_ratio(name)
                    print(f"   - å‰ªæå±‚: {name}, å‰ªææ¯”ä¾‹: {pruning_ratio:.2f}")

                    # åº”ç”¨LNç»“æ„åŒ–å‰ªæ
                    prune.ln_structured(module, name='weight', amount=pruning_ratio, **self.config.prune_params)
                    self.pruned_layers.append((name, module))

    def remove_pruning(self):
        """ç§»é™¤å‰ªæåŒ…è£…ï¼Œä½¿å‰ªææ°¸ä¹…åŒ–"""
        if not self.pruning_applied:
            return

        print(f"ğŸ”§ æ°¸ä¹…åŒ–å‰ªæ...")
        for name, module in self.pruned_layers:
            prune.remove(module, 'weight')
        print(f"âœ… å‰ªææ°¸ä¹…åŒ–å®Œæˆ")

    def restore_original_model(self):
        """æ¢å¤åŸå§‹æ¨¡å‹"""
        if self.original_model is not None:
            self.model.load_state_dict(self.original_model)
            self.pruning_applied = False
            self.pruned_layers = []
            print(f"ğŸ”„ æ¢å¤åŸå§‹æ¨¡å‹å®Œæˆ")

    def calculate_pruning_ratio(self) -> float:
        """è®¡ç®—å®é™…å‰ªææ¯”ä¾‹"""
        if not self.pruning_applied:
            return 0.0

        total_params = 0
        pruned_params = 0

        for name, module in self.model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear)):
                # æ£€æŸ¥æ˜¯å¦æœ‰weight_maskå±æ€§ï¼ˆå‰ªæåçš„æ ‡å¿—ï¼‰
                if hasattr(module, 'weight_mask'):
                    weight = module.weight
                    mask = module.weight_mask
                    total_params += weight.numel()
                    pruned_params += (mask == 0).sum().item()
                else:
                    weight = module.weight
                    total_params += weight.numel()

        if total_params == 0:
            return 0.0

        return pruned_params / total_params

    def get_pruned_model_info(self) -> Dict:
        """è·å–å‰ªæåçš„æ¨¡å‹ä¿¡æ¯"""
        info = {
            'pruning_applied': self.pruning_applied,
            'pruned_layers_count': len(self.pruned_layers),
            'pruning_ratio': self.calculate_pruning_ratio(),
            'pruned_layers': [name for name, _ in self.pruned_layers]
        }
        return info

    def is_pruning_time(self, epoch: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦åˆ°è¾¾å‰ªææ—¶é—´"""
        return self.config.enabled and epoch == self.config.pruning_epoch

    def is_finetuning_time(self, epoch: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¤„äºå¾®è°ƒé˜¶æ®µ"""
        if not self.config.enabled or not self.pruning_applied:
            return False

        start_finetune_epoch = self.config.pruning_epoch + 1
        end_finetune_epoch = start_finetune_epoch + self.config.finetune_epochs

        return start_finetune_epoch <= epoch < end_finetune_epoch

    def get_finetune_lr_multiplier(self, epoch: int) -> float:
        """è·å–å¾®è°ƒé˜¶æ®µçš„å­¦ä¹ ç‡ä¹˜æ•°"""
        if not self.is_finetuning_time(epoch):
            return 1.0

        # å¾®è°ƒé˜¶æ®µä½¿ç”¨è¾ƒä½çš„å­¦ä¹ ç‡
        return 0.1

def create_pruning_config(args, config_file: Optional[str] = None) -> PruningConfig:
    """åˆ›å»ºå‰ªæé…ç½®"""
    config = {
        'enabled': args.enable_pruning,
        'pruning_strategy': args.pruning_strategy,
        'pruning_ratio': args.pruning_ratio,
        'pruning_layers': args.pruning_layers,
        'pruning_epoch': args.pruning_epoch,
        'min_acc_drop': args.min_acc_drop,
        'finetune_epochs': args.finetune_epochs,
        'prune_criteria': args.prune_criteria,
        'layer_specific_ratios': {
            'backbone': args.backbone_pruning_ratio,
            'neck': args.neck_pruning_ratio,
            'decoder': args.decoder_pruning_ratio
        }
    }

    return PruningConfig(config)

class QuantizationStrategy(Enum):
    """é‡åŒ–ç­–ç•¥æšä¸¾"""
    INT8_DYN_ACT_INT4_WEIGHT = "int8_dyn_act_int4_weight"
    INT8_WEIGHT_ONLY = "int8_weight_only"
    INT4_WEIGHT_ONLY = "int4_weight_only"
    INT8_DYNAMIC_ACT_INT8_WEIGHT = "int8_dynamic_activation_int8_weight"
    MIXED_PRECISION = "mixed_precision"

class ObserverType(Enum):
    """è§‚å¯Ÿå™¨ç±»å‹æšä¸¾"""
    MOVING_AVERAGE = "moving_average"
    MIN_MAX = "min_max"
    PERCENTILE = "percentile"
    HISTOGRAM = "histogram"

class QuantizationGranularity(Enum):
    """é‡åŒ–ç²’åº¦æšä¸¾"""
    PER_TENSOR = "per_tensor"
    PER_CHANNEL = "per_channel"
    PER_GROUP = "per_group"

@dataclass
class QuantizationConfig:
    """é‡åŒ–é…ç½®æ•°æ®ç±»"""

    # åŸºç¡€é…ç½®
    enabled: bool = True
    strategy: QuantizationStrategy = QuantizationStrategy.INT8_DYN_ACT_INT4_WEIGHT
    quantization_aware_training: bool = True
    post_training_quantization: bool = False

    # è®­ç»ƒé…ç½®
    qat_epochs: int = 5
    ptq_epochs: int = 2
    calibration_batches: int = 100
    qat_learning_rate_multiplier: float = 0.1

    # é‡åŒ–ä½æ•°é…ç½®
    weight_bits: int = 4
    activation_bits: int = 8
    mixed_precision: bool = True

    # è§‚å¯Ÿå™¨é…ç½®
    observer_type: ObserverType = ObserverType.MOVING_AVERAGE
    observer_momentum: float = 0.1
    quantization_granularity: QuantizationGranularity = QuantizationGranularity.PER_CHANNEL

    # å±‚é…ç½®
    quantization_layers: List[str] = field(default_factory=lambda: ['linear', 'conv2d', 'attention'])
    excluded_layers: List[str] = field(default_factory=lambda: ['embedding', 'layernorm', 'batchnorm'])

    # æŸå¤±é…ç½®
    quantization_loss_weight: float = 0.01
    temperature_distillation: float = 4.0
    distillation_weight: float = 0.3

    # é«˜çº§é…ç½®
    dynamic_quantization: bool = False
    static_quantization: bool = True
    symmetric_quantization: bool = True
    clipping_threshold: float = 1.0

    # æ€§èƒ½ä¼˜åŒ–
    enable_cuda_graphs: bool = False
    memory_efficient: bool = True
    compile_model: bool = False

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'enabled': self.enabled,
            'strategy': self.strategy.value,
            'quantization_aware_training': self.quantization_aware_training,
            'post_training_quantization': self.post_training_quantization,
            'qat_epochs': self.qat_epochs,
            'ptq_epochs': self.ptq_epochs,
            'calibration_batches': self.calibration_batches,
            'qat_learning_rate_multiplier': self.qat_learning_rate_multiplier,
            'weight_bits': self.weight_bits,
            'activation_bits': self.activation_bits,
            'mixed_precision': self.mixed_precision,
            'observer_type': self.observer_type.value,
            'observer_momentum': self.observer_momentum,
            'quantization_granularity': self.quantization_granularity.value,
            'quantization_layers': self.quantization_layers,
            'excluded_layers': self.excluded_layers,
            'quantization_loss_weight': self.quantization_loss_weight,
            'temperature_distillation': self.temperature_distillation,
            'distillation_weight': self.distillation_weight,
            'dynamic_quantization': self.dynamic_quantization,
            'static_quantization': self.static_quantization,
            'symmetric_quantization': self.symmetric_quantization,
            'clipping_threshold': self.clipping_threshold,
            'enable_cuda_graphs': self.enable_cuda_graphs,
            'memory_efficient': self.memory_efficient,
            'compile_model': self.compile_model,
        }

    @classmethod
    def from_dict(cls, config_dict: Dict[str, Any]) -> 'QuantizationConfig':
        """ä»å­—å…¸åˆ›å»ºé…ç½®"""
        config = cls()

        # åŸºç¡€é…ç½®
        config.enabled = config_dict.get('enabled', True)
        config.strategy = QuantizationStrategy(config_dict.get('strategy', 'int8_dyn_act_int4_weight'))
        config.quantization_aware_training = config_dict.get('quantization_aware_training', True)
        config.post_training_quantization = config_dict.get('post_training_quantization', False)

        # è®­ç»ƒé…ç½®
        config.qat_epochs = config_dict.get('qat_epochs', 5)
        config.ptq_epochs = config_dict.get('ptq_epochs', 2)
        config.calibration_batches = config_dict.get('calibration_batches', 100)
        config.qat_learning_rate_multiplier = config_dict.get('qat_learning_rate_multiplier', 0.1)

        # é‡åŒ–ä½æ•°é…ç½®
        config.weight_bits = config_dict.get('weight_bits', 4)
        config.activation_bits = config_dict.get('activation_bits', 8)
        config.mixed_precision = config_dict.get('mixed_precision', True)

        # è§‚å¯Ÿå™¨é…ç½®
        config.observer_type = ObserverType(config_dict.get('observer_type', 'moving_average'))
        config.observer_momentum = config_dict.get('observer_momentum', 0.1)
        config.quantization_granularity = QuantizationGranularity(
            config_dict.get('quantization_granularity', 'per_channel')
        )

        # å±‚é…ç½®
        config.quantization_layers = config_dict.get('quantization_layers', ['linear', 'conv2d', 'attention'])
        config.excluded_layers = config_dict.get('excluded_layers', ['embedding', 'layernorm', 'batchnorm'])

        # æŸå¤±é…ç½®
        config.quantization_loss_weight = config_dict.get('quantization_loss_weight', 0.01)
        config.temperature_distillation = config_dict.get('temperature_distillation', 4.0)
        config.distillation_weight = config_dict.get('distillation_weight', 0.3)

        # é«˜çº§é…ç½®
        config.dynamic_quantization = config_dict.get('dynamic_quantization', False)
        config.static_quantization = config_dict.get('static_quantization', True)
        config.symmetric_quantization = config_dict.get('symmetric_quantization', True)
        config.clipping_threshold = config_dict.get('clipping_threshold', 1.0)

        # æ€§èƒ½ä¼˜åŒ–
        config.enable_cuda_graphs = config_dict.get('enable_cuda_graphs', False)
        config.memory_efficient = config_dict.get('memory_efficient', True)
        config.compile_model = config_dict.get('compile_model', False)

        return config

    def save_to_file(self, filepath: str, format: str = 'json'):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        config_dict = self.to_dict()

        if format.lower() == 'json':
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(config_dict, f, indent=2, ensure_ascii=False)
        elif format.lower() in ['yaml', 'yml']:
            with open(filepath, 'w', encoding='utf-8') as f:
                yaml.dump(config_dict, f, default_flow_style=False, allow_unicode=True)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {format}")

    @classmethod
    def load_from_file(cls, filepath: str) -> 'QuantizationConfig':
        """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
        if filepath.endswith('.json'):
            with open(filepath, 'r', encoding='utf-8') as f:
                config_dict = json.load(f)
        elif filepath.endswith(('.yaml', '.yml')):
            with open(filepath, 'r', encoding='utf-8') as f:
                config_dict = yaml.safe_load(f)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {filepath}")

        return cls.from_dict(config_dict)

class AdaptiveQuantizationConfig:
    """è‡ªé€‚åº”é‡åŒ–é…ç½® - æ ¹æ®æ¨¡å‹å’Œä»»åŠ¡åŠ¨æ€è°ƒæ•´å‚æ•°"""

    def __init__(self, model: nn.Module, task_type: str = 'ocr'):
        self.model = model
        self.task_type = task_type
        self.base_config = QuantizationConfig()

    def analyze_model_structure(self) -> Dict[str, Any]:
        """åˆ†ææ¨¡å‹ç»“æ„ï¼Œä¸ºé‡åŒ–é…ç½®æä¾›å»ºè®®"""
        analysis = {
            'total_params': 0,
            'linear_layers': 0,
            'conv_layers': 0,
            'attention_layers': 0,
            'embedding_layers': 0,
            'layer_norm_layers': 0,
            'largest_layer_params': 0,
            'model_size_mb': 0,
        }

        total_params = 0
        layer_counts = {
            'linear': 0,
            'conv': 0,
            'attention': 0,
            'embedding': 0,
            'layernorm': 0,
        }

        largest_params = 0

        for name, module in self.model.named_modules():
            if isinstance(module, nn.Linear):
                layer_counts['linear'] += 1
                params = module.weight.numel() + (module.bias.numel() if module.bias is not None else 0)
                total_params += params
                largest_params = max(largest_params, params)
            elif isinstance(module, (nn.Conv1d, nn.Conv2d, nn.Conv3d)):
                layer_counts['conv'] += 1
                params = module.weight.numel() + (module.bias.numel() if module.bias is not None else 0)
                total_params += params
                largest_params = max(largest_params, params)
            elif isinstance(module, nn.MultiheadAttention):
                layer_counts['attention'] += 1
                # ä¼°ç®—æ³¨æ„åŠ›å±‚å‚æ•°é‡
                params = 4 * module.embed_dim * module.embed_dim  # Q, K, V, O æŠ•å½±
                total_params += params
                largest_params = max(largest_params, params)
            elif isinstance(module, nn.Embedding):
                layer_counts['embedding'] += 1
                params = module.num_embeddings * module.embedding_dim
                total_params += params
            elif isinstance(module, nn.LayerNorm):
                layer_counts['layernorm'] += 1
                params = 2 * module.normalized_shape[0]  # weight + bias
                total_params += params

        analysis['total_params'] = total_params
        analysis['linear_layers'] = layer_counts['linear']
        analysis['conv_layers'] = layer_counts['conv']
        analysis['attention_layers'] = layer_counts['attention']
        analysis['embedding_layers'] = layer_counts['embedding']
        analysis['layer_norm_layers'] = layer_counts['layernorm']
        analysis['largest_layer_params'] = largest_params
        analysis['model_size_mb'] = total_params * 4 / (1024 * 1024)  # float32

        return analysis

    def recommend_config(self, target_compression_ratio: float = 0.25,
                        preserve_accuracy: bool = True) -> QuantizationConfig:
        """æ ¹æ®æ¨¡å‹åˆ†æç»“æœæ¨èé‡åŒ–é…ç½®"""
        analysis = self.analyze_model_structure()

        config = QuantizationConfig()

        # æ ¹æ®æ¨¡å‹å¤§å°è°ƒæ•´ç­–ç•¥
        model_size_mb = analysis['model_size_mb']
        if model_size_mb < 10:  # å°æ¨¡å‹
            config.weight_bits = 8
            config.activation_bits = 8
            config.qat_epochs = 3
            config.quantization_loss_weight = 0.005
        elif model_size_mb < 50:  # ä¸­ç­‰æ¨¡å‹
            config.weight_bits = 4
            config.activation_bits = 8
            config.qat_epochs = 5
            config.quantization_loss_weight = 0.01
        else:  # å¤§æ¨¡å‹
            config.weight_bits = 4
            config.activation_bits = 4
            config.qat_epochs = 8
            config.quantization_loss_weight = 0.02

        # æ ¹æ®ä»»åŠ¡ç±»å‹è°ƒæ•´
        if self.task_type == 'ocr':
            # OCRä»»åŠ¡å¯¹ç²¾åº¦è¦æ±‚é«˜ï¼Œä½¿ç”¨ä¿å®ˆçš„é‡åŒ–ç­–ç•¥
            config.strategy = QuantizationStrategy.INT8_DYN_ACT_INT4_WEIGHT
            config.observer_type = ObserverType.MOVING_AVERAGE
            config.quantization_granularity = QuantizationGranularity.PER_CHANNEL
            config.temperature_distillation = 6.0
            config.distillation_weight = 0.5
        elif self.task_type == 'classification':
            # åˆ†ç±»ä»»åŠ¡å¯ä»¥ä½¿ç”¨æ›´æ¿€è¿›çš„é‡åŒ–
            config.strategy = QuantizationStrategy.INT4_WEIGHT_ONLY
            config.observer_type = ObserverType.MIN_MAX
            config.quantization_granularity = QuantizationGranularity.PER_TENSOR
        elif self.task_type == 'detection':
            # æ£€æµ‹ä»»åŠ¡å¹³è¡¡ç²¾åº¦å’Œé€Ÿåº¦
            config.strategy = QuantizationStrategy.INT8_WEIGHT_ONLY
            config.observer_type = ObserverType.PERCENTILE
            config.quantization_granularity = QuantizationGranularity.PER_CHANNEL

        # æ ¹æ®å‹ç¼©æ¯”ç›®æ ‡è°ƒæ•´
        if target_compression_ratio < 0.2:  # é«˜å‹ç¼©æ¯”
            config.weight_bits = 4
            config.activation_bits = 4
            config.qat_epochs = max(8, config.qat_epochs)
        elif target_compression_ratio > 0.5:  # ä½å‹ç¼©æ¯”
            config.weight_bits = 8
            config.activation_bits = 8
            config.qat_epochs = min(3, config.qat_epochs)

        # æ ¹æ®ç²¾åº¦è¦æ±‚è°ƒæ•´
        if preserve_accuracy:
            config.qat_learning_rate_multiplier = 0.05  # æ›´ä½çš„å­¦ä¹ ç‡
            config.quantization_loss_weight = 0.02  # æ›´é«˜çš„é‡åŒ–æŸå¤±æƒé‡
            config.temperature_distillation = 8.0  # æ›´é«˜çš„è’¸é¦æ¸©åº¦
            config.distillation_weight = 0.7  # æ›´é«˜çš„è’¸é¦æƒé‡

        return config

    def optimize_config_for_hardware(self, hardware_target: str = 'cpu') -> QuantizationConfig:
        """é’ˆå¯¹ç‰¹å®šç¡¬ä»¶ä¼˜åŒ–é…ç½®"""
        config = self.recommend_config()

        if hardware_target == 'cpu':
            # CPUä¼˜åŒ–ï¼šä½¿ç”¨å¯¹ç§°é‡åŒ–ï¼Œå‡å°‘è®¡ç®—å¤æ‚åº¦
            config.symmetric_quantization = True
            config.quantization_granularity = QuantizationGranularity.PER_TENSOR
            config.enable_cuda_graphs = False
            config.memory_efficient = True
        elif hardware_target == 'gpu':
            # GPUä¼˜åŒ–ï¼šä½¿ç”¨é€šé“çº§é‡åŒ–ï¼Œæé«˜ç²¾åº¦
            config.symmetric_quantization = False
            config.quantization_granularity = QuantizationGranularity.PER_CHANNEL
            config.enable_cuda_graphs = True
            config.memory_efficient = False
        elif hardware_target == 'mobile':
            # ç§»åŠ¨ç«¯ä¼˜åŒ–ï¼šæ¿€è¿›çš„é‡åŒ–ç­–ç•¥
            config.weight_bits = 4
            config.activation_bits = 4
            config.symmetric_quantization = True
            config.quantization_granularity = QuantizationGranularity.PER_TENSOR
            config.memory_efficient = True
            config.compile_model = True

        return config

class QuantizationConfigValidator:
    """é‡åŒ–é…ç½®éªŒè¯å™¨ - éªŒè¯é…ç½®çš„æœ‰æ•ˆæ€§å’Œå…¼å®¹æ€§"""

    @staticmethod
    def validate_config(config: QuantizationConfig) -> List[str]:
        """éªŒè¯é…ç½®å¹¶è¿”å›é”™è¯¯ä¿¡æ¯åˆ—è¡¨"""
        errors = []

        # åŸºç¡€éªŒè¯
        if not config.enabled:
            return errors

        # è®­ç»ƒç­–ç•¥éªŒè¯
        if config.quantization_aware_training and config.post_training_quantization:
            errors.append("ä¸èƒ½åŒæ—¶å¯ç”¨QATå’ŒPTQ")

        if not config.quantization_aware_training and not config.post_training_quantization:
            errors.append("å¿…é¡»é€‰æ‹©QATæˆ–PTQä¸­çš„ä¸€ç§è®­ç»ƒç­–ç•¥")

        # é‡åŒ–ä½æ•°éªŒè¯
        if config.weight_bits not in [1, 2, 4, 8]:
            errors.append("æƒé‡é‡åŒ–ä½æ•°å¿…é¡»æ˜¯1, 2, 4, æˆ– 8")

        if config.activation_bits not in [1, 2, 4, 8, 16]:
            errors.append("æ¿€æ´»é‡åŒ–ä½æ•°å¿…é¡»æ˜¯1, 2, 4, 8, æˆ– 16")

        # è®­ç»ƒè½®æ•°éªŒè¯
        if config.qat_epochs < 1:
            errors.append("QATè®­ç»ƒè½®æ•°å¿…é¡»å¤§äº0")

        if config.ptq_epochs < 1:
            errors.append("PTQè®­ç»ƒè½®æ•°å¿…é¡»å¤§äº0")

        # æ ¡å‡†æ‰¹æ¬¡éªŒè¯
        if config.calibration_batches < 10:
            errors.append("æ ¡å‡†æ‰¹æ¬¡æ•°é‡è‡³å°‘ä¸º10")

        # è¶…å‚æ•°èŒƒå›´éªŒè¯
        if not (0.001 <= config.qat_learning_rate_multiplier <= 1.0):
            errors.append("QATå­¦ä¹ ç‡å€æ•°å¿…é¡»åœ¨0.001åˆ°1.0ä¹‹é—´")

        if not (0.0 <= config.quantization_loss_weight <= 1.0):
            errors.append("é‡åŒ–æŸå¤±æƒé‡å¿…é¡»åœ¨0.0åˆ°1.0ä¹‹é—´")

        if not (1.0 <= config.temperature_distillation <= 20.0):
            errors.append("è’¸é¦æ¸©åº¦å¿…é¡»åœ¨1.0åˆ°20.0ä¹‹é—´")

        if not (0.0 <= config.distillation_weight <= 1.0):
            errors.append("è’¸é¦æƒé‡å¿…é¡»åœ¨0.0åˆ°1.0ä¹‹é—´")

        # è§‚å¯Ÿå™¨åŠ¨é‡éªŒè¯
        if not (0.0 <= config.observer_momentum <= 1.0):
            errors.append("è§‚å¯Ÿå™¨åŠ¨é‡å¿…é¡»åœ¨0.0åˆ°1.0ä¹‹é—´")

        # è£å‰ªé˜ˆå€¼éªŒè¯
        if config.clipping_threshold <= 0:
            errors.append("è£å‰ªé˜ˆå€¼å¿…é¡»å¤§äº0")

        return errors

    @staticmethod
    def warn_config(config: QuantizationConfig) -> List[str]:
        """æ£€æŸ¥é…ç½®å¹¶è¿”å›è­¦å‘Šä¿¡æ¯åˆ—è¡¨"""
        warnings = []

        if not config.enabled:
            return warnings

        # ç²¾åº¦è­¦å‘Š
        if config.weight_bits < 4 and config.activation_bits < 8:
            warnings.append("ä½é‡åŒ–ä½æ•°å¯èƒ½å¯¼è‡´æ˜¾è‘—çš„ç²¾åº¦æŸå¤±")

        # è®­ç»ƒè­¦å‘Š
        if config.qat_epochs < 3:
            warnings.append("QATè®­ç»ƒè½®æ•°è¾ƒå°‘ï¼Œå¯èƒ½æ— æ³•å……åˆ†ä¼˜åŒ–é‡åŒ–å‚æ•°")

        if config.calibration_batches < 50:
            warnings.append("æ ¡å‡†æ‰¹æ¬¡è¾ƒå°‘ï¼Œå¯èƒ½å½±å“é‡åŒ–ç²¾åº¦")

        # æ€§èƒ½è­¦å‘Š
        if config.enable_cuda_graphs and config.quantization_granularity == QuantizationGranularity.PER_CHANNEL:
            warnings.append("CUDAå›¾ä¸é€šé“çº§é‡åŒ–åŒæ—¶ä½¿ç”¨å¯èƒ½å½±å“æ€§èƒ½")

        # è¶…å‚æ•°è­¦å‘Š
        if config.qat_learning_rate_multiplier > 0.5:
            warnings.append("QATå­¦ä¹ ç‡å€æ•°è¾ƒé«˜ï¼Œå¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®š")

        if config.quantization_loss_weight > 0.1:
            warnings.append("é‡åŒ–æŸå¤±æƒé‡è¾ƒé«˜ï¼Œå¯èƒ½å½±å“ä¸»ä»»åŠ¡æ€§èƒ½")

        if config.temperature_distillation > 10.0:
            warnings.append("è’¸é¦æ¸©åº¦è¿‡é«˜ï¼Œå¯èƒ½å¯¼è‡´çŸ¥è¯†è’¸é¦æ•ˆæœä¸ä½³")

        return warnings

@dataclass
class QuantizationMetrics:
    """é‡åŒ–è¯„ä¼°æŒ‡æ ‡"""
    # ç²¾åº¦æŒ‡æ ‡
    original_accuracy: float = 0.0
    quantized_accuracy: float = 0.0
    accuracy_drop: float = 0.0
    accuracy_drop_ratio: float = 0.0

    # æ¨¡å‹å¤§å°æŒ‡æ ‡
    original_model_size_mb: float = 0.0
    quantized_model_size_mb: float = 0.0
    compression_ratio: float = 0.0
    size_reduction_ratio: float = 0.0

    # æ¨ç†é€Ÿåº¦æŒ‡æ ‡
    original_inference_time_ms: float = 0.0
    quantized_inference_time_ms: float = 0.0
    speedup_ratio: float = 0.0

    # å†…å­˜ä½¿ç”¨æŒ‡æ ‡
    original_memory_usage_mb: float = 0.0
    quantized_memory_usage_mb: float = 0.0
    memory_reduction_ratio: float = 0.0

    # è®¡ç®—å¤æ‚åº¦æŒ‡æ ‡
    original_flops: int = 0
    quantized_flops: int = 0
    flops_reduction_ratio: float = 0.0

    # é‡åŒ–è¯¯å·®æŒ‡æ ‡
    mse_error: float = 0.0
    mae_error: float = 0.0
    snr_db: float = 0.0
    psnr_db: float = 0.0

    # æ¿€æ´»åˆ†å¸ƒæŒ‡æ ‡
    weight_clipping_ratio: float = 0.0
    activation_clipping_ratio: float = 0.0
    quantization_noise_std: float = 0.0

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'accuracy': {
                'original': self.original_accuracy,
                'quantized': self.quantized_accuracy,
                'drop': self.accuracy_drop,
                'drop_ratio': self.accuracy_drop_ratio,
            },
            'model_size': {
                'original_mb': self.original_model_size_mb,
                'quantized_mb': self.quantized_model_size_mb,
                'compression_ratio': self.compression_ratio,
                'reduction_ratio': self.size_reduction_ratio,
            },
            'inference_speed': {
                'original_time_ms': self.original_inference_time_ms,
                'quantized_time_ms': self.quantized_inference_time_ms,
                'speedup_ratio': self.speedup_ratio,
            },
            'memory_usage': {
                'original_mb': self.original_memory_usage_mb,
                'quantized_mb': self.quantized_memory_usage_mb,
                'reduction_ratio': self.memory_reduction_ratio,
            },
            'computational_complexity': {
                'original_flops': self.original_flops,
                'quantized_flops': self.quantized_flops,
                'reduction_ratio': self.flops_reduction_ratio,
            },
            'quantization_error': {
                'mse': self.mse_error,
                'mae': self.mae_error,
                'snr_db': self.snr_db,
                'psnr_db': self.psnr_db,
            },
            'activation_distribution': {
                'weight_clipping_ratio': self.weight_clipping_ratio,
                'activation_clipping_ratio': self.activation_clipping_ratio,
                'quantization_noise_std': self.quantization_noise_std,
            }
        }

class ModelAnalyzer:
    """æ¨¡å‹åˆ†æå™¨ - åˆ†ææ¨¡å‹ç»“æ„å’Œå¤æ‚åº¦"""

    def __init__(self, model: nn.Module):
        self.model = model
        self.layer_info = []

    def analyze_model_structure(self) -> Dict[str, Any]:
        """åˆ†ææ¨¡å‹ç»“æ„"""
        analysis = {
            'total_parameters': 0,
            'trainable_parameters': 0,
            'total_layers': 0,
            'layer_types': {},
            'parameter_distribution': {},
            'memory_usage_mb': 0,
            'model_size_mb': 0,
        }

        total_params = 0
        trainable_params = 0
        layer_types = {}
        param_distribution = {}

        for name, module in self.model.named_modules():
            if len(list(module.children())) == 0:  # å¶å­æ¨¡å—
                layer_type = module.__class__.__name__
                layer_types[layer_type] = layer_types.get(layer_type, 0) + 1

                # è®¡ç®—å‚æ•°æ•°é‡
                params = sum(p.numel() for p in module.parameters())
                trainable = sum(p.numel() for p in module.parameters() if p.requires_grad)

                total_params += params
                trainable_params += trainable

                if params > 0:
                    param_distribution[name] = {
                        'type': layer_type,
                        'parameters': params,
                        'trainable': trainable,
                        'size_mb': params * 4 / (1024 * 1024)  # float32
                    }

        analysis['total_parameters'] = total_params
        analysis['trainable_parameters'] = trainable_params
        analysis['total_layers'] = sum(layer_types.values())
        analysis['layer_types'] = layer_types
        analysis['parameter_distribution'] = param_distribution
        analysis['memory_usage_mb'] = self._estimate_memory_usage()
        analysis['model_size_mb'] = total_params * 4 / (1024 * 1024)

        return analysis

    def _estimate_memory_usage(self) -> float:
        """ä¼°ç®—æ¨¡å‹å†…å­˜ä½¿ç”¨"""
        # è€ƒè™‘æ¨¡å‹å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€
        param_size = sum(p.numel() for p in self.model.parameters()) * 4  # float32
        grad_size = sum(p.numel() for p in self.model.parameters() if p.requires_grad) * 4
        optimizer_size = grad_size * 2  # Adamä¼˜åŒ–å™¨æœ‰åŠ¨é‡å’Œæ–¹å·®

        total_bytes = param_size + grad_size + optimizer_size
        return total_bytes / (1024 * 1024)  # MB

    def count_flops(self, input_shape: Tuple[int, ...]) -> int:
        """ä¼°ç®—æ¨¡å‹FLOPs"""
        # ç®€åŒ–çš„FLOPsä¼°ç®—
        flops = 0

        def hook_fn(module, input, output):
            nonlocal flops

            if isinstance(module, nn.Conv2d):
                # Conv2d FLOPsä¼°ç®—
                batch_size = input[0].shape[0]
                output_shape = output.shape
                kernel_flops = module.kernel_size[0] * module.kernel_size[1] * module.in_channels
                output_elements = batch_size * output_shape[2] * output_shape[3] * module.out_channels
                flops += kernel_flops * output_elements

            elif isinstance(module, nn.Linear):
                # Linear FLOPsä¼°ç®—
                flops += module.in_features * module.out_features * input[0].shape[0]

        hooks = []
        for module in self.model.modules():
            hooks.append(module.register_forward_hook(hook_fn))

        # è¿è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­
        dummy_input = torch.randn(input_shape)
        self.model(dummy_input)

        # ç§»é™¤hooks
        for hook in hooks:
            hook.remove()

        return flops

class QuantizationEvaluator:
    """é‡åŒ–è¯„ä¼°å™¨ - å…¨é¢è¯„ä¼°é‡åŒ–æ•ˆæœ"""

    def __init__(self, original_model: nn.Module, quantized_model: nn.Module,
                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):
        self.original_model = original_model
        self.quantized_model = quantized_model
        self.device = device
        self.analyzer = ModelAnalyzer(original_model)

    def evaluate_quantization(self, dataloader, num_batches: int = 100) -> QuantizationMetrics:
        """å…¨é¢è¯„ä¼°é‡åŒ–æ•ˆæœ"""
        print("ğŸ” å¼€å§‹é‡åŒ–æ•ˆæœè¯„ä¼°...")

        metrics = QuantizationMetrics()

        # 1. è¯„ä¼°æ¨¡å‹ç²¾åº¦
        print("ğŸ“Š è¯„ä¼°æ¨¡å‹ç²¾åº¦...")
        original_accuracy = self._evaluate_accuracy(self.original_model, dataloader, num_batches)
        quantized_accuracy = self._evaluate_accuracy(self.quantized_model, dataloader, num_batches)

        metrics.original_accuracy = original_accuracy
        metrics.quantized_accuracy = quantized_accuracy
        metrics.accuracy_drop = original_accuracy - quantized_accuracy
        metrics.accuracy_drop_ratio = metrics.accuracy_drop / original_accuracy if original_accuracy > 0 else 0

        # 2. è¯„ä¼°æ¨¡å‹å¤§å°
        print("ğŸ“ è¯„ä¼°æ¨¡å‹å¤§å°...")
        original_size = self._get_model_size(self.original_model)
        quantized_size = self._get_model_size(self.quantized_model)

        metrics.original_model_size_mb = original_size
        metrics.quantized_model_size_mb = quantized_size
        metrics.compression_ratio = original_size / quantized_size if quantized_size > 0 else 1.0
        metrics.size_reduction_ratio = (original_size - quantized_size) / original_size if original_size > 0 else 0.0

        # 3. è¯„ä¼°æ¨ç†é€Ÿåº¦
        print("âš¡ è¯„ä¼°æ¨ç†é€Ÿåº¦...")
        original_time = self._benchmark_inference(self.original_model, dataloader, num_batches=50)
        quantized_time = self._benchmark_inference(self.quantized_model, dataloader, num_batches=50)

        metrics.original_inference_time_ms = original_time
        metrics.quantized_inference_time_ms = quantized_time
        metrics.speedup_ratio = original_time / quantized_time if quantized_time > 0 else 1.0

        # 4. è¯„ä¼°å†…å­˜ä½¿ç”¨
        print("ğŸ’¾ è¯„ä¼°å†…å­˜ä½¿ç”¨...")
        original_memory = self._measure_memory_usage(self.original_model, dataloader, num_batches=20)
        quantized_memory = self._measure_memory_usage(self.quantized_model, dataloader, num_batches=20)

        metrics.original_memory_usage_mb = original_memory
        metrics.quantized_memory_usage_mb = quantized_memory
        metrics.memory_reduction_ratio = (original_memory - quantized_memory) / original_memory if original_memory > 0 else 0.0

        # 5. è¯„ä¼°é‡åŒ–è¯¯å·®
        print("ğŸ”¬ è¯„ä¼°é‡åŒ–è¯¯å·®...")
        mse, mae, snr, psnr = self._calculate_quantization_error(dataloader, num_batches=30)

        metrics.mse_error = mse
        metrics.mae_error = mae
        metrics.snr_db = snr
        metrics.psnr_db = psnr

        # 6. è¯„ä¼°æ¿€æ´»åˆ†å¸ƒ
        print("ğŸ“ˆ è¯„ä¼°æ¿€æ´»åˆ†å¸ƒ...")
        weight_clip, act_clip, noise_std = self._analyze_activation_distribution(dataloader, num_batches=20)

        metrics.weight_clipping_ratio = weight_clip
        metrics.activation_clipping_ratio = act_clip
        metrics.quantization_noise_std = noise_std

        print("âœ… é‡åŒ–è¯„ä¼°å®Œæˆï¼")
        return metrics

    def _evaluate_accuracy(self, model: nn.Module, dataloader, num_batches: int) -> float:
        """è¯„ä¼°æ¨¡å‹ç²¾åº¦"""
        model.eval()
        correct = 0
        total = 0

        with torch.no_grad():
            for i, batch in enumerate(dataloader):
                if i >= num_batches:
                    break

                images = batch['images'].to(self.device)
                labels = batch['labels'].to(self.device)

                outputs = model(images)

                # å‡è®¾æ˜¯CTCæ¨¡å‹ï¼Œä½¿ç”¨CTCè§£ç 
                if 'ctc_logits' in outputs:
                    predictions = outputs['ctc_logits'].argmax(dim=-1)
                    # ç®€åŒ–çš„CTCè§£ç å’Œå‡†ç¡®ç‡è®¡ç®—
                    for pred, label in zip(predictions, labels):
                        pred_text = self._ctc_decode(pred.cpu().numpy())
                        label_text = self._ctc_decode(label.cpu().numpy())
                        if pred_text == label_text:
                            correct += 1
                        total += 1

        return correct / total if total > 0 else 0.0

    def _ctc_decode(self, predictions: np.ndarray) -> str:
        """ç®€åŒ–çš„CTCè§£ç """
        # å»é™¤é‡å¤å’Œç©ºç™½
        decoded = []
        prev = None
        for pred in predictions:
            if pred != prev and pred != 0:  # å‡è®¾0æ˜¯ç©ºç™½ç¬¦
                decoded.append(pred)
            prev = pred
        return ''.join(map(str, decoded))

    def _get_model_size(self, model: nn.Module) -> float:
        """è·å–æ¨¡å‹å¤§å°ï¼ˆMBï¼‰"""
        # ä¿å­˜æ¨¡å‹çŠ¶æ€å­—å…¸å¹¶è®¡ç®—å¤§å°
        state_dict = model.state_dict()
        size_bytes = 0

        for tensor in state_dict.values():
            size_bytes += tensor.numel() * tensor.element_size()

        return size_bytes / (1024 * 1024)  # MB

    def _benchmark_inference(self, model: nn.Module, dataloader, num_batches: int) -> float:
        """åŸºå‡†æµ‹è¯•æ¨ç†æ—¶é—´"""
        model.eval()
        times = []

        with torch.no_grad():
            for i, batch in enumerate(dataloader):
                if i >= num_batches:
                    break

                images = batch['images'].to(self.device)

                # é¢„çƒ­
                if i == 0:
                    for _ in range(5):
                        _ = model(images)

                # æ­£å¼æµ‹è¯•
                torch.cuda.synchronize() if torch.cuda.is_available() else None
                start_time = time.time()

                _ = model(images)

                torch.cuda.synchronize() if torch.cuda.is_available() else None
                end_time = time.time()

                times.append((end_time - start_time) * 1000)  # ms

        return np.mean(times) if times else 0.0

    def _measure_memory_usage(self, model: nn.Module, dataloader, num_batches: int) -> float:
        """æµ‹é‡å†…å­˜ä½¿ç”¨"""
        model.eval()
        memory_usage = []

        with torch.no_grad():
            for i, batch in enumerate(dataloader):
                if i >= num_batches:
                    break

                images = batch['images'].to(self.device)

                # æ¸…ç†å†…å­˜
                torch.cuda.empty_cache() if torch.cuda.is_available() else None

                # è®°å½•åˆå§‹å†…å­˜
                initial_memory = self._get_current_memory_usage()

                # æ¨ç†
                _ = model(images)

                # è®°å½•å³°å€¼å†…å­˜
                peak_memory = self._get_current_memory_usage()
                memory_usage.append(peak_memory - initial_memory)

        return np.mean(memory_usage) if memory_usage else 0.0

    def _get_current_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨ï¼ˆMBï¼‰"""
        if torch.cuda.is_available():
            return torch.cuda.max_memory_allocated() / (1024 * 1024)
        else:
            return psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)

    def _calculate_quantization_error(self, dataloader, num_batches: int) -> Tuple[float, float, float, float]:
        """è®¡ç®—é‡åŒ–è¯¯å·®"""
        self.original_model.eval()
        self.quantized_model.eval()

        mse_errors = []
        mae_errors = []
        snr_values = []
        psnr_values = []

        with torch.no_grad():
            for i, batch in enumerate(dataloader):
                if i >= num_batches:
                    break

                images = batch['images'].to(self.device)

                # è·å–åŸå§‹å’Œé‡åŒ–æ¨¡å‹çš„è¾“å‡º
                original_output = self.original_model(images)
                quantized_output = self.quantized_model(images)

                # æå–ç‰¹å¾è¿›è¡Œæ¯”è¾ƒ
                if isinstance(original_output, dict):
                    # å¤„ç†å­—å…¸è¾“å‡º
                    for key in original_output:
                        if torch.is_tensor(original_output[key]):
                            orig_feat = original_output[key].flatten()
                            quant_feat = quantized_output[key].flatten()

                            # è®¡ç®—è¯¯å·®
                            mse = F.mse_loss(quant_feat, orig_feat).item()
                            mae = F.l1_loss(quant_feat, orig_feat).item()

                            # è®¡ç®—SNR
                            signal_power = torch.mean(orig_feat ** 2).item()
                            noise_power = torch.mean((quant_feat - orig_feat) ** 2).item()
                            snr = 10 * np.log10(signal_power / (noise_power + 1e-10)) if noise_power > 0 else float('inf')

                            # è®¡ç®—PSNR
                            max_value = torch.max(torch.abs(orig_feat)).item()
                            psnr = 20 * np.log10(max_value / (np.sqrt(mse) + 1e-10)) if mse > 0 else float('inf')

                            mse_errors.append(mse)
                            mae_errors.append(mae)
                            snr_values.append(snr)
                            psnr_values.append(psnr)

        return (np.mean(mse_errors) if mse_errors else 0.0,
                np.mean(mae_errors) if mae_errors else 0.0,
                np.mean(snr_values) if snr_values else 0.0,
                np.mean(psnr_values) if psnr_values else 0.0)

    def _analyze_activation_distribution(self, dataloader, num_batches: int) -> Tuple[float, float, float]:
        """åˆ†ææ¿€æ´»åˆ†å¸ƒ"""
        self.original_model.eval()
        self.quantized_model.eval()

        weight_clips = []
        act_clips = []
        noise_stds = []

        with torch.no_grad():
            for i, batch in enumerate(dataloader):
                if i >= num_batches:
                    break

                images = batch['images'].to(self.device)

                # è·å–æ¿€æ´»å€¼
                original_activations = self._get_activations(self.original_model, images)
                quantized_activations = self._get_activations(self.quantized_model, images)

                for orig_act, quant_act in zip(original_activations, quantized_activations):
                    # è®¡ç®—è£å‰ªæ¯”ä¾‹
                    orig_range = torch.max(torch.abs(orig_act)).item()
                    quant_range = torch.max(torch.abs(quant_act)).item()
                    clipping_ratio = 1.0 - (quant_range / (orig_range + 1e-10))

                    # è®¡ç®—é‡åŒ–å™ªå£°
                    noise = quant_act - orig_act
                    noise_std = torch.std(noise).item()

                    act_clips.append(clipping_ratio)
                    noise_stds.append(noise_std)

        return 0.0, np.mean(act_clips) if act_clips else 0.0, np.mean(noise_stds) if noise_stds else 0.0

    def _get_activations(self, model: nn.Module, input_tensor: torch.Tensor) -> List[torch.Tensor]:
        """è·å–æ¨¡å‹æ¿€æ´»å€¼"""
        activations = []

        def hook_fn(module, input, output):
            if torch.is_tensor(output):
                activations.append(output.detach().flatten())

        hooks = []
        for module in model.modules():
            if isinstance(module, (nn.ReLU, nn.GELU, nn.SiLU, nn.LeakyReLU)):
                hooks.append(module.register_forward_hook(hook_fn))

        _ = model(input_tensor)

        for hook in hooks:
            hook.remove()

        return activations

    def generate_report(self, metrics: QuantizationMetrics, output_path: str = 'quantization_report.json'):
        """ç”Ÿæˆé‡åŒ–è¯„ä¼°æŠ¥å‘Š"""
        report = {
            'summary': {
                'accuracy_drop': f"{metrics.accuracy_drop:.4f} ({metrics.accuracy_drop_ratio*100:.2f}%)",
                'compression_ratio': f"{metrics.compression_ratio:.2f}x",
                'speedup_ratio': f"{metrics.speedup_ratio:.2f}x",
                'memory_reduction': f"{metrics.memory_reduction_ratio*100:.1f}%",
            },
            'detailed_metrics': metrics.to_dict(),
            'recommendations': self._generate_recommendations(metrics),
            'grade': self._grade_quantization(metrics),
        }

        # ä¿å­˜æŠ¥å‘Š
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“Š é‡åŒ–è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        return report

    def _generate_recommendations(self, metrics: QuantizationMetrics) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []

        # ç²¾åº¦ç›¸å…³å»ºè®®
        if metrics.accuracy_drop_ratio > 0.05:  # è¶…è¿‡5%çš„ç²¾åº¦ä¸‹é™
            recommendations.append("ç²¾åº¦ä¸‹é™è¾ƒå¤§ï¼Œå»ºè®®å¢åŠ QATè®­ç»ƒè½®æ•°æˆ–é™ä½é‡åŒ–å¼ºåº¦")
            recommendations.append("è€ƒè™‘ä½¿ç”¨çŸ¥è¯†è’¸é¦æ¥ä¿æŒç²¾åº¦")

        if metrics.accuracy_drop_ratio < 0.01:  # å°äº1%çš„ç²¾åº¦ä¸‹é™
            recommendations.append("ç²¾åº¦ä¿æŒè‰¯å¥½ï¼Œå¯ä»¥å°è¯•æ›´æ¿€è¿›çš„é‡åŒ–ç­–ç•¥")

        # å‹ç¼©ç›¸å…³å»ºè®®
        if metrics.compression_ratio < 2.0:  # å‹ç¼©æ¯”å°äº2å€
            recommendations.append("å‹ç¼©æ¯”åä½ï¼Œå¯ä»¥å°è¯•æ›´ä½çš„é‡åŒ–ä½æ•°")

        if metrics.compression_ratio > 8.0:  # å‹ç¼©æ¯”å¤§äº8å€
            recommendations.append("å‹ç¼©æ¯”å¾ˆé«˜ï¼Œéœ€è¦å¯†åˆ‡å…³æ³¨ç²¾åº¦å˜åŒ–")

        # é€Ÿåº¦ç›¸å…³å»ºè®®
        if metrics.speedup_ratio < 1.2:  # åŠ é€Ÿæ¯”å°äº1.2å€
            recommendations.append("åŠ é€Ÿæ•ˆæœä¸æ˜æ˜¾ï¼Œæ£€æŸ¥é‡åŒ–å®ç°æˆ–ç¡¬ä»¶å…¼å®¹æ€§")

        # å†…å­˜ç›¸å…³å»ºè®®
        if metrics.memory_reduction_ratio < 0.3:  # å†…å­˜å‡å°‘å°äº30%
            recommendations.append("å†…å­˜å‡å°‘æœ‰é™ï¼Œè€ƒè™‘ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼")

        # é‡åŒ–è¯¯å·®ç›¸å…³å»ºè®®
        if metrics.snr_db < 20:  # SNRå°äº20dB
            recommendations.append("é‡åŒ–å™ªå£°è¾ƒå¤§ï¼Œå»ºè®®æ”¹è¿›é‡åŒ–ç­–ç•¥æˆ–æ ¡å‡†æ–¹æ³•")

        return recommendations

    def _grade_quantization(self, metrics: QuantizationMetrics) -> str:
        """ç»™é‡åŒ–æ•ˆæœæ‰“åˆ†"""
        score = 0

        # ç²¾åº¦åˆ†æ•° (40åˆ†)
        if metrics.accuracy_drop_ratio <= 0.01:
            score += 40
        elif metrics.accuracy_drop_ratio <= 0.03:
            score += 30
        elif metrics.accuracy_drop_ratio <= 0.05:
            score += 20
        else:
            score += 10

        # å‹ç¼©åˆ†æ•° (30åˆ†)
        if metrics.compression_ratio >= 4.0:
            score += 30
        elif metrics.compression_ratio >= 2.0:
            score += 20
        elif metrics.compression_ratio >= 1.5:
            score += 10

        # é€Ÿåº¦åˆ†æ•° (20åˆ†)
        if metrics.speedup_ratio >= 1.5:
            score += 20
        elif metrics.speedup_ratio >= 1.2:
            score += 15
        elif metrics.speedup_ratio >= 1.1:
            score += 10

        # å†…å­˜åˆ†æ•° (10åˆ†)
        if metrics.memory_reduction_ratio >= 0.5:
            score += 10
        elif metrics.memory_reduction_ratio >= 0.3:
            score += 7
        elif metrics.memory_reduction_ratio >= 0.1:
            score += 5

        # è¯„çº§
        if score >= 90:
            return "A+ (ä¼˜ç§€)"
        elif score >= 80:
            return "A (è‰¯å¥½)"
        elif score >= 70:
            return "B (ä¸­ç­‰)"
        elif score >= 60:
            return "C (åŠæ ¼)"
        else:
            return "D (éœ€æ”¹è¿›)"

    def visualize_results(self, metrics: QuantizationMetrics, save_path: str = 'quantization_visualization.png'):
        """å¯è§†åŒ–é‡åŒ–ç»“æœ"""
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        fig.suptitle('é‡åŒ–æ•ˆæœå¯è§†åŒ–', fontsize=16)

        # 1. ç²¾åº¦å¯¹æ¯”
        ax = axes[0, 0]
        categories = ['åŸå§‹æ¨¡å‹', 'é‡åŒ–æ¨¡å‹']
        accuracies = [metrics.original_accuracy, metrics.quantized_accuracy]
        bars = ax.bar(categories, accuracies, color=['blue', 'red'], alpha=0.7)
        ax.set_ylabel('å‡†ç¡®ç‡')
        ax.set_title('æ¨¡å‹ç²¾åº¦å¯¹æ¯”')
        ax.set_ylim(0, 1.1 * max(accuracies))

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, acc in zip(bars, accuracies):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{acc:.4f}', ha='center', va='bottom')

        # 2. æ¨¡å‹å¤§å°å¯¹æ¯”
        ax = axes[0, 1]
        sizes = [metrics.original_model_size_mb, metrics.quantized_model_size_mb]
        bars = ax.bar(categories, sizes, color=['blue', 'red'], alpha=0.7)
        ax.set_ylabel('æ¨¡å‹å¤§å° (MB)')
        ax.set_title('æ¨¡å‹å¤§å°å¯¹æ¯”')

        for bar, size in zip(bars, sizes):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{size:.1f}MB', ha='center', va='bottom')

        # 3. æ¨ç†é€Ÿåº¦å¯¹æ¯”
        ax = axes[0, 2]
        times = [metrics.original_inference_time_ms, metrics.quantized_inference_time_ms]
        bars = ax.bar(categories, times, color=['blue', 'red'], alpha=0.7)
        ax.set_ylabel('æ¨ç†æ—¶é—´ (ms)')
        ax.set_title('æ¨ç†é€Ÿåº¦å¯¹æ¯”')

        for bar, time in zip(bars, times):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{time:.2f}ms', ha='center', va='bottom')

        # 4. å‹ç¼©æ¯”å’ŒåŠ é€Ÿæ¯”
        ax = axes[1, 0]
        ratios = [metrics.compression_ratio, metrics.speedup_ratio]
        ratio_labels = ['å‹ç¼©æ¯”', 'åŠ é€Ÿæ¯”']
        bars = ax.bar(ratio_labels, ratios, color=['green', 'orange'], alpha=0.7)
        ax.set_ylabel('æ¯”å€¼')
        ax.set_title('å‹ç¼©å’ŒåŠ é€Ÿæ•ˆæœ')

        for bar, ratio, label in zip(bars, ratios, ratio_labels):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{ratio:.2f}x', ha='center', va='bottom')

        # 5. é‡åŒ–è¯¯å·®
        ax = axes[1, 1]
        error_metrics = ['MSE', 'MAE']
        errors = [metrics.mse_error, metrics.mae_error]
        bars = ax.bar(error_metrics, errors, color=['purple', 'brown'], alpha=0.7)
        ax.set_ylabel('è¯¯å·®å€¼')
        ax.set_title('é‡åŒ–è¯¯å·®')
        ax.set_yscale('log')  # å¯¹æ•°åæ ‡

        for bar, error, metric in zip(bars, errors, error_metrics):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{error:.4f}', ha='center', va='bottom')

        # 6. ä¿¡å™ªæ¯”
        ax = axes[1, 2]
        snr_metrics = ['SNR (dB)', 'PSNR (dB)']
        snr_values = [metrics.snr_db, metrics.psnr_db]
        bars = ax.bar(snr_metrics, snr_values, color=['cyan', 'magenta'], alpha=0.7)
        ax.set_ylabel('dB')
        ax.set_title('ä¿¡å·è´¨é‡')

        for bar, snr, metric in zip(bars, snr_values, snr_metrics):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{snr:.1f}dB', ha='center', va='bottom')

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"ğŸ“ˆ é‡åŒ–å¯è§†åŒ–ç»“æœå·²ä¿å­˜åˆ°: {save_path}")

class QuantizationManager:
    """é‡åŒ–ç®¡ç†å™¨ - å¤„ç†æ‰€æœ‰é‡åŒ–ç›¸å…³æ“ä½œ"""

    def __init__(self, config: Dict, model: torch.nn.Module):
        self.config = config
        self.model = model
        self.quantizer = None
        self.calibration_data = []
        self.original_model = None

    def prepare_model_for_quantization(self) -> torch.nn.Module:
        """å‡†å¤‡æ¨¡å‹è¿›è¡Œé‡åŒ–"""
        if not self.config['enabled']:
            return self.model

        print("ğŸ”§ å‡†å¤‡æ¨¡å‹é‡åŒ–...")

        # ä¿å­˜åŸå§‹æ¨¡å‹ç”¨äºçŸ¥è¯†è’¸é¦
        self.original_model = self.model

        # æ ¹æ®é…ç½®é€‰æ‹©é‡åŒ–ç­–ç•¥
        strategy = self.config['quantization_strategy']

        if self.config['quantization_aware_training']:
            print(f"ğŸ¯ å¯ç”¨é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (QAT): {strategy}")
            self.model = self._apply_qat_quantization()
        elif self.config['post_training_quantization']:
            print(f"ğŸ¯ å¯ç”¨è®­ç»ƒåé‡åŒ– (PTQ): {strategy}")
            self.model = self._apply_ptq_quantization()
        else:
            print("âš ï¸  é‡åŒ–å·²å¯ç”¨ä½†æœªæŒ‡å®šè®­ç»ƒç­–ç•¥")

        return self.model

    def _apply_qat_quantization(self) -> torch.nn.Module:
        """åº”ç”¨é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ"""
        strategy = self.config['quantization_strategy']

        if strategy == 'int8_dyn_act_int4_weight':
            # ä½¿ç”¨é»˜è®¤å‚æ•°åˆå§‹åŒ–é‡åŒ–å™¨
            quantizer = Int8DynActInt4WeightQATQuantizer()
        elif strategy == 'int4_weight_only':
            quantizer = Int4WeightOnlyQATQuantizer()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„QATç­–ç•¥: {strategy}")

        # åº”ç”¨é‡åŒ–
        quantized_model = quantizer.prepare(self.model)

        print(f"âœ… QATé‡åŒ–åº”ç”¨å®Œæˆ: {strategy}")
        return quantized_model

    def _apply_ptq_quantization(self) -> torch.nn.Module:
        """åº”ç”¨è®­ç»ƒåé‡åŒ–"""
        strategy = self.config['quantization_strategy']

        if strategy == 'int8_dyn_act_int4_weight':
            quantizer = int8_dynamic_activation_int4_weight
        elif strategy == 'int8_weight_only':
            quantizer = int8_weight_only
        elif strategy == 'int4_weight_only':
            quantizer = int4_weight_only
        elif strategy == 'int8_dynamic_activation_int8_weight':
            quantizer = int8_dynamic_activation_int8_weight
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„PTQç­–ç•¥: {strategy}")

        # åº”ç”¨é‡åŒ–
        quantize_(self.model, quantizer())

        print(f"âœ… PTQé‡åŒ–åº”ç”¨å®Œæˆ: {strategy}")
        return self.model

    def calibrate_model(self, dataloader: DataLoader, num_batches: int = None):
        """æ ¡å‡†é‡åŒ–æ¨¡å‹"""
        if not self.config['enabled'] or not self.config['post_training_quantization']:
            return

        print("ğŸ“Š å¼€å§‹æ¨¡å‹æ ¡å‡†...")

        if num_batches is None:
            num_batches = self.config['calibration_batches']

        self.model.eval()
        with torch.no_grad():
            for i, batch in enumerate(tqdm(dataloader, total=num_batches, desc='æ ¡å‡†')):
                if i >= num_batches:
                    break

                images = batch['images'].cuda()
                _ = self.model(images)

        print(f"âœ… æ¨¡å‹æ ¡å‡†å®Œæˆ ({num_batches} æ‰¹æ¬¡)")

    def get_quantization_loss(self, quantized_features: torch.Tensor,
                            original_features: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—é‡åŒ–æŸå¤±ç”¨äºçŸ¥è¯†è’¸é¦"""
        if not self.config['enabled']:
            return torch.tensor(0.0, device=quantized_features.device)

        # ç‰¹å¾è’¸é¦æŸå¤±
        distillation_loss = F.kl_div(
            F.log_softmax(quantized_features / self.config['temperature_distillation'], dim=-1),
            F.softmax(original_features / self.config['temperature_distillation'], dim=-1),
            reduction='batchmean'
        )

        # é‡åŒ–æ„ŸçŸ¥æŸå¤±
        quantization_loss = F.mse_loss(quantized_features, original_features.detach())

        total_loss = (self.config['distillation_weight'] * distillation_loss +
                     self.config['quantization_loss_weight'] * quantization_loss)

        return total_loss

    @torch.no_grad()
    def export_quantized_model(self, pruning: bool, path: str, epoch: int, best_cer: float, best_em: float, example_input: torch.Tensor, opt: Dict, scaler: Dict, model: nn.Module):
        """å¯¼å‡ºé‡åŒ–æ¨¡å‹"""

        # ç¡®ä¿æ¨¡å‹åœ¨è¯„ä¼°æ¨¡å¼
        model.eval()

        state = {
            'pruning': pruning,
            'model': model.state_dict(), 'opt': opt,
            'scaler': scaler, 'epoch': epoch,
            'best_cer': best_cer, 'best_em': best_em,
            'config': self.config
        }
        if self.config['enabled']:
            print("ğŸ“¤ å¯¼å‡ºé‡åŒ–æ¨¡å‹...")

            # ä½¿ç”¨torch.exportå¯¼å‡ºé‡åŒ–æ¨¡å‹
            try:
                state['quantization_model'] = export(model, (example_input,))

                # ä¿å­˜å¯¼å‡ºçš„ç¨‹åº
                torch.save(state, path)

                print(f"âœ… é‡åŒ–æ¨¡å‹å¯¼å‡ºå®Œæˆ: {path}")

            except Exception as e:
                print(f"âš ï¸  å¯¼å‡ºå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•: {e}")

                # å¤‡ç”¨æ–¹æ³•ï¼šåªä¿å­˜çŠ¶æ€å­—å…¸
                del state['quantization_model']

                torch.save(state, path)
                print(f"âœ… æ¨¡å‹çŠ¶æ€å­—å…¸å¯¼å‡ºå®Œæˆ: {path}")
        else:
            print("âš ï¸  é‡åŒ–æœªå¯ç”¨ï¼Œå¯¼å‡ºåŸå§‹æ¨¡å‹")
            torch.save(state, path)
            return

# è‡ªå®šä¹‰ ParameterGrid å®ç°ï¼Œæ›¿ä»£ sklearn.model_selection.ParameterGrid çš„ä¾èµ–
class ParameterGrid:
    """ç”Ÿæˆå‚æ•°ç½‘æ ¼çš„æ‰€æœ‰ç»„åˆ"""
    def __init__(self, param_grid):
        self.param_grid = param_grid

    def __iter__(self):
        keys, values = zip(*self.param_grid.items())
        result = [dict(zip(keys, v)) for v in itertools.product(*values)]
        for item in result:
            yield item

    def __len__(self):

        return reduce(operator.mul, (len(v) for v in self.param_grid.values()), 1)

@dataclass
class OptimizationResult:
    """ä¼˜åŒ–ç»“æœ"""
    best_config: QuantizationConfig
    best_metrics: QuantizationMetrics
    optimization_history: List[Dict[str, Any]]
    total_trials: int
    best_trial: int
    optimization_time: float

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'best_config': self.best_config.to_dict(),
            'best_metrics': self.best_metrics.to_dict(),
            'optimization_history': self.optimization_history,
            'total_trials': self.total_trials,
            'best_trial': self.best_trial,
            'optimization_time': self.optimization_time
        }

class QuantizationObjective:
    """é‡åŒ–ç›®æ ‡å‡½æ•° - ç”¨äºä¼˜åŒ–"""

    def __init__(self, original_model: nn.Module, dataloader, device: str,
                 accuracy_weight: float = 0.6, compression_weight: float = 0.2,
                 speed_weight: float = 0.2, min_accuracy_threshold: float = 0.95):
        self.original_model = original_model
        self.dataloader = dataloader
        self.device = device
        self.accuracy_weight = accuracy_weight
        self.compression_weight = compression_weight
        self.speed_weight = speed_weight
        self.min_accuracy_threshold = min_accuracy_threshold

    def __call__(self, trial: Trial, config: QuantizationConfig) -> float:
        """è®¡ç®—ç›®æ ‡å‡½æ•°å€¼"""
        try:
            # æ ¹æ®trialå»ºè®®ä¿®æ”¹é…ç½®
            modified_config = self._suggest_config_changes(trial, config)

            # åˆ›å»ºé‡åŒ–æ¨¡å‹
            quantized_model = self._create_quantized_model(modified_config)

            # è¯„ä¼°é‡åŒ–æ•ˆæœ
            evaluator = QuantizationEvaluator(
                self.original_model, quantized_model, self.device
            )
            metrics = evaluator.evaluate_quantization(
                self.dataloader, num_batches=20  # å‡å°‘è¯„ä¼°æ‰¹æ¬¡ä»¥åŠ é€Ÿä¼˜åŒ–
            )

            # è®¡ç®—ç»¼åˆå¾—åˆ†
            score = self._calculate_score(metrics, modified_config)

            # ä¿å­˜trialä¿¡æ¯
            trial.set_user_attr('metrics', metrics.to_dict())
            trial.set_user_attr('config', modified_config.to_dict())

            return score

        except Exception as e:
            warnings.warn(f"Trial failed: {e}")
            return float('inf')  # è¿”å›ä¸€ä¸ªå¾ˆå¤§çš„å€¼è¡¨ç¤ºå¤±è´¥

    def _suggest_config_changes(self, trial: Trial, base_config: QuantizationConfig) -> QuantizationConfig:
        """æ ¹æ®trialå»ºè®®ä¿®æ”¹é…ç½®"""
        config = QuantizationConfig.from_dict(base_config.to_dict())

        # é‡åŒ–ä½æ•°
        config.weight_bits = trial.suggest_categorical('weight_bits', [4, 8])
        config.activation_bits = trial.suggest_categorical('activation_bits', [4, 8])

        # QATè®­ç»ƒè½®æ•°
        config.qat_epochs = trial.suggest_int('qat_epochs', 3, 10)

        # å­¦ä¹ ç‡å€æ•°
        config.qat_learning_rate_multiplier = trial.suggest_float(
            'qat_learning_rate_multiplier', 0.01, 0.5, log=True
        )

        # é‡åŒ–æŸå¤±æƒé‡
        config.quantization_loss_weight = trial.suggest_float(
            'quantization_loss_weight', 0.001, 0.1, log=True
        )

        # è’¸é¦æ¸©åº¦
        config.temperature_distillation = trial.suggest_float(
            'temperature_distillation', 2.0, 10.0
        )

        # è’¸é¦æƒé‡
        config.distillation_weight = trial.suggest_float(
            'distillation_weight', 0.1, 0.8
        )

        # è§‚å¯Ÿå™¨åŠ¨é‡
        config.observer_momentum = trial.suggest_float(
            'observer_momentum', 0.01, 0.2
        )

        # æ ¡å‡†æ‰¹æ¬¡æ•°é‡
        config.calibration_batches = trial.suggest_int(
            'calibration_batches', 50, 200, step=25
        )

        return config

    def _create_quantized_model(self, config: QuantizationConfig) -> nn.Module:
        """åˆ›å»ºé‡åŒ–æ¨¡å‹"""
        # è¿™é‡Œåº”è¯¥å®ç°å…·ä½“çš„é‡åŒ–é€»è¾‘
        # ç®€åŒ–ç‰ˆæœ¬ï¼šè¿”å›åŸå§‹æ¨¡å‹çš„å‰¯æœ¬
        return self.original_model

    def _calculate_score(self, metrics: QuantizationMetrics, config: QuantizationConfig) -> float:
        """è®¡ç®—ç»¼åˆå¾—åˆ†"""
        # ç²¾åº¦å¾—åˆ†ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
        accuracy_score = metrics.quantized_accuracy / metrics.original_accuracy

        # å¦‚æœç²¾åº¦ä½äºé˜ˆå€¼ï¼Œç»™äºˆæƒ©ç½š
        if accuracy_score < self.min_accuracy_threshold:
            accuracy_penalty = (self.min_accuracy_threshold - accuracy_score) * 10
            accuracy_score -= accuracy_penalty

        # å‹ç¼©å¾—åˆ†ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
        compression_score = metrics.compression_ratio / 4.0  # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
        compression_score = min(compression_score, 1.0)  # é™åˆ¶æœ€å¤§å€¼

        # é€Ÿåº¦å¾—åˆ†ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
        speed_score = metrics.speedup_ratio / 2.0  # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
        speed_score = min(speed_score, 1.0)  # é™åˆ¶æœ€å¤§å€¼

        # ç»¼åˆå¾—åˆ†
        total_score = (
            self.accuracy_weight * accuracy_score +
            self.compression_weight * compression_score +
            self.speed_weight * speed_score
        )

        return -total_score  # Optunaæœ€å°åŒ–ç›®æ ‡å‡½æ•°

class QuantizationHyperparameterOptimizer:
    """é‡åŒ–è¶…å‚æ•°ä¼˜åŒ–å™¨"""

    def __init__(self, original_model: nn.Module, dataloader, device: str):
        self.original_model = original_model
        self.dataloader = dataloader
        self.device = device

    def optimize_with_optuna(self, base_config: QuantizationConfig,
                           n_trials: int = 50,
                           timeout: int = 3600,
                           n_jobs: int = 1) -> OptimizationResult:
        """ä½¿ç”¨Optunaè¿›è¡Œè´å¶æ–¯ä¼˜åŒ–"""
        print(f"ğŸ” å¼€å§‹è´å¶æ–¯ä¼˜åŒ–ï¼Œè¯•éªŒæ¬¡æ•°: {n_trials}")

        start_time = time.time()

        # åˆ›å»ºç›®æ ‡å‡½æ•°
        objective = QuantizationObjective(
            self.original_model, self.dataloader, self.device
        )

        # åˆ›å»ºstudy
        study = optuna.create_study(
            direction='minimize',
            sampler=optuna.samplers.TPESampler(seed=42),
            pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=5)
        )

        # è¿è¡Œä¼˜åŒ–
        study.optimize(
            lambda trial: objective(trial, base_config),
            n_trials=n_trials,
            timeout=timeout,
            n_jobs=n_jobs,
            show_progress_bar=True
        )

        optimization_time = time.time() - start_time

        # è·å–æœ€ä½³ç»“æœ
        best_trial = study.best_trial
        best_config = QuantizationConfig.from_dict(best_trial.user_attrs['config'])
        best_metrics = QuantizationMetrics()
        # è¿™é‡Œéœ€è¦ä»trialä¸­æ¢å¤metricsï¼Œç®€åŒ–å¤„ç†

        # æ„å»ºä¼˜åŒ–å†å²
        history = []
        for trial in study.trials:
            if trial.value is not None:
                history.append({
                    'trial_number': trial.number,
                    'value': trial.value,
                    'params': trial.params,
                    'metrics': trial.user_attrs.get('metrics', {}),
                    'config': trial.user_attrs.get('config', {})
                })

        result = OptimizationResult(
            best_config=best_config,
            best_metrics=best_metrics,
            optimization_history=history,
            total_trials=len(study.trials),
            best_trial=best_trial.number,
            optimization_time=optimization_time
        )

        print(f"âœ… è´å¶æ–¯ä¼˜åŒ–å®Œæˆï¼Œæœ€ä½³è¯•éªŒ: {best_trial.number}, å¾—åˆ†: {best_trial.value:.4f}")
        return result

    def optimize_with_grid_search(self, base_config: QuantizationConfig,
                                param_grid: Optional[Dict[str, List]] = None) -> OptimizationResult:
        """ä½¿ç”¨ç½‘æ ¼æœç´¢è¿›è¡Œä¼˜åŒ–"""
        print("ğŸ” å¼€å§‹ç½‘æ ¼æœç´¢ä¼˜åŒ–")

        start_time = time.time()

        # é»˜è®¤å‚æ•°ç½‘æ ¼
        if param_grid is None:
            param_grid = {
                'weight_bits': [4, 8],
                'activation_bits': [4, 8],
                'qat_epochs': [3, 5, 8],
                'qat_learning_rate_multiplier': [0.05, 0.1, 0.2],
                'quantization_loss_weight': [0.005, 0.01, 0.02],
                'temperature_distillation': [4.0, 6.0, 8.0],
                'distillation_weight': [0.3, 0.5, 0.7],
                'observer_momentum': [0.05, 0.1, 0.15],
                'calibration_batches': [50, 100, 150]
            }

        # ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ
        grid = list(ParameterGrid(param_grid))
        print(f"ğŸ“Š æ€»å‚æ•°ç»„åˆæ•°: {len(grid)}")

        best_score = float('inf')
        best_config = None
        best_metrics = None
        best_trial = 0
        history = []

        for i, params in enumerate(tqdm(grid, desc="ç½‘æ ¼æœç´¢")):
            try:
                # åˆ›å»ºé…ç½®
                config = QuantizationConfig.from_dict(base_config.to_dict())
                for key, value in params.items():
                    setattr(config, key, value)

                # åˆ›å»ºé‡åŒ–æ¨¡å‹
                quantized_model = self._create_quantized_model_for_grid_search(config)

                # è¯„ä¼°
                evaluator = QuantizationEvaluator(
                    self.original_model, quantized_model, self.device
                )
                metrics = evaluator.evaluate_quantization(
                    self.dataloader, num_batches=10  # å‡å°‘è¯„ä¼°æ‰¹æ¬¡
                )

                # è®¡ç®—å¾—åˆ†
                score = self._calculate_simple_score(metrics)

                # ä¿å­˜å†å²
                history.append({
                    'trial_number': i,
                    'params': params,
                    'score': score,
                    'metrics': metrics.to_dict(),
                    'config': config.to_dict()
                })

                # æ›´æ–°æœ€ä½³ç»“æœ
                if score < best_score:
                    best_score = score
                    best_config = config
                    best_metrics = metrics
                    best_trial = i

            except Exception as e:
                warnings.warn(f"ç½‘æ ¼æœç´¢è¯•éªŒ {i} å¤±è´¥: {e}")
                continue

        optimization_time = time.time() - start_time

        result = OptimizationResult(
            best_config=best_config,
            best_metrics=best_metrics,
            optimization_history=history,
            total_trials=len(grid),
            best_trial=best_trial,
            optimization_time=optimization_time
        )

        print(f"âœ… ç½‘æ ¼æœç´¢å®Œæˆï¼Œæœ€ä½³å¾—åˆ†: {best_score:.4f}")
        return result

    def optimize_with_random_search(self, base_config: QuantizationConfig,
                                  n_trials: int = 100) -> OptimizationResult:
        """ä½¿ç”¨éšæœºæœç´¢è¿›è¡Œä¼˜åŒ–"""
        print(f"ğŸ” å¼€å§‹éšæœºæœç´¢ä¼˜åŒ–ï¼Œè¯•éªŒæ¬¡æ•°: {n_trials}")

        start_time = time.time()

        best_score = float('inf')
        best_config = None
        best_metrics = None
        best_trial = 0
        history = []

        for i in range(n_trials):
            try:
                # éšæœºç”Ÿæˆå‚æ•°
                config = self._generate_random_config(base_config)

                # åˆ›å»ºé‡åŒ–æ¨¡å‹
                quantized_model = self._create_quantized_model_for_grid_search(config)

                # è¯„ä¼°
                evaluator = QuantizationEvaluator(
                    self.original_model, quantized_model, self.device
                )
                metrics = evaluator.evaluate_quantization(
                    self.dataloader, num_batches=10
                )

                # è®¡ç®—å¾—åˆ†
                score = self._calculate_simple_score(metrics)

                # ä¿å­˜å†å²
                history.append({
                    'trial_number': i,
                    'config': config.to_dict(),
                    'score': score,
                    'metrics': metrics.to_dict()
                })

                # æ›´æ–°æœ€ä½³ç»“æœ
                if score < best_score:
                    best_score = score
                    best_config = config
                    best_metrics = metrics
                    best_trial = i

            except Exception as e:
                warnings.warn(f"éšæœºæœç´¢è¯•éªŒ {i} å¤±è´¥: {e}")
                continue

        optimization_time = time.time() - start_time

        result = OptimizationResult(
            best_config=best_config,
            best_metrics=best_metrics,
            optimization_history=history,
            total_trials=n_trials,
            best_trial=best_trial,
            optimization_time=optimization_time
        )

        print(f"âœ… éšæœºæœç´¢å®Œæˆï¼Œæœ€ä½³å¾—åˆ†: {best_score:.4f}")
        return result

    def _create_quantized_model_for_grid_search(self, config: QuantizationConfig) -> nn.Module:
        """ä¸ºç½‘æ ¼æœç´¢åˆ›å»ºé‡åŒ–æ¨¡å‹"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼šè¿”å›åŸå§‹æ¨¡å‹çš„å‰¯æœ¬
        # å®é™…å®ç°ä¸­åº”è¯¥åº”ç”¨çœŸæ­£çš„é‡åŒ–
        return self.original_model

    def _generate_random_config(self, base_config: QuantizationConfig) -> QuantizationConfig:
        """ç”Ÿæˆéšæœºé…ç½®"""
        import random

        config = QuantizationConfig.from_dict(base_config.to_dict())

        # éšæœºå‚æ•°
        config.weight_bits = random.choice([4, 8])
        config.activation_bits = random.choice([4, 8])
        config.qat_epochs = random.randint(3, 10)
        config.qat_learning_rate_multiplier = random.uniform(0.01, 0.5)
        config.quantization_loss_weight = random.uniform(0.001, 0.1)
        config.temperature_distillation = random.uniform(2.0, 10.0)
        config.distillation_weight = random.uniform(0.1, 0.8)
        config.observer_momentum = random.uniform(0.01, 0.2)
        config.calibration_batches = random.choice([50, 75, 100, 125, 150, 175, 200])

        return config

    def _calculate_simple_score(self, metrics: QuantizationMetrics) -> float:
        """è®¡ç®—ç®€å•å¾—åˆ†"""
        # ç²¾åº¦å¾—åˆ†
        accuracy_score = metrics.quantized_accuracy / metrics.original_accuracy

        # å‹ç¼©å¾—åˆ†
        compression_score = min(metrics.compression_ratio / 4.0, 1.0)

        # é€Ÿåº¦å¾—åˆ†
        speed_score = min(metrics.speedup_ratio / 2.0, 1.0)

        # ç»¼åˆå¾—åˆ†
        total_score = 0.6 * accuracy_score + 0.2 * compression_score + 0.2 * speed_score

        return -total_score  # è´Ÿå€¼ç”¨äºæœ€å°åŒ–

class AdaptiveHyperparameterOptimizer:
    """è‡ªé€‚åº”è¶…å‚æ•°ä¼˜åŒ–å™¨ - æ ¹æ®æ¨¡å‹å’Œä»»åŠ¡è‡ªåŠ¨é€‰æ‹©ä¼˜åŒ–ç­–ç•¥"""

    def __init__(self, model: nn.Module, dataloader, device: str):
        self.model = model
        self.dataloader = dataloader
        self.device = device
        self.base_optimizer = QuantizationHyperparameterOptimizer(model, dataloader, device)

    def optimize(self, base_config: QuantizationConfig,
                optimization_budget: str = 'medium',
                target_metric: str = 'balanced',
                method: str = 'bayesian') -> OptimizationResult:
        """
        è‡ªé€‚åº”ä¼˜åŒ–

        Args:
            base_config: åŸºç¡€é…ç½®
            optimization_budget: ä¼˜åŒ–é¢„ç®— ('low', 'medium', 'high')
            target_metric: ç›®æ ‡æŒ‡æ ‡ ('accuracy', 'compression', 'speed', 'balanced')
            method: ä¼˜åŒ–æ–¹æ³• ('bayesian', 'grid_search', 'random_search')
        """
        print(f"ğŸ¯ å¼€å§‹è‡ªé€‚åº”ä¼˜åŒ–ï¼Œé¢„ç®—: {optimization_budget}, ç›®æ ‡: {target_metric}, æ–¹æ³•: {method}")

        # æ ¹æ®é¢„ç®—é€‰æ‹©ä¼˜åŒ–å‚æ•°
        if optimization_budget == 'low':
            n_trials = 20
            timeout = 600  # 10åˆ†é’Ÿ
            n_jobs = 1
        elif optimization_budget == 'medium':
            n_trials = 50
            timeout = 1800  # 30åˆ†é’Ÿ
            n_jobs = 1  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
        else:  # high
            n_trials = 100
            timeout = 3600  # 1å°æ—¶
            n_jobs = 1  # é¿å…å¤šè¿›ç¨‹é—®é¢˜

        # æ ¹æ®ç›®æ ‡è°ƒæ•´æƒé‡
        if target_metric == 'accuracy':
            accuracy_weight, compression_weight, speed_weight = 0.8, 0.1, 0.1
        elif target_metric == 'compression':
            accuracy_weight, compression_weight, speed_weight = 0.3, 0.6, 0.1
        elif target_metric == 'speed':
            accuracy_weight, compression_weight, speed_weight = 0.3, 0.1, 0.6
        else:  # balanced
            accuracy_weight, compression_weight, speed_weight = 0.6, 0.2, 0.2

        # æ ¹æ®æŒ‡å®šçš„æ–¹æ³•é€‰æ‹©ä¼˜åŒ–ç­–ç•¥
        if method == 'bayesian':
            # ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–
            result = self.base_optimizer.optimize_with_optuna(
                base_config, n_trials=n_trials, timeout=timeout, n_jobs=n_jobs
            )
        elif method == 'grid_search':
            # ä½¿ç”¨ç½‘æ ¼æœç´¢
            result = self.base_optimizer.optimize_with_grid_search(base_config)
        elif method == 'random_search':
            # ä½¿ç”¨éšæœºæœç´¢
            result = self.base_optimizer.optimize_with_random_search(
                base_config, n_trials=n_trials
            )
        else:
            # é»˜è®¤ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–
            result = self.base_optimizer.optimize_with_optuna(
                base_config, n_trials=n_trials, timeout=timeout, n_jobs=n_jobs
            )

        return result

    def progressive_optimization(self, base_config: QuantizationConfig,
                               n_stages: int = 3) -> List[OptimizationResult]:
        """æ¸è¿›å¼ä¼˜åŒ– - åˆ†é˜¶æ®µä¼˜åŒ–ä¸åŒå‚æ•°"""
        print(f"ğŸš€ å¼€å§‹æ¸è¿›å¼ä¼˜åŒ–ï¼Œé˜¶æ®µæ•°: {n_stages}")

        results = []
        current_config = QuantizationConfig.from_dict(base_config.to_dict())

        # é˜¶æ®µ1: ä¼˜åŒ–é‡åŒ–ä½æ•°
        print("ğŸ“Š é˜¶æ®µ1: ä¼˜åŒ–é‡åŒ–ä½æ•°")
        stage1_config = QuantizationConfig.from_dict(current_config.to_dict())
        stage1_config.qat_epochs = 3  # å¿«é€Ÿè¯„ä¼°
        stage1_config.calibration_batches = 50

        param_grid = {
            'weight_bits': [4, 8],
            'activation_bits': [4, 8],
            'observer_type': ['moving_average', 'min_max'],
            'quantization_granularity': ['per_channel', 'per_tensor']
        }

        result1 = self.base_optimizer.optimize_with_grid_search(stage1_config, param_grid)
        results.append(result1)
        current_config = result1.best_config

        # é˜¶æ®µ2: ä¼˜åŒ–è®­ç»ƒå‚æ•°
        print("ğŸ“ˆ é˜¶æ®µ2: ä¼˜åŒ–è®­ç»ƒå‚æ•°")
        stage2_config = QuantizationConfig.from_dict(current_config.to_dict())

        param_grid = {
            'qat_epochs': [3, 5, 8],
            'qat_learning_rate_multiplier': [0.05, 0.1, 0.2],
            'calibration_batches': [50, 100, 150]
        }

        result2 = self.base_optimizer.optimize_with_grid_search(stage2_config, param_grid)
        results.append(result2)
        current_config = result2.best_config

        # é˜¶æ®µ3: ä¼˜åŒ–æŸå¤±å‡½æ•°å‚æ•°
        print("ğŸ¯ é˜¶æ®µ3: ä¼˜åŒ–æŸå¤±å‡½æ•°å‚æ•°")
        stage3_config = QuantizationConfig.from_dict(current_config.to_dict())

        param_grid = {
            'quantization_loss_weight': [0.005, 0.01, 0.02],
            'temperature_distillation': [4.0, 6.0, 8.0],
            'distillation_weight': [0.3, 0.5, 0.7],
            'observer_momentum': [0.05, 0.1, 0.15]
        }

        result3 = self.base_optimizer.optimize_with_grid_search(stage3_config, param_grid)
        results.append(result3)

        print("âœ… æ¸è¿›å¼ä¼˜åŒ–å®Œæˆ")
        return results

# é¢„å®šä¹‰é…ç½®æ¨¡æ¿
QUANTIZATION_CONFIG = {
    'enabled': True,                            # æ˜¯å¦å¯ç”¨é‡åŒ–
    'qat_epochs': 5,                           # QATè®­ç»ƒè½®æ•°
    'ptq_epochs': 2,                           # PTQè®­ç»ƒè½®æ•°
    'quantization_strategy': 'int8_dyn_act_int4_weight',  # é‡åŒ–ç­–ç•¥
    'calibration_batches': 100,                # æ ¡å‡†æ‰¹æ¬¡æ•°é‡
    'quantization_aware_training': True,       # æ˜¯å¦ä½¿ç”¨QAT
    'post_training_quantization': False,       # æ˜¯å¦ä½¿ç”¨PTQ
    'mixed_precision': True,                   # æ··åˆç²¾åº¦è®­ç»ƒ
    'quantization_layers': ['linear', 'conv2d', 'attention'],  # éœ€è¦é‡åŒ–çš„å±‚ç±»å‹
    'excluded_layers': ['embedding', 'layernorm'],  # æ’é™¤çš„å±‚
    'quantization_bits': {
        'weight': 4,                           # æƒé‡é‡åŒ–ä½æ•°
        'activation': 8,                       # æ¿€æ´»é‡åŒ–ä½æ•°
    },
    'observer_type': 'moving_average',         # è§‚å¯Ÿå™¨ç±»å‹
    'observer_momentum': 0.1,                  # è§‚å¯Ÿå™¨åŠ¨é‡
    'quantization_granularity': 'per_channel', # é‡åŒ–ç²’åº¦
    'qat_learning_rate_multiplier': 0.1,       # QATå­¦ä¹ ç‡å€æ•°
    'quantization_loss_weight': 0.01,          # é‡åŒ–æŸå¤±æƒé‡
    'temperature_distillation': 4.0,           # çŸ¥è¯†è’¸é¦æ¸©åº¦
    'distillation_weight': 0.3,                # çŸ¥è¯†è’¸é¦æƒé‡
}
PREDEFINED_CONFIGS = {
    'ocr_conservative': QuantizationConfig(
        enabled=True,
        strategy=QuantizationStrategy.INT8_DYN_ACT_INT4_WEIGHT,
        quantization_aware_training=True,
        qat_epochs=8,
        weight_bits=4,
        activation_bits=8,
        observer_type=ObserverType.MOVING_AVERAGE,
        quantization_granularity=QuantizationGranularity.PER_CHANNEL,
        quantization_loss_weight=0.02,
        temperature_distillation=6.0,
        distillation_weight=0.5,
        qat_learning_rate_multiplier=0.05,
    ),

    'ocr_balanced': QuantizationConfig(
        enabled=True,
        strategy=QuantizationStrategy.INT8_DYN_ACT_INT4_WEIGHT,
        quantization_aware_training=True,
        qat_epochs=5,
        weight_bits=4,
        activation_bits=8,
        observer_type=ObserverType.MOVING_AVERAGE,
        quantization_granularity=QuantizationGranularity.PER_CHANNEL,
        quantization_loss_weight=0.01,
        temperature_distillation=4.0,
        distillation_weight=0.3,
        qat_learning_rate_multiplier=0.1,
    ),

    'ocr_aggressive': QuantizationConfig(
        enabled=True,
        strategy=QuantizationStrategy.INT4_WEIGHT_ONLY,
        quantization_aware_training=True,
        qat_epochs=10,
        weight_bits=4,
        activation_bits=4,
        observer_type=ObserverType.MIN_MAX,
        quantization_granularity=QuantizationGranularity.PER_TENSOR,
        quantization_loss_weight=0.03,
        temperature_distillation=8.0,
        distillation_weight=0.7,
        qat_learning_rate_multiplier=0.05,
    ),

    'mobile_optimized': QuantizationConfig(
        enabled=True,
        strategy=QuantizationStrategy.INT4_WEIGHT_ONLY,
        quantization_aware_training=True,
        qat_epochs=6,
        weight_bits=4,
        activation_bits=4,
        observer_type=ObserverType.MIN_MAX,
        quantization_granularity=QuantizationGranularity.PER_TENSOR,
        quantization_loss_weight=0.02,
        temperature_distillation=6.0,
        distillation_weight=0.5,
        qat_learning_rate_multiplier=0.08,
        memory_efficient=True,
        compile_model=True,
    ),

    'server_optimized': QuantizationConfig(
        enabled=True,
        strategy=QuantizationStrategy.INT8_DYN_ACT_INT4_WEIGHT,
        quantization_aware_training=True,
        qat_epochs=4,
        weight_bits=4,
        activation_bits=8,
        observer_type=ObserverType.MOVING_AVERAGE,
        quantization_granularity=QuantizationGranularity.PER_CHANNEL,
        quantization_loss_weight=0.01,
        temperature_distillation=4.0,
        distillation_weight=0.3,
        qat_learning_rate_multiplier=0.1,
        enable_cuda_graphs=True,
        memory_efficient=False,
    ),
}

def get_config_template(template_name: str) -> QuantizationConfig:
    """è·å–é¢„å®šä¹‰é…ç½®æ¨¡æ¿"""
    if template_name not in PREDEFINED_CONFIGS:
        available_templates = list(PREDEFINED_CONFIGS.keys())
        raise ValueError(f"æœªçŸ¥çš„é…ç½®æ¨¡æ¿: {template_name}. å¯ç”¨æ¨¡æ¿: {available_templates}")

    return PREDEFINED_CONFIGS[template_name]

def create_optimal_config(model: nn.Module, task_type: str = 'ocr',
                         hardware_target: str = 'cpu',
                         target_compression_ratio: float = 0.25,
                         preserve_accuracy: bool = True) -> QuantizationConfig:
    """åˆ›å»ºæœ€ä¼˜é‡åŒ–é…ç½®"""
    adaptive_config = AdaptiveQuantizationConfig(model, task_type)

    # è·å–æ¨èé…ç½®
    config = adaptive_config.recommend_config(target_compression_ratio, preserve_accuracy)

    # é’ˆå¯¹ç¡¬ä»¶ä¼˜åŒ–
    config = adaptive_config.optimize_config_for_hardware(hardware_target)

    # éªŒè¯é…ç½®
    errors = QuantizationConfigValidator.validate_config(config)
    if errors:
        raise ValueError(f"é…ç½®éªŒè¯å¤±è´¥: {errors}")

    # æ£€æŸ¥è­¦å‘Š
    warnings = QuantizationConfigValidator.warn_config(config)
    if warnings:
        print("âš ï¸  é…ç½®è­¦å‘Š:")
        for warning in warnings:
            print(f"  - {warning}")

    return config

# ä½¿ç”¨ç¤ºä¾‹å’Œå·¥å…·å‡½æ•°
def create_optimization_study(device: str, model: nn.Module, dataloader: DataLoader, output_dir: str, study_name: str, method: str = 'bayesian',
                            n_trials: int = 50, param_config: dict = None,
                            optimization_target: str = 'balanced', dry_run: bool = False) -> None:
    """
    åˆ›å»ºå¹¶è¿è¡Œè¶…å‚æ•°è‡ªåŠ¨ä¼˜åŒ–ç ”ç©¶

    Args:
        output_dir: è¾“å‡ºç›®å½•
        study_name: ç ”ç©¶åç§°
        method: ä¼˜åŒ–æ–¹æ³• ('bayesian', 'grid_search', 'random_search')
        n_trials: è¯•éªŒæ¬¡æ•°
        param_config: å‚æ•°é…ç½®æ–‡ä»¶è·¯å¾„æˆ–å­—å…¸
        optimization_target: ä¼˜åŒ–ç›®æ ‡ ('balanced', 'accuracy', 'compression', 'speed')
        dry_run: æ˜¯å¦åªéªŒè¯ä»£ç æµç¨‹ï¼Œä¸æ‰§è¡Œå®é™…ä¼˜åŒ–
    """

    # åˆ›å»ºç ”ç©¶ç›®å½•
    study_dir = Path(output_dir) / f"optimization_study_{study_name}"
    study_dir.mkdir(parents=True, exist_ok=True)

    # åˆ›å»ºåŸºç¡€é…ç½®
    base_config = create_optimal_config(
        model=model,
        task_type='ocr',
        hardware_target='cpu',
        target_compression_ratio=0.25,
        preserve_accuracy=True
    )

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = AdaptiveHyperparameterOptimizer(model, dataloader, device)

    # æ ¹æ®ä¼˜åŒ–ç›®æ ‡è°ƒæ•´é¢„ç®—
    if optimization_target == 'balanced':
        budget = 'medium'
    elif optimization_target == 'accuracy':
        budget = 'high'  # æ›´é«˜é¢„ç®—ä»¥è·å¾—æ›´å¥½ç²¾åº¦
    else:
        budget = 'medium'

    # è¿è¡Œä¼˜åŒ–
    print(f"ğŸš€ å¼€å§‹ä¼˜åŒ–ç ”ç©¶: {study_name}")
    print(f"   - ä¼˜åŒ–æ–¹æ³•: {method}")
    print(f"   - è¯•éªŒæ¬¡æ•°: {n_trials}")
    print(f"   - ä¼˜åŒ–ç›®æ ‡: {optimization_target}")
    print(f"   - è¾“å‡ºç›®å½•: {study_dir}")

    # åˆ›å»ºç ”ç©¶é…ç½®
    study_config = {
        'study_name': study_name,
        'created_at': time.strftime('%Y-%m-%d %H:%M:%S'),
        'output_directory': str(study_dir),
        'optimization_method': method,
        'n_trials': n_trials,
        'optimization_target': optimization_target,
        'base_config': base_config.to_dict(),
        'evaluation_metrics': ['accuracy', 'compression', 'speed', 'memory']
    }

    # è¿è¡Œä¸åŒçš„ä¼˜åŒ–æ–¹æ³•
    if dry_run:
        print("ğŸ” æ‰§è¡Œdry_runï¼Œè·³è¿‡å®é™…ä¼˜åŒ–...")
        # åˆ›å»ºæ¨¡æ‹Ÿç»“æœï¼Œç”¨äºéªŒè¯æµç¨‹

        # æ¨¡æ‹Ÿä¼˜åŒ–ç»“æœ
        result = OptimizationResult(
            best_config=base_config,
            best_metrics=QuantizationMetrics(),
            optimization_history=[
                {
                    'trial_number': 0,
                    'score': -0.85,
                    'params': {
                        'weight_bits': 4,
                        'activation_bits': 8,
                        'qat_epochs': 5,
                        'qat_learning_rate_multiplier': 0.1,
                        'quantization_loss_weight': 0.01,
                        'temperature_distillation': 4.0,
                        'distillation_weight': 0.3,
                        'observer_momentum': 0.1,
                        'calibration_batches': 100
                    },
                    'metrics': {
                        'accuracy': {
                            'original': 0.98,
                            'quantized': 0.975,
                            'drop': 0.005,
                            'drop_ratio': 0.0051
                        },
                        'model_size': {
                            'original_mb': 15.2,
                            'quantized_mb': 4.8,
                            'compression_ratio': 3.17,
                            'reduction_ratio': 0.684
                        }
                    },
                    'config': base_config.to_dict()
                }
            ],
            total_trials=1,
            best_trial=0,
            optimization_time=10.5
        )
    else:
        if method == 'bayesian':
            print("ğŸ¯ è¿è¡Œè´å¶æ–¯ä¼˜åŒ–...")
            result = optimizer.optimize(
                base_config=base_config,
                optimization_budget=budget,
                target_metric=optimization_target,
                method=method
            )
        elif method == 'grid_search':
            print("ğŸ“‹ è¿è¡Œç½‘æ ¼æœç´¢...")
            result = optimizer.optimize(
                base_config=base_config,
                optimization_budget=budget,
                target_metric=optimization_target,
                method=method
            )
        elif method == 'random_search':
            print("ğŸ² è¿è¡Œéšæœºæœç´¢...")
            result = optimizer.optimize(
                base_config=base_config,
                optimization_budget=budget,
                target_metric=optimization_target,
                method=method
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–æ–¹æ³•: {method}")

    # ä¿å­˜ä¼˜åŒ–ç»“æœ
    print("ğŸ’¾ ä¿å­˜ä¼˜åŒ–ç»“æœ...")

    # ä¿å­˜ç ”ç©¶é…ç½®
    config_path = study_dir / 'study_config.json'
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(study_config, f, indent=2, ensure_ascii=False)

    # ä¿å­˜æœ€ä½³é…ç½®
    best_config_path = study_dir / 'best_quantization_config.json'
    result.best_config.save_to_file(str(best_config_path))

    # ä¿å­˜ä¼˜åŒ–å†å²
    history_path = study_dir / 'optimization_history.json'
    with open(history_path, 'w', encoding='utf-8') as f:
        json.dump(result.optimization_history, f, indent=2, ensure_ascii=False)

    # ä¿å­˜æœ€ä½³æŒ‡æ ‡
    best_metrics_path = study_dir / 'best_metrics.json'
    with open(best_metrics_path, 'w', encoding='utf-8') as f:
        json.dump(result.best_metrics.to_dict(), f, indent=2, ensure_ascii=False)

    # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
    generate_optimization_report(result, study_dir, study_name)

    print(f"âœ… ä¼˜åŒ–ç ”ç©¶å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {study_dir}")
    print(f"ğŸ† æœ€ä½³é…ç½®å·²ä¿å­˜åˆ°: {best_config_path}")


def generate_optimization_report(result: OptimizationResult, study_dir: Path, study_name: str) -> None:
    """
    ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š

    Args:
        result: ä¼˜åŒ–ç»“æœ
        study_dir: è¾“å‡ºç›®å½•
        study_name: ç ”ç©¶åç§°
    """
    report = {
        'study_name': study_name,
        'total_trials': result.total_trials,
        'best_trial': result.best_trial,
        'optimization_time': result.optimization_time,
        'best_config': result.best_config.to_dict(),
        'best_metrics': result.best_metrics.to_dict(),
        'optimization_history': result.optimization_history,
        'summary': {
            'accuracy_improvement': 'N/A',
            'compression_ratio': f"{result.best_metrics.compression_ratio:.2f}x",
            'speedup_ratio': f"{result.best_metrics.speedup_ratio:.2f}x",
            'memory_reduction': f"{result.best_metrics.memory_reduction_ratio*100:.1f}%"
        }
    }

    # ä¿å­˜æŠ¥å‘Š
    report_path = study_dir / 'optimization_report.json'
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    # ç”Ÿæˆå¯è§†åŒ–
    print("ğŸ“ˆ ç”Ÿæˆä¼˜åŒ–å¯è§†åŒ–...")
    visualize_optimization_results(result, study_dir)

    print(f"ğŸ“„ ä¼˜åŒ–æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")


def visualize_optimization_results(result: OptimizationResult, study_dir: Path) -> None:
    """
    å¯è§†åŒ–ä¼˜åŒ–ç»“æœ

    Args:
        result: ä¼˜åŒ–ç»“æœ
        study_dir: è¾“å‡ºç›®å½•
    """
    import matplotlib.pyplot as plt

    # æå–å†å²æ•°æ®
    trial_numbers = [item['trial_number'] for item in result.optimization_history]
    scores = [item['score'] for item in result.optimization_history]

    # åˆ›å»ºå›¾è¡¨
    plt.figure(figsize=(12, 6))

    # 1. ä¼˜åŒ–å¾—åˆ†å˜åŒ–
    plt.subplot(1, 2, 1)
    plt.plot(trial_numbers, scores, 'b-', alpha=0.7, label='Optimization Score')
    plt.scatter(trial_numbers, scores, c='red', s=20, alpha=0.5)
    plt.xlabel('Trial Number')
    plt.ylabel('Score (Lower is Better)')
    plt.title('Optimization Score Over Trials')
    plt.grid(True, alpha=0.3)
    plt.legend()

    # 2. æœ€ä½³é…ç½®å‚æ•°
    plt.subplot(1, 2, 2)
    best_config = result.best_config.to_dict()

    # æå–å…³é”®å‚æ•°
    params = {
        'Weight Bits': best_config['weight_bits'],
        'Activation Bits': best_config['activation_bits'],
        'QAT Epochs': best_config['qat_epochs'],
        'Learning Rate Multiplier': best_config['qat_learning_rate_multiplier'],
        'Quantization Loss Weight': best_config['quantization_loss_weight'],
        'Distillation Temperature': best_config['temperature_distillation'],
        'Distillation Weight': best_config['distillation_weight'],
        'Calibration Batches': best_config['calibration_batches']
    }

    # è½¬æ¢ä¸ºé€‚åˆå›¾è¡¨çš„æ•°æ®
    param_names = list(params.keys())
    param_values = list(params.values())

    # ä½¿ç”¨æ°´å¹³æ¡å½¢å›¾æ˜¾ç¤ºå‚æ•°
    plt.barh(param_names, param_values, color='green', alpha=0.7)
    plt.xlabel('Value')
    plt.title('Best Configuration Parameters')
    plt.grid(True, alpha=0.3)

    # ä¿å­˜å›¾è¡¨
    plt.tight_layout()
    viz_path = study_dir / 'optimization_visualization.png'
    plt.savefig(viz_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"ğŸ“Š ä¼˜åŒ–å¯è§†åŒ–å·²ä¿å­˜: {viz_path}")

def compare_optimization_methods(results: Dict[str, OptimizationResult]) -> None:
    """æ¯”è¾ƒä¸åŒä¼˜åŒ–æ–¹æ³•çš„ç»“æœ"""
    print("\nğŸ“Š ä¼˜åŒ–æ–¹æ³•æ¯”è¾ƒ:")
    print("="*80)

    for method, result in results.items():
        print(f"\nğŸ” {method.upper()} ä¼˜åŒ–ç»“æœ:")
        print(f"  - æœ€ä½³è¯•éªŒ: #{result.best_trial}")
        print(f"  - ä¼˜åŒ–æ—¶é—´: {result.optimization_time:.1f}ç§’")
        print(f"  - æ€»è¯•éªŒæ•°: {result.total_trials}")

        if result.best_metrics:
            print(f"  - ç²¾åº¦ä¿æŒ: {(1 - result.best_metrics.accuracy_drop_ratio)*100:.1f}%")
            print(f"  - å‹ç¼©æ¯”: {result.best_metrics.compression_ratio:.2f}x")
            print(f"  - åŠ é€Ÿæ¯”: {result.best_metrics.speedup_ratio:.2f}x")

# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    # åˆ›å»ºç¤ºä¾‹æ¨¡å‹

    class ExampleModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.conv1 = nn.Conv2d(3, 64, 3)
            self.linear1 = nn.Linear(64 * 30 * 30, 128)
            self.linear2 = nn.Linear(128, 10)

        def forward(self, x):
            x = self.conv1(x)
            x = x.view(x.size(0), -1)
            x = torch.relu(self.linear1(x))
            return self.linear2(x)

    model = ExampleModel()

    # åˆ›å»ºæœ€ä¼˜é…ç½®
    config = create_optimal_config(
        model=model,
        task_type='ocr',
        hardware_target='mobile',
        target_compression_ratio=0.2,
        preserve_accuracy=True
    )

    print("ğŸ¯ æœ€ä¼˜é‡åŒ–é…ç½®:")
    print(json.dumps(config.to_dict(), indent=2, ensure_ascii=False))

    # ä¿å­˜é…ç½®
    config.save_to_file('optimal_quantization_config.json')
    print("âœ… é…ç½®å·²ä¿å­˜åˆ° optimal_quantization_config.json")
